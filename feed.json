{
    "version": "https://jsonfeed.org/version/1",
    "title": "iwpnd",
    "home_page_url": "https://iwpnd.pw//",
    "feed_url": "https://iwpnd.pw//feed.json",
    "description": "geographer turned data engineer turned backend developer",
    "icon": "https://iwpnd.pw//apple-touch-icon.png",
    "favicon": "https://iwpnd.pw//favicon.ico",
    "expired": false,
    
    "author":  {
        "name": "iwpnd",
        "url": null,
        "avatar": "/img/iwpnd-logo.png"
    },
    
"items": [
    
        {
            "id": "https://iwpnd.pw//articles/2021-07/pyle38-tile38-python-client",
            "title": "pyle38 - An async python client for Tile38",
            "summary": null,
            "content_text": "Back in May I wrote an introductory blog post about Tile38. Professionally I work with Tile38 ina TypeScript eco-system. Together with a colleague I authored a TypeScript clientthat is now used internally - and hopefully soon to be released to the public.As there was also no client for Python either yet, and due to the pandemic, therewas more free time than usual on my hand I wrote one myself.Introducing Pyle38Pyle38 is a lazy asynchonous Python client for Tile38 that allows for fast and easyinteraction with the worlds fastest in-memory geodatabase Tile38build on top of the re-designed aioredis 2.0.0.Even though the Python community is only slowly adapting type hints into Python, Imade sure that Pyle38 is already fully typed and passes mypyvalidation. Pydantic is used to enforcethose type hints at runtime.Feature: Command chainingTile38 provides a lot of options for its queries, configs and insert commands.WITHIN fleet LIMIT 10 NOFIELDS IDS CIRCLE 52.25 13.37 1000Returns the ids of 10 objects in the fleet collection in a radius of 1000around a point with latitude 52.25 and longitude 13.37. The order of thethe commands and options is fixed, putting the LIMIT 10 at the end of the commandwould result in an error.The same query in pyle38 would look like this:await tile38.within('fleet').limit(10).nofields().circle(52.25, 13.37, 1000).asIds()The order of the commands in pyle38 can, but does not have to be exactly asrequired in Tile38. Commands can be chained to the users liking and pyle38 willtake care that the order is correct when the command is passed to Tile38.Feature: fully asynchronousPyle38 methods are almost entirely asynchronous. Why? For one I wanted to learnhow to do it. On the other hand the application I had in mind for Pyle38/Tile38was as a backend for a FastAPI app.Feature: Leader/Follower replicationTile38 supports leader/follower replicationand so does Pyle38. Pyle38 allows to instantiate two clients at once; one forthe leader and another one for a follower. That allows to let writes be handledby the leader and reads by the follower. Currently only one follower uri is supported.The reason is, that ideally you think of how to spread the query loadacross all followers. Now this can be done on the client, but it woulda load balancer will probably do a better job.Basic Usageimport asynciofrom pyle38 import Tile38async def main():    tile38 = Tile38(url=\"redis://localhost:9851\", follower_url=\"redis://localhost:9851\")    await tile38.set(\"fleet\", \"truck\").point(52.25,13.37).exec()    response = await tile38.follower()        .within(\"fleet\")        .circle(52.25, 13.37, 1000)        .asObjects()    assert response.ok    print(response.dict())asyncio.run(main())&gt; {    \"ok\": True,    \"elapsed\": \"48.8µs\",    \"objects\": [        {            \"object\": {                \"type\": \"Point\",                \"coordinates\": [                    13.37,                    52.25                ]            },            \"id\": \"truck\"        }    ],    \"count\": 1,    \"cursor\": 0}Let’s go and dissect this example.tile38 = Tile38(url=\"redis://localhost:9851\", follower_url=\"redis://localhost:9851\")At First, an instance of Tile38 is created. Optionally a follower url can be passedto Tile38 in the constructor, that will create a both a client for the leader,as well as the follower.await tile38.set(\"fleet\", \"truck\").point(52.25,13.37).exec()Now we add an object with the name truck to the fleet collection as a pointwith latitude 52.25 and longitude 13.37. SET’s have to be executed specificallywith .exec().response = await tile38.follower()        .within(\"fleet\")        .circle(52.25, 13.37, 1000)        .asObjects()As there is now an object in the fleet collection. The user can now send a queryto Tile38. The .follower() method indicates that the query should specificallybe send to the follower instance of Tile38. If follower() is omitted, the queryis send to the leader instance instead..within('fleet') searches a collection for objects that are fullycontained within a given bounding area. The bounding area that is provided inthis query is a .circle(52.25, 13.37, 1000) with a radius of 1000 around thecoordinates 52.25 and 13.37.Get it nowYou can download pyle38 via pip install pyle38 or if Python is not your thing, you cancheck our other officially supported Tile38 clients.",
            "content_html": "<p align=\"center\"><img src=\"/img/2021-07-31-pyle38/pyle38.png\" alt=\"pyle38\" /></p><p>Back in May I wrote an introductory blog post about <a href=\"https://iwpnd.pw/articles/2021-04/Tile38-in-memory-geodatabase\">Tile38</a>. Professionally I work with Tile38 ina TypeScript eco-system. Together with a colleague I authored a TypeScript clientthat is now used internally - and hopefully soon to be released to the public.As there was also no client for Python either yet, and due to the pandemic, therewas more free time than usual on my hand I wrote one myself.</p><h2 id=\"introducing-pyle38\">Introducing Pyle38</h2><p>Pyle38 is a lazy asynchonous Python client for Tile38 that allows for fast and easyinteraction with the worlds fastest in-memory geodatabase <a href=\"https://tile38.com\">Tile38</a>build on top of the re-designed <a href=\"https://aioredis.readthedocs.io/en/latest/migration/\">aioredis 2.0.0</a>.Even though the Python community is only slowly adapting type hints into Python, Imade sure that Pyle38 is already fully typed and passes <a href=\"http://mypy-lang.org/\">mypy</a>validation. <a href=\"https://pydantic-docs.helpmanual.io/\">Pydantic</a> is used to enforcethose type hints at runtime.</p><h2 id=\"feature-command-chaining\">Feature: Command chaining</h2><p>Tile38 provides a lot of options for its queries, configs and insert commands.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>WITHIN fleet LIMIT 10 NOFIELDS IDS CIRCLE 52.25 13.37 1000</code></pre></div></div><p>Returns the ids of 10 objects in the <code class=\"language-plaintext highlighter-rouge\">fleet</code> collection in a radius of 1000around a point with latitude 52.25 and longitude 13.37. The order of thethe commands and options is fixed, putting the <code class=\"language-plaintext highlighter-rouge\">LIMIT 10</code> at the end of the commandwould result in an error.</p><p>The same query in pyle38 would look like this:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">await</span> <span class=\"n\">tile38</span><span class=\"p\">.</span><span class=\"n\">within</span><span class=\"p\">(</span><span class=\"s\">'fleet'</span><span class=\"p\">).</span><span class=\"n\">limit</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">).</span><span class=\"n\">nofields</span><span class=\"p\">().</span><span class=\"n\">circle</span><span class=\"p\">(</span><span class=\"mf\">52.25</span><span class=\"p\">,</span> <span class=\"mf\">13.37</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">).</span><span class=\"n\">asIds</span><span class=\"p\">()</span></code></pre></div></div><p>The order of the commands in pyle38 can, but does not have to be exactly asrequired in Tile38. Commands can be chained to the users liking and pyle38 willtake care that the order is correct when the command is passed to Tile38.</p><h2 id=\"feature-fully-asynchronous\">Feature: fully asynchronous</h2><p>Pyle38 methods are almost entirely asynchronous. Why? For one I wanted to learnhow to do it. On the other hand the application I had in mind for Pyle38/Tile38was as a backend for a <a href=\"https://fastapi.tiangolo.com/\">FastAPI</a> app.</p><h2 id=\"feature-leaderfollower-replication\">Feature: Leader/Follower replication</h2><p>Tile38 supports <a href=\"https://iwpnd.pw/articles/2021-04/Tile38-in-memory-geodatabase#leader-and-follower-replication\">leader/follower replication</a>and so does Pyle38. Pyle38 allows to instantiate two clients at once; one forthe leader and another one for a follower. That allows to let writes be handledby the leader and reads by the follower. Currently only one follower uri is supported.The reason is, that ideally you think of how to spread the query loadacross all followers. Now this can be done on the client, but it woulda load balancer will probably do a better job.</p><h2 id=\"basic-usage\">Basic Usage</h2><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">asyncio</span><span class=\"kn\">from</span> <span class=\"nn\">pyle38</span> <span class=\"kn\">import</span> <span class=\"n\">Tile38</span><span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">():</span>    <span class=\"n\">tile38</span> <span class=\"o\">=</span> <span class=\"n\">Tile38</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"o\">=</span><span class=\"s\">\"redis://localhost:9851\"</span><span class=\"p\">,</span> <span class=\"n\">follower_url</span><span class=\"o\">=</span><span class=\"s\">\"redis://localhost:9851\"</span><span class=\"p\">)</span>    <span class=\"k\">await</span> <span class=\"n\">tile38</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"s\">\"fleet\"</span><span class=\"p\">,</span> <span class=\"s\">\"truck\"</span><span class=\"p\">).</span><span class=\"n\">point</span><span class=\"p\">(</span><span class=\"mf\">52.25</span><span class=\"p\">,</span><span class=\"mf\">13.37</span><span class=\"p\">).</span><span class=\"k\">exec</span><span class=\"p\">()</span>    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">tile38</span><span class=\"p\">.</span><span class=\"n\">follower</span><span class=\"p\">()</span>        <span class=\"p\">.</span><span class=\"n\">within</span><span class=\"p\">(</span><span class=\"s\">\"fleet\"</span><span class=\"p\">)</span>        <span class=\"p\">.</span><span class=\"n\">circle</span><span class=\"p\">(</span><span class=\"mf\">52.25</span><span class=\"p\">,</span> <span class=\"mf\">13.37</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">)</span>        <span class=\"p\">.</span><span class=\"n\">asObjects</span><span class=\"p\">()</span>    <span class=\"k\">assert</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">ok</span>    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"nb\">dict</span><span class=\"p\">())</span><span class=\"n\">asyncio</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">main</span><span class=\"p\">())</span><span class=\"o\">&gt;</span> <span class=\"p\">{</span>    <span class=\"s\">\"ok\"</span><span class=\"p\">:</span> <span class=\"bp\">True</span><span class=\"p\">,</span>    <span class=\"s\">\"elapsed\"</span><span class=\"p\">:</span> <span class=\"s\">\"48.8µs\"</span><span class=\"p\">,</span>    <span class=\"s\">\"objects\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>        <span class=\"p\">{</span>            <span class=\"s\">\"object\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>                <span class=\"s\">\"type\"</span><span class=\"p\">:</span> <span class=\"s\">\"Point\"</span><span class=\"p\">,</span>                <span class=\"s\">\"coordinates\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>                    <span class=\"mf\">13.37</span><span class=\"p\">,</span>                    <span class=\"mf\">52.25</span>                <span class=\"p\">]</span>            <span class=\"p\">},</span>            <span class=\"s\">\"id\"</span><span class=\"p\">:</span> <span class=\"s\">\"truck\"</span>        <span class=\"p\">}</span>    <span class=\"p\">],</span>    <span class=\"s\">\"count\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>    <span class=\"s\">\"cursor\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">}</span></code></pre></div></div><p>Let’s go and dissect this example.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">tile38</span> <span class=\"o\">=</span> <span class=\"n\">Tile38</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"o\">=</span><span class=\"s\">\"redis://localhost:9851\"</span><span class=\"p\">,</span> <span class=\"n\">follower_url</span><span class=\"o\">=</span><span class=\"s\">\"redis://localhost:9851\"</span><span class=\"p\">)</span></code></pre></div></div><p>At First, an instance of Tile38 is created. Optionally a follower url can be passedto Tile38 in the constructor, that will create a both a client for the leader,as well as the follower.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">await</span> <span class=\"n\">tile38</span><span class=\"p\">.</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"s\">\"fleet\"</span><span class=\"p\">,</span> <span class=\"s\">\"truck\"</span><span class=\"p\">).</span><span class=\"n\">point</span><span class=\"p\">(</span><span class=\"mf\">52.25</span><span class=\"p\">,</span><span class=\"mf\">13.37</span><span class=\"p\">).</span><span class=\"k\">exec</span><span class=\"p\">()</span></code></pre></div></div><p>Now we add an object with the name <code class=\"language-plaintext highlighter-rouge\">truck</code> to the <code class=\"language-plaintext highlighter-rouge\">fleet</code> collection as a pointwith latitude 52.25 and longitude 13.37. <code class=\"language-plaintext highlighter-rouge\">SET</code>’s have to be executed specificallywith <code class=\"language-plaintext highlighter-rouge\">.exec()</code>.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">tile38</span><span class=\"p\">.</span><span class=\"n\">follower</span><span class=\"p\">()</span>        <span class=\"p\">.</span><span class=\"n\">within</span><span class=\"p\">(</span><span class=\"s\">\"fleet\"</span><span class=\"p\">)</span>        <span class=\"p\">.</span><span class=\"n\">circle</span><span class=\"p\">(</span><span class=\"mf\">52.25</span><span class=\"p\">,</span> <span class=\"mf\">13.37</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">)</span>        <span class=\"p\">.</span><span class=\"n\">asObjects</span><span class=\"p\">()</span></code></pre></div></div><p>As there is now an object in the fleet collection. The user can now send a queryto Tile38. The <code class=\"language-plaintext highlighter-rouge\">.follower()</code> method indicates that the query should specificallybe send to the follower instance of Tile38. If <code class=\"language-plaintext highlighter-rouge\">follower()</code> is omitted, the queryis send to the leader instance instead.<code class=\"language-plaintext highlighter-rouge\">.within('fleet')</code> searches a collection for objects that are fullycontained within a given bounding area. The bounding area that is provided inthis query is a <code class=\"language-plaintext highlighter-rouge\">.circle(52.25, 13.37, 1000)</code> with a radius of <code class=\"language-plaintext highlighter-rouge\">1000</code> around thecoordinates <code class=\"language-plaintext highlighter-rouge\">52.25</code> and <code class=\"language-plaintext highlighter-rouge\">13.37</code>.</p><h2 id=\"get-it-now\">Get it now</h2><p>You can download pyle38 via <code class=\"language-plaintext highlighter-rouge\">pip install pyle38</code> or if Python is not your thing, you cancheck our other officially supported <a href=\"https://tile38.com/topics/client-libraries\">Tile38 clients</a>.</p>",
            "url": "https://iwpnd.pw//articles/2021-07/pyle38-tile38-python-client",
            
            
            
            "tags": ["tile38","database","geodata","pyle38","python","async"],
            
            "date_published": "2021-07-31T11:37:00+00:00",
            "date_modified": "2021-07-31T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2021-05/docker-compose-in-monorepos",
            "title": "docker-compose in monorepos",
            "summary": null,
            "content_text": "MonorepoAs I was joining TIER Mobility not only did I have to learn TypeScript quickly, but I also hadto learn how to work with a monorepo - something that first felt weird, but I got to love quickly.The idea is that instead of having a package for every one of your services with its pre-commit hooks,lint setups, prettier configs, ci/cd pipelines, and whatnot, every service is instead a packagein a single repository that is being orchestrated with yarn and Lerna. They share the same configurations,the same dependabot that keeps them up to date and you can even reference them from within each other.Do you have a logger that is used in all of your services? Have it as a package in the monorepo andimport it in your other services.Local development environmentOne way or another you will want to have your development environment mirror your deployment environment.For that purpose, and because it comes shipped alongside Docker, you will probably use docker compose CLIand up your stack like that.In our monorepo, we started with a single docker-compose.yaml at the top level of our monorepo. Wehave our database, our Redis, and maybe something like RabbitMQ, that we tend to use across our stackconsistently. All good and fine.Until..Our monorepo grew, and so did our stack.There we had a docker-compose.yml that up RabbitMQ, Tile38, Redis, some databases, and Kafka - a stack thatbrings even the biggest machine to hold (if you run lofi.cafe) next to it.So instead of having a single docker-compose.yml, I created a file for each service within its packageto only start what the service needed.Yarns task runner is pretty useful to set up each respective package.json with a up/down script.{  \"name\": \"my-service\",  \"scripts\": {    \"lint\": \"eslint --ext .ts src\",    \"test\": \"jest --testMatch '**/*.test.ts'\",    \"up\": \"docker compose up\",    \"down\": \"docker compose down\"  }}Now if you work on a service, you can use yarn workspace my-service up to start the development environment with the dependencies you need for your service, and not those of your monorepo.There is one more thingThis is already pretty helpful. However, something like Redis is more often used than not.That means that  redis:    image: redis:6.2.3    ports:      - 6379:6379occurs in almost every docker-compose.yml across the entire monorepo. That means duplication, and you willhave to make sure that the version is updated.So instead of duplicating a Redis service in every docker-compose file, today I learned thatcompose configurations can be shared across file and projects.// monorepo/docker-compose.ymlversion: \"3\"services:  redis:    image: redis:5.0.6    ports:      - 6379:6379  (...) // other shared dependencies// monorepo/packages/my-service/docker-compose.ymlversion: \"3\"services:  redis:    extends:      file: ../../docker-compose.yml      service: redis  (...) // other service dependenciesIn this setup, Redis is added to the top-level docker-compose file and is extended in the servicesdocker-compose file.If we now up the service with yarn workspace my-service up it will check for Redis in the top-levelup it, and continue to add the dependencies as they are defined in the services docker-compose.Do this sooner than later, and your monorepo is on course to grow indefinitely.",
            "content_html": "<p align=\"center\"><img src=\"/img/2021-05-05-docker-compose/docker-compose-monorepos.png\" alt=\"docker-compose-monorepos\" /></p><h2 id=\"monorepo\">Monorepo</h2><p>As I was joining TIER Mobility not only did I have to learn TypeScript quickly, but I also hadto learn how to work with a monorepo - something that first felt weird, but I got to love quickly.The idea is that instead of having a package for every one of your services with its pre-commit hooks,lint setups, prettier configs, ci/cd pipelines, and whatnot, every service is instead a packagein a single repository that is being orchestrated with yarn and Lerna. They share the same configurations,the same dependabot that keeps them up to date and you can even reference them from within each other.Do you have a <code class=\"language-plaintext highlighter-rouge\">logger</code> that is used in all of your services? Have it as a package in the monorepo andimport it in your other services.</p><h2 id=\"local-development-environment\">Local development environment</h2><p>One way or another you will want to have your development environment mirror your deployment environment.For that purpose, and because it comes shipped alongside Docker, you will probably use <code class=\"language-plaintext highlighter-rouge\">docker compose</code> CLIand <code class=\"language-plaintext highlighter-rouge\">up</code> your stack like that.In our monorepo, we started with a single <code class=\"language-plaintext highlighter-rouge\">docker-compose.yaml</code> at the top level of our monorepo. Wehave our database, our Redis, and maybe something like RabbitMQ, that we tend to use across our stackconsistently. All good and fine.</p><h2 id=\"until\">Until..</h2><p>Our monorepo grew, and so did our stack.</p><p>There we had a <code class=\"language-plaintext highlighter-rouge\">docker-compose.yml</code> that up RabbitMQ, Tile38, Redis, some databases, and Kafka - a stack thatbrings even the biggest machine to hold (if you run <a href=\"https://lofi.cafe\">lofi.cafe</a>) next to it.So instead of having a single <code class=\"language-plaintext highlighter-rouge\">docker-compose.yml</code>, I created a file for each service within its packageto only start what the service needed.Yarns task runner is pretty useful to set up each respective <code class=\"language-plaintext highlighter-rouge\">package.json</code> with a <code class=\"language-plaintext highlighter-rouge\">up</code>/<code class=\"language-plaintext highlighter-rouge\">down</code> script.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{  \"name\": \"my-service\",  \"scripts\": {    \"lint\": \"eslint --ext .ts src\",    \"test\": \"jest --testMatch '**/*.test.ts'\",    \"up\": \"docker compose up\",    \"down\": \"docker compose down\"  }}</code></pre></div></div><p>Now if you work on a service, you can use <code class=\"language-plaintext highlighter-rouge\">yarn workspace my-service up</code> to start the development environment with the dependencies you need for your service, and not those of your monorepo.</p><h2 id=\"there-is-one-more-thing\">There is one more thing</h2><p>This is already pretty helpful. However, something like Redis is more often used than not.That means that</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>  redis:    image: redis:6.2.3    ports:      - 6379:6379</code></pre></div></div><p>occurs in almost every <code class=\"language-plaintext highlighter-rouge\">docker-compose.yml</code> across the entire monorepo. That means duplication, and you willhave to make sure that the version is updated.So instead of duplicating a Redis service in every <code class=\"language-plaintext highlighter-rouge\">docker-compose</code> file, <strong>today I learned</strong> that<a href=\"https://docs.docker.com/compose/extends/\">compose configurations can be shared across file and projects</a>.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>// monorepo/docker-compose.ymlversion: \"3\"services:  redis:    image: redis:5.0.6    ports:      - 6379:6379  (...) // other shared dependencies</code></pre></div></div><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>// monorepo/packages/my-service/docker-compose.ymlversion: \"3\"services:  redis:    extends:      file: ../../docker-compose.yml      service: redis  (...) // other service dependencies</code></pre></div></div><p>In this setup, Redis is added to the top-level <code class=\"language-plaintext highlighter-rouge\">docker-compose</code> file and is extended in the services<code class=\"language-plaintext highlighter-rouge\">docker-compose</code> file.If we now up the service with <code class=\"language-plaintext highlighter-rouge\">yarn workspace my-service up</code> it will check for Redis in the top-levelup it, and continue to add the dependencies as they are defined in the services <code class=\"language-plaintext highlighter-rouge\">docker-compose</code>.</p><p>Do this sooner than later, and your monorepo is on course to grow indefinitely.</p>",
            "url": "https://iwpnd.pw//articles/2021-05/docker-compose-in-monorepos",
            
            
            
            "tags": ["til","monorepo","docker","docker compose"],
            
            "date_published": "2021-05-05T11:37:00+00:00",
            "date_modified": "2021-05-05T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2021-04/Tile38-in-memory-geodatabase",
            "title": "Tile38 in-memory geodatabase",
            "summary": null,
            "content_text": "For the longest time, it was everything about PostgreSQL and PostGIS. Building something that only remotely has to handle geo-data I had PostGIS on speed-dial. Requirement analysis of what I needed from the vastness of functionality that PostGIS packs? Nope. Never. Really.What it mostly boiled down to were point-in-polygon, radius searches and if generous a KNN nearby search for either points or polygons. Do you need to use PostGIS for that? Well, if you have a bunch of points or a bunch of polygons I would do best to consider PostGIS for geo-indexed searches. That is until a friend introduced me to Tile38.Tile38Tile38 is an open-source, in-memory geo data store, spatial index, and real-time geofence framework written by Josh Baker (@tidwall)There are some things to unpack here.In-memory geo datastoreYes, Tile38 works in memory like Redis, without Redis but with broadly understood Redis Protocol. So while PostGIS stores and reads data from disk, Tile38 reads and writes to and from memory. This makes it blazingly fast, as Disk I/O is removed almost entirely. Almost? Well, Tile38 by default stores every command it receives in an append-only file that can be used to restore state in case of failure.As Tile38 uses the Redis Protocol, using Tile38 comes naturally to everyone that previously worked with Redis.SET fleet truck1 POINT 33.5123 -112.2693This adds truck1 to a fleet collection as a POINT with latitude 33.4626 and longitude -112.1695.GET fleet truck1And GET returns truck1 from the fleet collection.Rtree spatial indexSpatial indexes are what make a spatial database. Without an index, any search would be a sequential scan - something everybody wants to avoid at all costs when querying a database. Put simply what a spatial index does, is it groups geometries (Points, LineStrings, Polygons) using a minimum bounding rectangle. If you now have a point database and want to know how many points are within a request polygon - instead of checking every possible point, it would compare if the minimum bounding rectangles overlap and only check those that are overlapping. Overly simplified, but that’s the gist (ha!) of it.Now let’s put this index to use.WITHIN fleet CIRCLE 33.462 -112.268 6000&gt; {    \"ok\": True,    \"elapsed\": \"48.8µs\",    \"objects\": [        {            \"object\": {                \"type\": \"Point\",                \"coordinates\": [                    -112.2693,                    33.5123                ]            },            \"id\": \"truck\"        }    ],    \"count\": 1,    \"cursor\": 0}This will check the Tile38 collection fleet for every item, that is within a CIRCLE with a center point of latitude 33.462, longitude -112.268, and a radius of 6000 meters.Realtime geofence frameworkPerforming geo-searches is mind-boggling. When I was first able to show a manager of PostGIS capabilities and speed, it blew their mind. And even today, some fellow developers stand in awe, when they request a feature from me, and I can wrap something up with PostGIS, in an instance that speeds up their services. But now imagine that instead of requesting whether a point is in a polygon from a database, the database would instead tell you when the point enters and exits the area. That is what a geofence is, and this is what Tile38 provides. This is just next-level and opens up so many possible applications.Adding a geofenced search to Tile38 is as easy as:SETHOOK warehouse http://10.1.3.37/endpoint NEARBY fleet FENCE POINT 33.5123, -112.2693 500This will make Tile38 send events to the given endpoint, whenever an object is in the collection fleet enters the area of a 500-meter radius around latitude 33.5123 and longitude -112.2693.Leader and follower replicationLeader-follower replication is something that you want for every database where reliability and availability are key. In most setups like that (e.g. AWS Aurora PostgreSQL) you will have a writer (leader) that will handle incoming CREATE and UPDATE statements. And on the other hand, you will have a variable amount of reader instances that share the load of incoming requests among each other. Tile38 offers a similar setup. Say you start two Tile38 instances and call FOLLOW leader-uri:9581, then one will become a leader, the other will be a follower. The follower will request the current state of the leader and once caught up, will be good to receive queries. A leader will still be able to handle incoming query commands, but a follower will reject SET commands going forward. If the leader dies, the followers will become stale until the leader is back up, but are still able to complete requests sent to them so your application will continue to work. Something you will have to make sure is that data sent to the leader during downtime is stored somewhere - something like Apache Kafka is a good candidate for that. This setup allows for a pretty fault-tolerant setup as it works like a charm with horizontal auto-scalers for example in Kubernetes. The auto-scaler will just add pods to your service when the load on all followers exceeds a set threshold. And trust me, this happens fast. Compare this to something like AWS Aurora with PostgreSQL, and you’re not only more reliable but also waaaay cheaper off using Tile38.Why is PostGIS still widely used then?Good question, with the most obvious answer being that it is plain awesome still, well understood, highly customizable, and offers functionality that is unrivaled by any other geodatabase out there. But Tile38 brought a viable alternative to PostGIS into the mix that for most applications is not only sufficient enough but also offers completely new use-cases in today’s modern event-driven architectures. The possibility to track IoT devices in space and being notified about their whereabouts is mind-bogglingly awesome. I build and will continue building applications with Tile38 going forward. But don’t worry PostGIS, there’s not a kid on the block that rivals you completely.",
            "content_html": "<p align=\"center\"><img src=\"/img/2021-04-tile38/tile38banner.png\" alt=\"tile38banner\" /></p><p>For the longest time, it was everything about PostgreSQL and PostGIS. Building something that only remotely has to handle geo-data I had PostGIS on speed-dial. Requirement analysis of what I needed from the vastness of functionality that PostGIS packs? Nope. Never. Really.What it mostly boiled down to were point-in-polygon, radius searches and if generous a KNN nearby search for either points or polygons. Do you need to use PostGIS for that? Well, if you have a bunch of points or a bunch of polygons I would do best to consider PostGIS for geo-indexed searches. That is until a friend introduced me to Tile38.</p><h2 id=\"tile38\">Tile38</h2><p><a href=\"https://tile38.com\">Tile38</a> is an open-source, in-memory geo data store, spatial index, and real-time geofence framework written by Josh Baker (<a href=\"https://github.com/tidwall\">@tidwall</a>)There are some things to unpack here.</p><h2 id=\"in-memory-geo-datastore\">In-memory geo datastore</h2><p>Yes, Tile38 works in memory like Redis, without Redis but with broadly understood Redis Protocol. So while PostGIS stores and reads data from disk, Tile38 reads and writes to and from memory. This makes it blazingly fast, as Disk I/O is removed almost entirely. Almost? Well, Tile38 by default stores every command it receives in an append-only file that can be used to restore state in case of failure.As Tile38 uses the Redis Protocol, using Tile38 comes naturally to everyone that previously worked with Redis.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>SET fleet truck1 POINT 33.5123 -112.2693</code></pre></div></div><p>This adds <code class=\"language-plaintext highlighter-rouge\">truck1</code> to a <code class=\"language-plaintext highlighter-rouge\">fleet</code> collection as a <code class=\"language-plaintext highlighter-rouge\">POINT</code> with latitude <code class=\"language-plaintext highlighter-rouge\">33.4626</code> and longitude <code class=\"language-plaintext highlighter-rouge\">-112.1695</code>.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>GET fleet truck1</code></pre></div></div><p>And <code class=\"language-plaintext highlighter-rouge\">GET</code> returns <code class=\"language-plaintext highlighter-rouge\">truck1</code> from the <code class=\"language-plaintext highlighter-rouge\">fleet</code> collection.</p><h2 id=\"rtree-spatial-index\">Rtree spatial index</h2><p>Spatial indexes are what make a spatial database. Without an index, any search would be a sequential scan - something everybody wants to avoid at all costs when querying a database. Put simply what a spatial index does, is it groups geometries (Points, LineStrings, Polygons) using a minimum bounding rectangle. If you now have a point database and want to know how many points are within a request polygon - instead of checking every possible point, it would compare if the minimum bounding rectangles overlap and only check those that are overlapping. Overly simplified, but that’s the gist (ha!) of it.Now let’s put this index to use.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>WITHIN fleet CIRCLE 33.462 -112.268 6000&gt; {    \"ok\": True,    \"elapsed\": \"48.8µs\",    \"objects\": [        {            \"object\": {                \"type\": \"Point\",                \"coordinates\": [                    -112.2693,                    33.5123                ]            },            \"id\": \"truck\"        }    ],    \"count\": 1,    \"cursor\": 0}</code></pre></div></div><p>This will check the Tile38 collection <code class=\"language-plaintext highlighter-rouge\">fleet</code> for every item, that is within a <code class=\"language-plaintext highlighter-rouge\">CIRCLE</code> with a center point of latitude <code class=\"language-plaintext highlighter-rouge\">33.462</code>, longitude <code class=\"language-plaintext highlighter-rouge\">-112.268</code>, and a radius of <code class=\"language-plaintext highlighter-rouge\">6000</code> meters.</p><h2 id=\"realtime-geofence-framework\">Realtime geofence framework</h2><p>Performing geo-searches is mind-boggling. When I was first able to show a manager of PostGIS capabilities and speed, it blew their mind. And even today, some fellow developers stand in awe, when they request a feature from me, and I can wrap something up with PostGIS, in an instance that speeds up their services. But now imagine that instead of requesting whether a point is in a polygon from a database, the database would instead tell you when the point enters and exits the area. That is what a geofence is, and this is what Tile38 provides. This is just next-level and opens up so many possible applications.Adding a geofenced search to Tile38 is as easy as:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>SETHOOK warehouse http://10.1.3.37/endpoint NEARBY fleet FENCE POINT 33.5123, -112.2693 500</code></pre></div></div><p>This will make Tile38 send events to the given endpoint, whenever an object is in the collection <code class=\"language-plaintext highlighter-rouge\">fleet</code> enters the area of a <code class=\"language-plaintext highlighter-rouge\">500</code>-meter radius around latitude <code class=\"language-plaintext highlighter-rouge\">33.5123</code> and longitude <code class=\"language-plaintext highlighter-rouge\">-112.2693</code>.</p><h2 id=\"leader-and-follower-replication\">Leader and follower replication</h2><p>Leader-follower replication is something that you want for every database where reliability and availability are key. In most setups like that (e.g. AWS Aurora PostgreSQL) you will have a writer (leader) that will handle incoming CREATE and UPDATE statements. And on the other hand, you will have a variable amount of reader instances that share the load of incoming requests among each other. Tile38 offers a similar setup. Say you start two Tile38 instances and call <code class=\"language-plaintext highlighter-rouge\">FOLLOW leader-uri:9581</code>, then one will become a leader, the other will be a follower. The follower will request the current state of the leader and once caught up, will be good to receive queries. A leader will still be able to handle incoming query commands, but a follower will reject <code class=\"language-plaintext highlighter-rouge\">SET</code> commands going forward. If the leader dies, the followers will become stale until the leader is back up, but are still able to complete requests sent to them so your application will continue to work. Something you will have to make sure is that data sent to the leader during downtime is stored somewhere - something like Apache Kafka is a good candidate for that. This setup allows for a pretty fault-tolerant setup as it works like a charm with horizontal auto-scalers for example in Kubernetes. The auto-scaler will just add pods to your service when the load on all followers exceeds a set threshold. And trust me, this happens fast. Compare this to something like AWS Aurora with PostgreSQL, and you’re not only more reliable but also waaaay cheaper off using Tile38.</p><h2 id=\"why-is-postgis-still-widely-used-then\">Why is PostGIS still widely used then?</h2><p>Good question, with the most obvious answer being that it is plain awesome still, well understood, highly customizable, and offers functionality that is unrivaled by any other geodatabase out there. But Tile38 brought a viable alternative to PostGIS into the mix that for most applications is not only sufficient enough but also offers completely new use-cases in today’s modern event-driven architectures. The possibility to track IoT devices in space and being notified about their whereabouts is mind-bogglingly awesome. I build and will continue building applications with Tile38 going forward. But don’t worry PostGIS, there’s not a kid on the block that rivals you completely.</p>",
            "url": "https://iwpnd.pw//articles/2021-04/Tile38-in-memory-geodatabase",
            
            
            
            "tags": ["tile38","database","geodata"],
            
            "date_published": "2021-04-24T11:37:00+00:00",
            "date_modified": "2021-04-24T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-10/TIL-destructuring-reassignment-typescript",
            "title": "Typescript destructuring re-assignment",
            "summary": null,
            "content_text": "Completely unnecessary, but if you ever want to re-assign a variable from within destructuring, here you go:Destructuring needs to be either after a let, const or var declaration OR it needs to be in an expression context to distinguish it from a block statement.let variable: numberif (true) {\t({ bla: variable } = someFunction());} else {\t({ bla: variable } = someOtherFunction());}",
            "content_html": "<p>Completely unnecessary, but if you ever want to re-assign a variable from within destructuring, here you go:</p><p>Destructuring needs to be either after a <code class=\"language-plaintext highlighter-rouge\">let</code>, <code class=\"language-plaintext highlighter-rouge\">const</code> or <code class=\"language-plaintext highlighter-rouge\">var</code> declaration <strong>OR</strong> it needs to be <strong>in an expression context</strong> to distinguish it from a block statement.</p><div class=\"language-tsx highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">let</span> <span class=\"nx\">variable</span><span class=\"p\">:</span> <span class=\"kr\">number</span><span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"kc\">true</span><span class=\"p\">)</span> <span class=\"p\">{</span>\t<span class=\"p\">({</span> <span class=\"na\">bla</span><span class=\"p\">:</span> <span class=\"nx\">variable</span> <span class=\"p\">}</span> <span class=\"o\">=</span> <span class=\"nx\">someFunction</span><span class=\"p\">());</span><span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\t<span class=\"p\">({</span> <span class=\"na\">bla</span><span class=\"p\">:</span> <span class=\"nx\">variable</span> <span class=\"p\">}</span> <span class=\"o\">=</span> <span class=\"nx\">someOtherFunction</span><span class=\"p\">());</span><span class=\"p\">}</span></code></pre></div></div>",
            "url": "https://iwpnd.pw//articles/2020-10/TIL-destructuring-reassignment-typescript",
            
            
            
            "tags": ["til","typescript"],
            
            "date_published": "2020-10-07T07:37:00+00:00",
            "date_modified": "2020-10-07T07:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-09/TIL-degree-precision-vs-length",
            "title": "Degree Precision vs. Length",
            "summary": null,
            "content_text": "Trust me. It’s okay to truncate your GPS updates coordinates.var Point = [24.9322965651688, 60.1921775614599]decimalplaces   degrees          distance-------  -------          --------0        1                111  km1        0.1              11.1 km2        0.01             1.11 km3        0.001            111  m4        0.0001           11.1 m5        0.00001          1.11 m6        0.000001         11.1 cm7        0.0000001        1.11 cm8        0.00000001       1.11 mm9        0.000000001      111  μm10       0.0000000001     11.1 μm11       0.00000000001    1.11 μm12       0.000000000001   111  nm13       0.0000000000001  11.1 nm",
            "content_html": "<p>Trust me. It’s okay to truncate your GPS updates coordinates.</p><div class=\"language-jsx highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">var</span> <span class=\"nx\">Point</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">24.9322965651688</span><span class=\"p\">,</span> <span class=\"mf\">60.1921775614599</span><span class=\"p\">]</span></code></pre></div></div><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>decimalplaces   degrees          distance-------  -------          --------0        1                111  km1        0.1              11.1 km2        0.01             1.11 km3        0.001            111  m4        0.0001           11.1 m5        0.00001          1.11 m6        0.000001         11.1 cm7        0.0000001        1.11 cm8        0.00000001       1.11 mm9        0.000000001      111  μm10       0.0000000001     11.1 μm11       0.00000000001    1.11 μm12       0.000000000001   111  nm13       0.0000000000001  11.1 nm</code></pre></div></div>",
            "url": "https://iwpnd.pw//articles/2020-09/TIL-degree-precision-vs-length",
            
            
            
            "tags": ["til","postgis"],
            
            "date_published": "2020-09-10T07:37:00+00:00",
            "date_modified": "2020-09-10T07:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-08/TIL-spatially-cluster-postgis-tables",
            "title": "Spatially cluster PostGIS tables",
            "summary": null,
            "content_text": "If your query pattern evolves around getting geometries that are often spatially close together it might make sense to cluster the table on a spatial correlated field.-- first create a functional index on the geohash of your geometrycreate index idx_polygon_geohash on zone (st_geohash(polygon));-- vacuum and analyze your dbvacuum analyze zone;-- cluster the table on that newly created indexcluster zone using idx_cl_polygon_geohash;performance gains heavily depend on the size of the response payload and the query pattern.see: here",
            "content_html": "<p>If your query pattern evolves around getting geometries that are often spatially close together it might make sense to <code class=\"language-plaintext highlighter-rouge\">cluster</code> the table on a spatial correlated field.</p><div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">-- first create a functional index on the geohash of your geometry</span><span class=\"k\">create</span> <span class=\"k\">index</span> <span class=\"n\">idx_polygon_geohash</span> <span class=\"k\">on</span> <span class=\"k\">zone</span> <span class=\"p\">(</span><span class=\"n\">st_geohash</span><span class=\"p\">(</span><span class=\"n\">polygon</span><span class=\"p\">));</span><span class=\"c1\">-- vacuum and analyze your db</span><span class=\"k\">vacuum</span> <span class=\"k\">analyze</span> <span class=\"k\">zone</span><span class=\"p\">;</span><span class=\"c1\">-- cluster the table on that newly created index</span><span class=\"k\">cluster</span> <span class=\"k\">zone</span> <span class=\"k\">using</span> <span class=\"n\">idx_cl_polygon_geohash</span><span class=\"p\">;</span></code></pre></div></div><p>performance gains heavily depend on the size of the response payload and the query pattern.</p><p>see: <a href=\"http://postgis.net/workshops/postgis-intro/clusterindex.html\">here</a></p>",
            "url": "https://iwpnd.pw//articles/2020-08/TIL-spatially-cluster-postgis-tables",
            
            
            
            "tags": ["til","postgis"],
            
            "date_published": "2020-08-05T07:37:00+00:00",
            "date_modified": "2020-08-05T07:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-07/TIL-howto-import-sqldump-into-docker-postgis",
            "title": "How to import an sql dump into a database that is running in a container",
            "summary": null,
            "content_text": "First you get the volume that is attached to the container your database is running in.docker inspect -f '' db-name | python -m json.tool&gt;&gt;[    {        \"Type\": \"volume\",        \"Name\": \"99b4bbceeb44156d1b8a800f1e1ba3bdd9d381bf46de383acccf92a6f813b7c7\",        \"Source\": \"/var/lib/docker/volumes/99b4bbceeb44156d1b8a800f1e1ba3bdd9d381bf46de383acccf92a6f813b7c7/_data\",        \"Destination\": \"/var/lib/postgresql/data\",        \"Driver\": \"local\",        \"Mode\": \"\",        \"RW\": true,        \"Propagation\": \"\"    }]Afterwards you get the id of the container your db is running indocker ps -aqf \"name=^db-name$\"&gt;&gt; 503ca8bf7a07Now you can copy your dump.sql using this:docker cp dump.sql &lt;container-id&gt;:/var/lib/postgresql/dataORdo it in one go using this:docker cp dump.sql $(docker ps -aqf \"name=^db-name$\"):/var/lib/postgresql/dataThen usedocker-compose exec db-name bashto get a bash console in you db container and execpsql -U username db-name &lt; var/lib/postgresql/data/dump.sql",
            "content_html": "<p>First you get the volume that is attached to the container your database is running in.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>docker inspect -f '' db-name | python -m json.tool&gt;&gt;[    {        \"Type\": \"volume\",        \"Name\": \"99b4bbceeb44156d1b8a800f1e1ba3bdd9d381bf46de383acccf92a6f813b7c7\",        \"Source\": \"/var/lib/docker/volumes/99b4bbceeb44156d1b8a800f1e1ba3bdd9d381bf46de383acccf92a6f813b7c7/_data\",        \"Destination\": \"/var/lib/postgresql/data\",        \"Driver\": \"local\",        \"Mode\": \"\",        \"RW\": true,        \"Propagation\": \"\"    }]</code></pre></div></div><p>Afterwards you get the <code class=\"language-plaintext highlighter-rouge\">id</code> of the container your db is running in</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>docker ps -aqf \"name=^db-name$\"&gt;&gt; 503ca8bf7a07</code></pre></div></div><p>Now you can copy your <code class=\"language-plaintext highlighter-rouge\">dump.sql</code> using this:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>docker cp dump.sql &lt;container-id&gt;:/var/lib/postgresql/data</code></pre></div></div><p>OR</p><p>do it in one go using this:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>docker cp dump.sql $(docker ps -aqf \"name=^db-name$\"):/var/lib/postgresql/data</code></pre></div></div><p>Then use</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>docker-compose exec db-name bash</code></pre></div></div><p>to get a bash console in you db container and exec</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>psql -U username db-name &lt; var/lib/postgresql/data/dump.sql</code></pre></div></div>",
            "url": "https://iwpnd.pw//articles/2020-07/TIL-howto-import-sqldump-into-docker-postgis",
            
            
            
            "tags": ["til","postgis"],
            
            "date_published": "2020-07-01T07:37:00+00:00",
            "date_modified": "2020-07-01T07:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-07/TIL-get-unused-indices-of-your-postgis",
            "title": "Get unused indices of your PostGIS database",
            "summary": null,
            "content_text": "Unused indices are the bane of your production database as even though they are unused, they have to be rebuild and cleaned along the lifetime of a table.To quickly validate if the index you’ve created on design time are actually used at run time, you can simply use this handy little snipped to clean up your indices.SELECT \tui.schemaname,       \tui.relname \t\tAS table_name,       \tui.indexrelname \tAS index_name,       \tpg_relation_size(ui.indexrelid) AS index_sizeFROM \tpg_catalog.pg_stat_user_indexes ui,\tpg_catalog.pg_index piWHERE \tui.idx_scan = 0      -- has never been scannedAND    \tui.indexrelid = pi.indexrelid  AND \t0 &lt;&gt;ALL (pi.indkey)  -- no index column is an expressionAND NOT pi.indisunique   -- is not a UNIQUE indexAND NOT EXISTS          -- does not enforce a constraint         \t(\tSELECT 1 \tFROM pg_catalog.pg_constraint pc        WHERE pc.conindid = ui.indexrelid)ORDER BY table_name, pg_relation_size(ui.indexrelid) DESCsource: AWS",
            "content_html": "<p>Unused indices are the bane of your production database as even though they are unused, they have to be rebuild and cleaned along the lifetime of a table.</p><p>To quickly validate if the index you’ve created on design time are actually used at run time, you can simply use this handy little snipped to clean up your indices.</p><div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">SELECT</span> \t<span class=\"n\">ui</span><span class=\"p\">.</span><span class=\"n\">schemaname</span><span class=\"p\">,</span>       \t<span class=\"n\">ui</span><span class=\"p\">.</span><span class=\"n\">relname</span> \t\t<span class=\"k\">AS</span> <span class=\"k\">table_name</span><span class=\"p\">,</span>       \t<span class=\"n\">ui</span><span class=\"p\">.</span><span class=\"n\">indexrelname</span> \t<span class=\"k\">AS</span> <span class=\"n\">index_name</span><span class=\"p\">,</span>       \t<span class=\"n\">pg_relation_size</span><span class=\"p\">(</span><span class=\"n\">ui</span><span class=\"p\">.</span><span class=\"n\">indexrelid</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"n\">index_size</span><span class=\"k\">FROM</span> \t<span class=\"n\">pg_catalog</span><span class=\"p\">.</span><span class=\"n\">pg_stat_user_indexes</span> <span class=\"n\">ui</span><span class=\"p\">,</span>\t<span class=\"n\">pg_catalog</span><span class=\"p\">.</span><span class=\"n\">pg_index</span> <span class=\"n\">pi</span><span class=\"k\">WHERE</span> \t<span class=\"n\">ui</span><span class=\"p\">.</span><span class=\"n\">idx_scan</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>      <span class=\"c1\">-- has never been scanned</span><span class=\"k\">AND</span>    \t<span class=\"n\">ui</span><span class=\"p\">.</span><span class=\"n\">indexrelid</span> <span class=\"o\">=</span> <span class=\"n\">pi</span><span class=\"p\">.</span><span class=\"n\">indexrelid</span>  <span class=\"k\">AND</span> \t<span class=\"mi\">0</span> <span class=\"o\">&lt;&gt;</span><span class=\"k\">ALL</span> <span class=\"p\">(</span><span class=\"n\">pi</span><span class=\"p\">.</span><span class=\"n\">indkey</span><span class=\"p\">)</span>  <span class=\"c1\">-- no index column is an expression</span><span class=\"k\">AND</span> <span class=\"k\">NOT</span> <span class=\"n\">pi</span><span class=\"p\">.</span><span class=\"n\">indisunique</span>   <span class=\"c1\">-- is not a UNIQUE index</span><span class=\"k\">AND</span> <span class=\"k\">NOT</span> <span class=\"k\">EXISTS</span>          <span class=\"c1\">-- does not enforce a constraint</span>         \t<span class=\"p\">(</span>\t<span class=\"k\">SELECT</span> <span class=\"mi\">1</span> \t<span class=\"k\">FROM</span> <span class=\"n\">pg_catalog</span><span class=\"p\">.</span><span class=\"n\">pg_constraint</span> <span class=\"n\">pc</span>        <span class=\"k\">WHERE</span> <span class=\"n\">pc</span><span class=\"p\">.</span><span class=\"n\">conindid</span> <span class=\"o\">=</span> <span class=\"n\">ui</span><span class=\"p\">.</span><span class=\"n\">indexrelid</span><span class=\"p\">)</span><span class=\"k\">ORDER</span> <span class=\"k\">BY</span> <span class=\"k\">table_name</span><span class=\"p\">,</span> <span class=\"n\">pg_relation_size</span><span class=\"p\">(</span><span class=\"n\">ui</span><span class=\"p\">.</span><span class=\"n\">indexrelid</span><span class=\"p\">)</span> <span class=\"k\">DESC</span></code></pre></div></div><p>source: <a href=\"https://aws.amazon.com/blogs/database/reducing-aurora-postgresql-storage-i-o-costs/\">AWS</a></p>",
            "url": "https://iwpnd.pw//articles/2020-07/TIL-get-unused-indices-of-your-postgis",
            
            
            
            "tags": ["til","postgis"],
            
            "date_published": "2020-07-01T07:37:00+00:00",
            "date_modified": "2020-07-01T07:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-07/TIL-create-functional-index-in-postgis",
            "title": "Create a functional index in PostGIS",
            "summary": null,
            "content_text": "It possible to set an index on the return value of a function.Say you’re casting a geometry to geography a lot. You have an GIST on the geometry, but that index is not utilized when you cast the geometry to geography.What you can do to circumvent that without creating a geography column and index that, is to create the index on the return of CAST(your_geometry AS geography) or geography(your_geometry_column.create index idx_cast_geography_on_your_geometry_column on your_table using GIST (\tgeography(your_geometry_column));source: gis.stackexchange.com",
            "content_html": "<p>It possible to set an index on the return value of a function.</p><p>Say you’re casting a <code class=\"language-plaintext highlighter-rouge\">geometry</code> to <code class=\"language-plaintext highlighter-rouge\">geography</code> a lot. You have an <code class=\"language-plaintext highlighter-rouge\">GIST</code> on the <code class=\"language-plaintext highlighter-rouge\">geometry</code>, but that index is not utilized when you cast the <code class=\"language-plaintext highlighter-rouge\">geometry</code> to <code class=\"language-plaintext highlighter-rouge\">geography</code>.</p><p>What you can do to circumvent that without creating a <code class=\"language-plaintext highlighter-rouge\">geography</code> column and index that, is to create the index on the return of <code class=\"language-plaintext highlighter-rouge\">CAST(your_geometry AS geography)</code> or <code class=\"language-plaintext highlighter-rouge\">geography(your_geometry_column</code>.</p><div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">create</span> <span class=\"k\">index</span> <span class=\"n\">idx_cast_geography_on_your_geometry_column</span> <span class=\"k\">on</span> <span class=\"n\">your_table</span> <span class=\"k\">using</span> <span class=\"n\">GIST</span> <span class=\"p\">(</span>\t<span class=\"n\">geography</span><span class=\"p\">(</span><span class=\"n\">your_geometry_column</span><span class=\"p\">)</span><span class=\"p\">);</span></code></pre></div></div><p>source: <a href=\"https://gis.stackexchange.com/a/247131\">gis.stackexchange.com</a></p>",
            "url": "https://iwpnd.pw//articles/2020-07/TIL-create-functional-index-in-postgis",
            
            
            
            "tags": ["til","postgis"],
            
            "date_published": "2020-07-01T07:37:00+00:00",
            "date_modified": "2020-07-01T07:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-05/part1-docker-swarm-on-ec2-initial-setup",
            "title": "Docker Swarm on AWS EC2 Part 1&#58; Initial provisioning and setup",
            "summary": null,
            "content_text": "If you work with data, chances are, you came across or worked with a compute cluster of some sorts. There is not a conference, meetup, GitHub repository, or whatnot that doesn’t mention Docker containers and/or their orchestration. If you’ve read some of my blog posts you know that I too, try to develop in and with Docker containers. But I have never actually deployed an application to a compute cluster other than a local setup Docker Swarm. Let’s change that, shall we?What will we learn in this series?In this series, we’re going to find answers to some questions like:How do I provision and set up an AWS EC2 instance?How do I set up a cluster in the cloud? How do I route my applications? How do I manage my applications How do I even do CI/CD with a Docker Swarm?The first part of this series will focus on the initial steps of the setup. Like how to set up a Virtual Private Cloud (VPC) and attach security groups. I will also show how to launch an AWS EC2 instance and do the necessary steps to set up a Docker Swarm on it.In the second part of the series I will show how to set up a Traefik Proxy with HTTPS that will handle incoming connections, expose your services and applications based on their domain name, manage multiple domains, handle and acquire HTTPS certificates with Let’s Encrypt. I will also show how to set up Portainer) as the first service.Part three will focus on how to deploy services to your Docker Swarm in a CI/CD pipeline using GitHub and GitHub Actions.What will I not cover in this series?I love automation. However, what I will not be doing, is showing you how to set up this cluster using Terraform or AWS Cloudformation (yet). I feel doing all of this from scratch, even if you only do it once, will go a long way when it comes to understanding it.PrerequisitesYou will need to have access to the AWS management console. Your user will also need permission to create resources within AWS e.g. a VPC, security groups, EC2 instances. For this exercise, I will be using macOS. Windows users will have to read up on how to set up and ssh into a remote server, e.g. via PuTTY.Virtual Private CloudAs Elastic Compute Resources (EC2 Instances) can only be launched inside a Virtual Private Cloud (VPC), we will need a VPC. Accounts older than 04.12.2013 will have to create a Virtual Private Cloud (VPC), as described here. Newer accounts already have a default VPC setup that we can launch the EC2 instance for our Docker Swarm into.Launch an EC2 instanceWith the prerequisites out of the way, we can go ahead and launch our first instance. In the AWS Management Console go to Services &gt; Compute &gt; EC2 and spank that Launch button.Choose an Amazon Machine Image (AMI)First, you will have to select an AMI to run on your instance. I opted for Amazon Linux 2 AMI (HVM) 64-bit (x86) for no particular reason. It’s free tier eligible, if you still can make use of that, and is built by Amazon to have optimal performance on EC2. You can use another AMI that suits you. Just make sure to adapt things like package management that I will be using in this post to the OS of your choice.Choose an Instance TypeNext up, you’re prompted to select an instance type. Now, this is where you can either go all out or start reasonably small. I was planning to use the Docker Swarm cluster for small services that I want to test in a production environment. I do not expect any users, nor do I run computational heavy stuff on it. You can, however, that is up to you and your budget. A t2.small is roughly 18,00 Euro a month including storage and traffic if it’s running 24/7 on eu-west-1. If you registered an account with AWS because you’re on a learning path, then you will most likely still be eligible for the free tier, which means that t2.micro instance is free of charge for a year. A t2.micro, for this tutorial, is all we need.Configure your InstanceThere is not much to change here. If you’re just starting with your AWS account the Network will be prepopulated with your VPC already. The subnet can also stay on default. We want to, however, enable Auto-assign Public IP. If you plan to create resources on AWS with this particular EC2 instance, e.g. create an Amazon S3 Bucket, you will need to attach an IAM role to your instance for the proper permissions set up. In our case, we will not create any other resources WITH the EC2 Instance, so we leave that blank. Stop - Hibernate behavior is interesting, if you plan on often shutting an instance down if you don’t use it, and bring it back up when you use it. In this case, on Stop all your instances Files and RAM will be dumped to an attached file system and you can use this if you bring it up another time to have everything where you left it off. Keep in mind, however, that you will still pay for the attached storage 24/7. Monitoring is also pretty good with CloudWatch. It does come with a cost, however, and since we will utilize Traefik and Portainer, we will have our monitoring in place. Leave the rest as seen in the Screenshot and continue with Next: Add Storage.Add StoragePretty much just leave as is, unless you need more storage or have other requirements that you think will not be fulfilled with the default settings. ContinueAdd TagsThis EC2 instance will function as the Manager Node of your Docker Swarm Cluster that receives commands on behalf of the cluster and assign containers to other Swarm nodes (if you plan on having more than 1). Tags will only help you to distinguish your instance from each other in the AWS Management Console. It will not have an effect on your Instances hostname or whatever. Do yourself a favor and Tag your AWS resources and continue.Configure Security GroupAn AWS security group acts as a virtual firewall for your AWS resources inside of your VPC. As such it is highly recommended to read up on some best practices for setting up security groups and how to manage each layer of your infrastructure properly using security groups (e.g. best practices, AWS intro on security groups). For this demo, we need to set up a security group that is open to inbound traffic from a couple of ports and protocols. You can set it up right now or in the Management Console go to Services &gt; search for VPC &gt; under Security go to Security Groups and hit Create Security Group. Give it a significant name and description add inbound rules for:            Protocol      Port      Source                  TCP      2377      0.0.0.0/0              TCP      7946      0.0.0.0/0              TCP      8501      0.0.0.0/0              UDP      4789      0.0.0.0/0              UDP      7946      0.0.0.0/0              SSH      7946      0.0.0.0/0              HTTPS      443      0.0.0.0/0              HTTP      80      0.0.0.0/0      Now make sure to attach the newly created security group to your instance and continue.Review your instance and launchUpon launch, you will be asked to create a keypair, a private key, and a public key. AWS EC2 uses public-key cryptography to encrypt and decrypt login information and therefore allows you to access your instance using a private key instead of a password. For a detailed description of the concept of Public Key Authentification, I refer you to ssh.com.AWS will ask you to name and create a key pair. The private key will be stored in a .pem file and should be downloaded to a secure location on your local file system or trusted storage of your choice. I store it in ~/.ssh/. If you create an EC2 instance only this file/this key will let you access it. AWS does not store a duplicate it anywhere, so if you have lost it, you lost it and you will have to shut down your instance for good. The other part, the public key, will be stored on your Linux EC2 instance in ~/.ssh/authorized_keys.Congratulations, you just set up your first EC2 instanceNow open a terminal, you will need it from now on.Configure SSH to access your instanceFor convenience’s sake, I create a ~/.ssh/config file and store my remote instances information there for easy access from the terminal.vim ~/.ssh/configFor your hostname, you will set up the Public DNS (IPv4) of your EC2 instance. The User depends on your OS. Fox Amazon Linux 2 it’s ec2-user, for other AMIs you can look up the default user here.  Your IdentityFile will be the .pem file with your private key you downloaded earlier. Save and close the file :wq!.ssh swarmChange your Instances hostnameFirst, we will change the hostname of the instance using a subdomain of a domain you plan on using for your swarm. Changing the hostname will require you to reboot your instance. Don’t worry, if you’re only in it for this part of the tutorial and only want to set up your first swarm for the lulz, you can skip this step.sudo hostnamectl set-hostname webserver.mydomain.comsudo rebootWait for your instance to reboot, ssh back in, and check the result using echo $HOSTNAME.Install and setup DockerNow, this part comes down to the OS you chose for your AMI. Amazon Linux 2 AMI uses yum package manager. Adapt the next steps to your chosen OS.# update sudo yum update# install docker sudo yum install docker# start dockersudo service docker start# check if its installed properlydocker --version# add ec2-user to docker group to allow docker commands without sudosudo usermod -a -G docker ec2-user# add docker to autostart and reboot to see if it's workingsudo chkconfig docker onsudo reboot# test user rights and autostartdocker versionInit your Docker swarmWith the setup of Docker out of the way, we can finally initiate our Docker Swarm.docker swarm initThis will greet you with this:If you want to add another node to your cluster, you can go back to the AWS Management Console, and do all of this again. If that EC2 instance is ready and docker installed, you can just use the command in the screenshot above to join this new instance to your Docker swarm as a worker node.Now you can check the nodes in your swarm via:docker node lsAs you can see I only have set up a single node, which is my leading node - the manager.Congratulations! You just set up your first Docker Swarm on AWS EC2!The next part will focus on how to setup Traefik and Portainer) to handle connections in your cluster, manage HTTPS certificates and routing as well as monitoring your Docker Swarm.",
            "content_html": "<p>If you work with data, chances are, you came across or worked with a compute cluster of some sorts. There is not a conference, meetup, GitHub repository, or whatnot that doesn’t mention Docker containers and/or their orchestration. If you’ve read some of my blog posts you know that I too, try to develop in and with Docker containers. But I have never actually deployed an application to a compute cluster other than a local setup Docker Swarm. Let’s change that, shall we?</p><h2 id=\"what-will-we-learn-in-this-series\">What will we learn in this series?</h2><p>In this series, we’re going to find answers to some questions like:</p><p>How do I provision and set up an AWS EC2 instance?How do I set up a cluster in the cloud? How do I route my applications? How do I manage my applications How do I even do CI/CD with a Docker Swarm?</p><p>The first part of this series will focus on the initial steps of the setup. Like how to set up a Virtual Private Cloud (VPC) and attach security groups. I will also show how to launch an AWS EC2 instance and do the necessary steps to set up a Docker Swarm on it.</p><p>In the second part of the series I will show how to set up a <a href=\"https://containo.us/traefik/\">Traefik</a> Proxy with HTTPS that will handle incoming connections, expose your services and applications based on their domain name, manage multiple domains, handle and acquire HTTPS certificates with <a href=\"https://letsencrypt.org/\">Let’s Encrypt</a>. I will also show how to set up <a href=\"https://www.portainer.io/\">Portainer</a>) as the first service.</p><p>Part three will focus on how to deploy services to your Docker Swarm in a CI/CD pipeline using <a href=\"https://github.com\">GitHub</a> and <a href=\"https://github.com/features/actions\">GitHub Actions</a>.</p><h2 id=\"what-will-i-not-cover-in-this-series\">What will I not cover in this series?</h2><p>I love automation. However, what I will not be doing, is showing you how to set up this cluster using Terraform or AWS Cloudformation (yet). I feel doing all of this from scratch, even if you only do it once, will go a long way when it comes to understanding it.</p><h2 id=\"prerequisites\">Prerequisites</h2><p>You will need to have access to the AWS management console. Your user will also need permission to create resources within AWS e.g. a VPC, security groups, EC2 instances. For this exercise, I will be using macOS. Windows users will have to read up on how to set up and ssh into a remote server, e.g. via <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html\">PuTTY</a>.</p><h3 id=\"virtual-private-cloud\">Virtual Private Cloud</h3><p>As Elastic Compute Resources (EC2 Instances) can only be launched inside a <a href=\"https://aws.amazon.com/vpc/\">Virtual Private Cloud (VPC)</a>, we will need a VPC. Accounts older than 04.12.2013 will have to create a <a href=\"https://aws.amazon.com/vpc/\">Virtual Private Cloud (VPC)</a>, as described <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html#create-default-vpc\">here</a>. Newer accounts already have a default VPC setup that we can launch the EC2 instance for our Docker Swarm into.</p><h2 id=\"launch-an-ec2-instance\">Launch an EC2 instance</h2><p>With the prerequisites out of the way, we can go ahead and launch our first instance. In the AWS Management Console go to Services &gt; Compute &gt; EC2 and spank that Launch button.</p><h3 id=\"choose-an-amazon-machine-image-ami\">Choose an Amazon Machine Image (AMI)</h3><p>First, you will have to select an AMI to run on your instance. I opted for Amazon Linux 2 AMI (HVM) 64-bit (x86) for no particular reason. It’s free tier eligible, if you still can make use of that, and is built by Amazon to have optimal performance on EC2. You can use another AMI that suits you. Just make sure to adapt things like package management that I will be using in this post to the OS of your choice.</p><h3 id=\"choose-an-instance-type\">Choose an Instance Type</h3><p>Next up, you’re prompted to select an instance type. Now, this is where you can either go all out or start reasonably small. I was planning to use the Docker Swarm cluster for small services that I want to test in a production environment. I do not expect any users, nor do I run computational heavy stuff on it. You can, however, that is up to you and your budget. A <code class=\"language-plaintext highlighter-rouge\">t2.small</code> is roughly 18,00 Euro a month including storage and traffic if it’s running 24/7 on eu-west-1. If you registered an account with AWS because you’re on a learning path, then you will most likely still be eligible for the free tier, which means that <code class=\"language-plaintext highlighter-rouge\">t2.micro</code> instance is free of charge for a year. A <code class=\"language-plaintext highlighter-rouge\">t2.micro</code>, for this tutorial, is all we need.</p><h3 id=\"configure-your-instance\">Configure your Instance</h3><p align=\"center\"><img src=\"/img/2020-05-docker-swarm/aws-ec2-config.png\" alt=\"aws ec2 configuration\" /></p><p>There is not much to change here. If you’re just starting with your AWS account the Network will be prepopulated with your VPC already. The subnet can also stay on default. We want to, however, <strong>enable</strong> <strong>Auto-assign Public IP</strong>. If you plan to create resources on AWS with this particular EC2 instance, e.g. create an Amazon S3 Bucket, you will need to attach an IAM role to your instance for the proper permissions set up. In our case, we will not create any other resources WITH the EC2 Instance, so we leave that blank. <strong>Stop - Hibernate behavior</strong> is interesting, if you plan on often shutting an instance down if you don’t use it, and bring it back up when you use it. In this case, on <strong>Stop</strong> all your instances Files and RAM will be dumped to an attached file system and you can use this if you bring it up another time to have everything where you left it off. Keep in mind, however, that you will still pay for the attached storage 24/7. Monitoring is also pretty good with CloudWatch. It does come with a cost, however, and since we will utilize <a href=\"https://containo.us/traefik/\">Traefik</a> and <a href=\"https://www.portainer.io/\">Portainer</a>, we will have our monitoring in place. Leave the rest as seen in the Screenshot and continue with <strong>Next: Add Storage</strong>.</p><h3 id=\"add-storage\">Add Storage</h3><p>Pretty much just leave as is, unless you need more storage or have other requirements that you think will not be fulfilled with the default settings. Continue</p><h3 id=\"add-tags\">Add Tags</h3><p>This EC2 instance will function as the Manager Node of your Docker Swarm Cluster that receives commands on behalf of the cluster and assign containers to other Swarm nodes (if you plan on having more than 1). Tags will only help you to distinguish your instance from each other in the AWS Management Console. It will not have an effect on your Instances hostname or whatever. Do yourself a favor and Tag your AWS resources and continue.</p><h3 id=\"configure-security-group\">Configure Security Group</h3><p>An AWS security group acts as a virtual firewall for your AWS resources inside of your VPC. As such it is highly recommended to read up on some best practices for setting up security groups and how to manage each layer of your infrastructure properly using security groups (e.g. <a href=\"https://www.stratoscale.com/blog/compute/aws-security-groups-5-best-practices/\">best practices</a>, <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">AWS intro on security groups</a>). For this demo, we need to set up a security group that is open to inbound traffic from a couple of ports and protocols. You can set it up right now or in the Management Console go to Services &gt; search for VPC &gt; under Security go to Security Groups and hit Create Security Group. Give it a significant name and description add inbound rules for:</p><table>  <thead>    <tr>      <th>Protocol</th>      <th>Port</th>      <th>Source</th>    </tr>  </thead>  <tbody>    <tr>      <td>TCP</td>      <td>2377</td>      <td>0.0.0.0/0</td>    </tr>    <tr>      <td>TCP</td>      <td>7946</td>      <td>0.0.0.0/0</td>    </tr>    <tr>      <td>TCP</td>      <td>8501</td>      <td>0.0.0.0/0</td>    </tr>    <tr>      <td>UDP</td>      <td>4789</td>      <td>0.0.0.0/0</td>    </tr>    <tr>      <td>UDP</td>      <td>7946</td>      <td>0.0.0.0/0</td>    </tr>    <tr>      <td>SSH</td>      <td>7946</td>      <td>0.0.0.0/0</td>    </tr>    <tr>      <td>HTTPS</td>      <td>443</td>      <td>0.0.0.0/0</td>    </tr>    <tr>      <td>HTTP</td>      <td>80</td>      <td>0.0.0.0/0</td>    </tr>  </tbody></table><p>Now make sure to attach the newly created security group to your instance and continue.</p><h3 id=\"review-your-instance-and-launch\">Review your instance and launch</h3><p align=\"center\"><img src=\"/img/2020-05-docker-swarm/aws-ec2-config-keypair.png\" alt=\"aws ec2 configuration keypair\" /></p><p>Upon launch, you will be asked to create a keypair, a <strong>private</strong> key, and a <strong>public</strong> key. AWS EC2 uses public-key cryptography to encrypt and decrypt login information and therefore allows you to access your instance using a private key instead of a password. For a detailed description of the concept of Public Key Authentification, I refer you to <a href=\"https://www.ssh.com/ssh/public-key-authentication\">ssh.com</a>.AWS will ask you to name and create a key pair. The private key will be stored in a <code class=\"language-plaintext highlighter-rouge\">.pem</code> file and should be downloaded to a secure location on your local file system or trusted storage of your choice. I store it in <code class=\"language-plaintext highlighter-rouge\">~/.ssh/</code>. If you create an EC2 instance only this file/this key will let you access it. AWS does not store a duplicate it anywhere, so if you have lost it, you lost it and you will have to shut down your instance for good. The other part, the public key, will be stored on your Linux EC2 instance in <code class=\"language-plaintext highlighter-rouge\">~/.ssh/authorized_keys</code>.</p><h2 id=\"congratulations-you-just-set-up-your-first-ec2-instance\">Congratulations, you just set up your first EC2 instance</h2><p>Now open a terminal, you will need it from now on.</p><h3 id=\"configure-ssh-to-access-your-instance\">Configure SSH to access your instance</h3><p>For convenience’s sake, I create a <code class=\"language-plaintext highlighter-rouge\">~/.ssh/config</code> file and store my remote instances information there for easy access from the terminal.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>vim ~/.ssh/config</code></pre></div></div><p align=\"center\"><img src=\"/img/2020-05-docker-swarm/aws-ec2-ssh-config.png\" alt=\"aws ec2 configuration keypair\" /></p><p>For your hostname, you will set up the Public DNS (IPv4) of your EC2 instance. The <strong>User</strong> depends on your OS. Fox Amazon Linux 2 it’s <code class=\"language-plaintext highlighter-rouge\">ec2-user</code>, for other AMIs you can look up the default user <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connection-prereqs.html\">here</a>.  Your <strong>IdentityFile</strong> will be the <code class=\"language-plaintext highlighter-rouge\">.pem</code> file with your private key you downloaded earlier. Save and close the file <code class=\"language-plaintext highlighter-rouge\">:wq!</code>.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>ssh swarm</code></pre></div></div><p align=\"center\"><img src=\"/img/2020-05-docker-swarm/aws-ec2-ssh.png\" alt=\"ssh into AWS EC2\" /></p><h3 id=\"change-your-instances-hostname\">Change your Instances hostname</h3><p>First, we will change the hostname of the instance using a subdomain of a domain you plan on using for your swarm. Changing the hostname will require you to reboot your instance. Don’t worry, if you’re only in it for this part of the tutorial and only want to set up your first swarm for the lulz, you can skip this step.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">sudo </span>hostnamectl set-hostname webserver.mydomain.com<span class=\"nb\">sudo </span>reboot</code></pre></div></div><p>Wait for your instance to reboot, ssh back in, and check the result using <code class=\"language-plaintext highlighter-rouge\">echo $HOSTNAME</code>.</p><h3 id=\"install-and-setup-docker\">Install and setup Docker</h3><p>Now, this part comes down to the OS you chose for your AMI. Amazon Linux 2 AMI uses <code class=\"language-plaintext highlighter-rouge\">yum</code> package manager. Adapt the next steps to your chosen OS.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># update </span><span class=\"nb\">sudo </span>yum update<span class=\"c\"># install docker </span><span class=\"nb\">sudo </span>yum <span class=\"nb\">install </span>docker<span class=\"c\"># start docker</span><span class=\"nb\">sudo </span>service docker start<span class=\"c\"># check if its installed properly</span>docker <span class=\"nt\">--version</span><span class=\"c\"># add ec2-user to docker group to allow docker commands without sudo</span><span class=\"nb\">sudo </span>usermod <span class=\"nt\">-a</span> <span class=\"nt\">-G</span> docker ec2-user<span class=\"c\"># add docker to autostart and reboot to see if it's working</span><span class=\"nb\">sudo </span>chkconfig docker on<span class=\"nb\">sudo </span>reboot<span class=\"c\"># test user rights and autostart</span>docker version</code></pre></div></div><h3 id=\"init-your-docker-swarm\">Init your Docker swarm</h3><p>With the setup of Docker out of the way, we can finally initiate our Docker Swarm.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>docker swarm init</code></pre></div></div><p>This will greet you with this:</p><p align=\"center\"><img src=\"/img/2020-05-docker-swarm/aws-ec2-docker-swarm-init.png\" alt=\"Docker swarm init\" /></p><p>If you want to add another node to your cluster, you can go back to the AWS Management Console, and do all of this again. If that EC2 instance is ready and docker installed, you can just use the command in the screenshot above to join this new instance to your Docker swarm as a worker node.Now you can check the nodes in your swarm via:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>docker node <span class=\"nb\">ls</span></code></pre></div></div><p align=\"center\"><img src=\"/img/2020-05-docker-swarm/aws-ec2-docker-swarm-node-ls.png\" alt=\"Docker swarm init\" /></p><p>As you can see I only have set up a single node, which is my leading node - the manager.</p><p>Congratulations! You just set up your first Docker Swarm on AWS EC2!</p><p>The next part will focus on how to setup <a href=\"https://containo.us/traefik/\">Traefik</a> and <a href=\"https://www.portainer.io/\">Portainer</a>) to handle connections in your cluster, manage HTTPS certificates and routing as well as monitoring your Docker Swarm.</p>",
            "url": "https://iwpnd.pw//articles/2020-05/part1-docker-swarm-on-ec2-initial-setup",
            
            
            
            "tags": ["docker","swarm","aws ec2"],
            
            "date_published": "2020-05-13T11:37:00+00:00",
            "date_modified": "2020-05-13T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-04/toponym-flashtext-newspaper",
            "title": "How to extract city mentions from slavic newspaper articles",
            "summary": null,
            "content_text": "In Slavic languages a word can change, depending on how and where it is used within a sentence. The city Москва (Moscow) changes to Москве when used prepositional. So when you want to eg. know if a newspaper article is talking about Moscow and do something like\"Москва\" in \"В Москве с начала года отремонтировали 3 тысячи подъездов\"&gt;&gt; Falseeven though the article is mentioning Moscow.For that purpose I wrote toponym.This little tutorial will quickly show you how you can work around that issue. We will be using newspaper3k (github) to extract an article text from The Moscow Times. Then we will create toponyms using toponym (github) library and search the article text for city name mentions using flashgeotext (github).Download and extract an article with newspaper3kfrom newspaper import Articleurl = \"https://www.themoscowtimes.com/ru/2020/04/21/v-moskve-zhenschina-umerla-na-ulichnoi-skameike-posle-otritsatelnogo-testa-na-koronavirus-a125\"article = Article(url=url, language=\"ru\")article.download()article.parse()This instantiate an article object, download the article at the given url and will dechrome the html to parse the actual article text.print(article.text)&gt;&gt; 'Оригинал этой статьи был опубликован 20 апреля в англоязычной версии сайта The Moscow Times.\\n\\nКак сообщается, власти Москвы расследуют смерть женщины на скамейке возле ее дома - в день, когда она была выписана после тестирования на коронавирус.\\n\\nВидеозапись, предоставленная российским телеканалом REN TV, показала, что женщина, которую опознали как 48-летнюю Елену Чуклову, оставалась на скамейке, пока социальные работники и соседи пытались, но так и не смогли проникнуть в ее квартиру. Телеканал сообщил, что Следственный комитет России начал расследование сообщения о смерти женщины.\\n\\n«У входа женщина почувствовала плохо, социальные работники немедленно вызвали скорую помощь, — говорится в заявлении департамента здравоохранения мэрии Москвы. — Скорая помощь прибыла через 11 минут и, к сожалению, констатировала смерть».\\n\\nМинистерство здравоохранения сообщило, что женщина умерла в субботу, на следующий день после того, как ее госпитализировали с подозрением на пневмонию. Тогда ее имя не называлось.\\n\\nКак сообщили в департаменте здравоохранения, вскрытие показало, что женщина умерла от острой сердечной недостаточности. Было отмечено, что у нее была кардиомиопатия, заболевание сердечной мышцы, которое может привести к сердечной недостаточности, и «выраженные изменения в органах алкогольного происхождения».'For good measure we will remove the newlines and add the article.title to the text we want to search for city mentions.text = article.title + \" - \" + article.texttext = \" \".join(text.split())&gt;&gt; 'В Москве женщина умерла на уличной скамейке после отрицательного теста на коронавирус - Оригинал этой статьи был опубликован 20 апреля в англоязычной версии сайта The Moscow Times. Как сообщается, власти Москвы расследуют смерть женщины на скамейке возле ее дома - в день, когда она была выписана после тестирования на коронавирус. Видеозапись, предоставленная российским телеканалом REN TV, показала, что женщина, которую опознали как 48-летнюю Елену Чуклову, оставалась на скамейке, пока социальные работники и соседи пытались, но так и не смогли проникнуть в ее квартиру. Телеканал сообщил, что Следственный комитет России начал расследование сообщения о смерти женщины. «У входа женщина почувствовала плохо, социальные работники немедленно вызвали скорую помощь, — говорится в заявлении департамента здравоохранения мэрии Москвы. — Скорая помощь прибыла через 11 минут и, к сожалению, констатировала смерть». Министерство здравоохранения сообщило, что женщина умерла в субботу, на следующий день после того, как ее госпитализировали с подозрением на пневмонию. Тогда ее имя не называлось. Как сообщили в департаменте здравоохранения, вскрытие показало, что женщина умерла от острой сердечной недостаточности. Было отмечено, что у нее была кардиомиопатия, заболевание сердечной мышцы, которое может привести к сердечной недостаточности, и «выраженные изменения в органах алкогольного происхождения».'Create the toponymsNow we create toponyms for Москва using toponyms.from toponym.recipes import Recipesfrom toponym.toponym import Toponymrecipes = Recipes()recipes.load_from_language(language=\"russian\")t = Toponym(\"Москва\", recipes)t.build()Toponyms are stored int.toponyms&gt;&gt; {'nominative': ['Москва'], 'genitive': ['Москвы', 'Москви'], 'dative': ['Москве'], 'accusative': ['Москву'], 'instrumental': ['Москвой'], 'prepositional': ['Москве']}To use toponyms with flashgeotext they have to adhere another structure.t.list_toponyms()&gt;&gt; ['Москвой', 'Москвы', 'Москве', 'Москву', 'Москви', 'Москва']lookup = {    \"Москва\": t.list_toponyms()    }print(lookup)&gt;&gt; {'Москва': ['Москвой', 'Москвы', 'Москве', 'Москву', 'Москви', 'Москва']}If you want to lookup more than just one city, you would create toponyms in a loop, and store them in a dictionary like so:list_of_cities = [\"Москва\", \"Ростов\"]lookup = dict()for city in list_of_cities:    t = Toponym(input_word=city, recipes=recipes)    t.build()    lookup[city] = t.list_toponyms()print(lookup)&gt;&gt; {    'Москва': [        'Москвой', 'Москвы', 'Москве',        'Москву', 'Москви', 'Москва'        ],    'Ростов': [        'Ростовя', 'Ростовю', 'Ростов',        'Ростовем', 'Ростова', 'Ростовом',        'Ростову', 'Ростове'        ]    }Extract city mentions from the article textNow that we have a collection of toponyms, we can use flashgeotext to extract the city mentions from the article text.First we instantiate a Lookup with flashgeotextfrom flashgeotext.lookup import LookupDatacity_lookup = LookupData(name=\"city_names\", data=lookup, script=\"cyrillic\")By default LookupData will expect you to look for words with latin characters. So we have to specificly add cyrillic characters here. Then we instantiate an instance of GeoText while explicitly not using the demo data that comes with it, but with our own data lookup.from flashgeotext.geotext import GeoTextgeotext = GeoText(use_demo_data=False)geotext.add(lookup_city)Now we use the extract method of GeoText to extract city mentions from a newspaper article.geotext.extract(text, span_info=True)&gt;&gt; {    'city_names': {        'Москва': {            'count': 3,            'span_info': [(2, 8), (204, 210), (826, 832)]            }        }    }And there you have it. Moscow is mention three times in the article. Flashgeotext scales pretty great with increasing number of cities to look up (read here). So you could create a pipeline that would extract city mentions from a stream of newspaper articles in your framework of choice.",
            "content_html": "<p>In Slavic languages a word can change, depending on how and where it is used within a sentence. The city <code class=\"language-plaintext highlighter-rouge\">Москва</code> (Moscow) changes to <code class=\"language-plaintext highlighter-rouge\">Москве</code> when used prepositional. So when you want to eg. know if a newspaper article is talking about Moscow and do something like</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>\"Москва\" in \"В Москве с начала года отремонтировали 3 тысячи подъездов\"&gt;&gt; False</code></pre></div></div><p>even though the article is mentioning Moscow.</p><p>For that purpose I wrote <a href=\"https://github.com/iwpnd/toponym\">toponym</a>.</p><p>This little tutorial will quickly show you how you can work around that issue. We will be using <code class=\"language-plaintext highlighter-rouge\">newspaper3k</code> (<a href=\"https://github.com/codelucas/newspaper\">github</a>) to extract an article text from <a href=\"https://www.themoscowtimes.com/\">The Moscow Times</a>. Then we will create toponyms using <code class=\"language-plaintext highlighter-rouge\">toponym</code> (<a href=\"https://github.com/iwpnd/toponym\">github</a>) library and search the article text for city name mentions using <code class=\"language-plaintext highlighter-rouge\">flashgeotext</code> (<a href=\"https://github.com/iwpnd/flashgeotext\">github</a>).</p><h2 id=\"download-and-extract-an-article-with-newspaper3k\">Download and extract an article with newspaper3k</h2><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">newspaper</span> <span class=\"kn\">import</span> <span class=\"n\">Article</span><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s\">\"https://www.themoscowtimes.com/ru/2020/04/21/v-moskve-zhenschina-umerla-na-ulichnoi-skameike-posle-otritsatelnogo-testa-na-koronavirus-a125\"</span><span class=\"n\">article</span> <span class=\"o\">=</span> <span class=\"n\">Article</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"o\">=</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">language</span><span class=\"o\">=</span><span class=\"s\">\"ru\"</span><span class=\"p\">)</span><span class=\"n\">article</span><span class=\"p\">.</span><span class=\"n\">download</span><span class=\"p\">()</span><span class=\"n\">article</span><span class=\"p\">.</span><span class=\"n\">parse</span><span class=\"p\">()</span></code></pre></div></div><p>This instantiate an article object, download the article at the given url and will dechrome the html to parse the actual article text.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">article</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"s\">'Оригинал этой статьи был опубликован 20 апреля в англоязычной версии сайта The Moscow Times.</span><span class=\"se\">\\n\\n</span><span class=\"s\">Как сообщается, власти Москвы расследуют смерть женщины на скамейке возле ее дома - в день, когда она была выписана после тестирования на коронавирус.</span><span class=\"se\">\\n\\n</span><span class=\"s\">Видеозапись, предоставленная российским телеканалом REN TV, показала, что женщина, которую опознали как 48-летнюю Елену Чуклову, оставалась на скамейке, пока социальные работники и соседи пытались, но так и не смогли проникнуть в ее квартиру. Телеканал сообщил, что Следственный комитет России начал расследование сообщения о смерти женщины.</span><span class=\"se\">\\n\\n</span><span class=\"s\">«У входа женщина почувствовала плохо, социальные работники немедленно вызвали скорую помощь, — говорится в заявлении департамента здравоохранения мэрии Москвы. — Скорая помощь прибыла через 11 минут и, к сожалению, констатировала смерть».</span><span class=\"se\">\\n\\n</span><span class=\"s\">Министерство здравоохранения сообщило, что женщина умерла в субботу, на следующий день после того, как ее госпитализировали с подозрением на пневмонию. Тогда ее имя не называлось.</span><span class=\"se\">\\n\\n</span><span class=\"s\">Как сообщили в департаменте здравоохранения, вскрытие показало, что женщина умерла от острой сердечной недостаточности. Было отмечено, что у нее была кардиомиопатия, заболевание сердечной мышцы, которое может привести к сердечной недостаточности, и «выраженные изменения в органах алкогольного происхождения».'</span></code></pre></div></div><p>For good measure we will remove the newlines and add the <code class=\"language-plaintext highlighter-rouge\">article.title</code> to the text we want to search for city mentions.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">article</span><span class=\"p\">.</span><span class=\"n\">title</span> <span class=\"o\">+</span> <span class=\"s\">\" - \"</span> <span class=\"o\">+</span> <span class=\"n\">article</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s\">\" \"</span><span class=\"p\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">.</span><span class=\"n\">split</span><span class=\"p\">())</span><span class=\"o\">&gt;&gt;</span> <span class=\"s\">'В Москве женщина умерла на уличной скамейке после отрицательного теста на коронавирус - Оригинал этой статьи был опубликован 20 апреля в англоязычной версии сайта The Moscow Times. Как сообщается, власти Москвы расследуют смерть женщины на скамейке возле ее дома - в день, когда она была выписана после тестирования на коронавирус. Видеозапись, предоставленная российским телеканалом REN TV, показала, что женщина, которую опознали как 48-летнюю Елену Чуклову, оставалась на скамейке, пока социальные работники и соседи пытались, но так и не смогли проникнуть в ее квартиру. Телеканал сообщил, что Следственный комитет России начал расследование сообщения о смерти женщины. «У входа женщина почувствовала плохо, социальные работники немедленно вызвали скорую помощь, — говорится в заявлении департамента здравоохранения мэрии Москвы. — Скорая помощь прибыла через 11 минут и, к сожалению, констатировала смерть». Министерство здравоохранения сообщило, что женщина умерла в субботу, на следующий день после того, как ее госпитализировали с подозрением на пневмонию. Тогда ее имя не называлось. Как сообщили в департаменте здравоохранения, вскрытие показало, что женщина умерла от острой сердечной недостаточности. Было отмечено, что у нее была кардиомиопатия, заболевание сердечной мышцы, которое может привести к сердечной недостаточности, и «выраженные изменения в органах алкогольного происхождения».'</span></code></pre></div></div><h2 id=\"create-the-toponyms\">Create the toponyms</h2><p>Now we create toponyms for <code class=\"language-plaintext highlighter-rouge\">Москва</code> using <a href=\"https://github.com/iwpnd/toponym\">toponyms</a>.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">toponym.recipes</span> <span class=\"kn\">import</span> <span class=\"n\">Recipes</span><span class=\"kn\">from</span> <span class=\"nn\">toponym.toponym</span> <span class=\"kn\">import</span> <span class=\"n\">Toponym</span><span class=\"n\">recipes</span> <span class=\"o\">=</span> <span class=\"n\">Recipes</span><span class=\"p\">()</span><span class=\"n\">recipes</span><span class=\"p\">.</span><span class=\"n\">load_from_language</span><span class=\"p\">(</span><span class=\"n\">language</span><span class=\"o\">=</span><span class=\"s\">\"russian\"</span><span class=\"p\">)</span><span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">Toponym</span><span class=\"p\">(</span><span class=\"s\">\"Москва\"</span><span class=\"p\">,</span> <span class=\"n\">recipes</span><span class=\"p\">)</span><span class=\"n\">t</span><span class=\"p\">.</span><span class=\"n\">build</span><span class=\"p\">()</span></code></pre></div></div><p>Toponyms are stored in</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">t</span><span class=\"p\">.</span><span class=\"n\">toponyms</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">{</span><span class=\"s\">'nominative'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'Москва'</span><span class=\"p\">],</span> <span class=\"s\">'genitive'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'Москвы'</span><span class=\"p\">,</span> <span class=\"s\">'Москви'</span><span class=\"p\">],</span> <span class=\"s\">'dative'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'Москве'</span><span class=\"p\">],</span> <span class=\"s\">'accusative'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'Москву'</span><span class=\"p\">],</span> <span class=\"s\">'instrumental'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'Москвой'</span><span class=\"p\">],</span> <span class=\"s\">'prepositional'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'Москве'</span><span class=\"p\">]}</span></code></pre></div></div><p>To use toponyms with <a href=\"https://github.com/iwpnd/flashgeotext\">flashgeotext</a> they have to adhere another structure.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">t</span><span class=\"p\">.</span><span class=\"n\">list_toponyms</span><span class=\"p\">()</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">[</span><span class=\"s\">'Москвой'</span><span class=\"p\">,</span> <span class=\"s\">'Москвы'</span><span class=\"p\">,</span> <span class=\"s\">'Москве'</span><span class=\"p\">,</span> <span class=\"s\">'Москву'</span><span class=\"p\">,</span> <span class=\"s\">'Москви'</span><span class=\"p\">,</span> <span class=\"s\">'Москва'</span><span class=\"p\">]</span><span class=\"n\">lookup</span> <span class=\"o\">=</span> <span class=\"p\">{</span>    <span class=\"s\">\"Москва\"</span><span class=\"p\">:</span> <span class=\"n\">t</span><span class=\"p\">.</span><span class=\"n\">list_toponyms</span><span class=\"p\">()</span>    <span class=\"p\">}</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">lookup</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">{</span><span class=\"s\">'Москва'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'Москвой'</span><span class=\"p\">,</span> <span class=\"s\">'Москвы'</span><span class=\"p\">,</span> <span class=\"s\">'Москве'</span><span class=\"p\">,</span> <span class=\"s\">'Москву'</span><span class=\"p\">,</span> <span class=\"s\">'Москви'</span><span class=\"p\">,</span> <span class=\"s\">'Москва'</span><span class=\"p\">]}</span></code></pre></div></div><p>If you want to lookup more than just one city, you would create toponyms in a loop, and store them in a dictionary like so:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">list_of_cities</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"Москва\"</span><span class=\"p\">,</span> <span class=\"s\">\"Ростов\"</span><span class=\"p\">]</span><span class=\"n\">lookup</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">()</span><span class=\"k\">for</span> <span class=\"n\">city</span> <span class=\"ow\">in</span> <span class=\"n\">list_of_cities</span><span class=\"p\">:</span>    <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">Toponym</span><span class=\"p\">(</span><span class=\"n\">input_word</span><span class=\"o\">=</span><span class=\"n\">city</span><span class=\"p\">,</span> <span class=\"n\">recipes</span><span class=\"o\">=</span><span class=\"n\">recipes</span><span class=\"p\">)</span>    <span class=\"n\">t</span><span class=\"p\">.</span><span class=\"n\">build</span><span class=\"p\">()</span>    <span class=\"n\">lookup</span><span class=\"p\">[</span><span class=\"n\">city</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">t</span><span class=\"p\">.</span><span class=\"n\">list_toponyms</span><span class=\"p\">()</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">lookup</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">{</span>    <span class=\"s\">'Москва'</span><span class=\"p\">:</span> <span class=\"p\">[</span>        <span class=\"s\">'Москвой'</span><span class=\"p\">,</span> <span class=\"s\">'Москвы'</span><span class=\"p\">,</span> <span class=\"s\">'Москве'</span><span class=\"p\">,</span>        <span class=\"s\">'Москву'</span><span class=\"p\">,</span> <span class=\"s\">'Москви'</span><span class=\"p\">,</span> <span class=\"s\">'Москва'</span>        <span class=\"p\">],</span>    <span class=\"s\">'Ростов'</span><span class=\"p\">:</span> <span class=\"p\">[</span>        <span class=\"s\">'Ростовя'</span><span class=\"p\">,</span> <span class=\"s\">'Ростовю'</span><span class=\"p\">,</span> <span class=\"s\">'Ростов'</span><span class=\"p\">,</span>        <span class=\"s\">'Ростовем'</span><span class=\"p\">,</span> <span class=\"s\">'Ростова'</span><span class=\"p\">,</span> <span class=\"s\">'Ростовом'</span><span class=\"p\">,</span>        <span class=\"s\">'Ростову'</span><span class=\"p\">,</span> <span class=\"s\">'Ростове'</span>        <span class=\"p\">]</span>    <span class=\"p\">}</span></code></pre></div></div><h2 id=\"extract-city-mentions-from-the-article-text\">Extract city mentions from the article text</h2><p>Now that we have a collection of toponyms, we can use <a href=\"https://github.com/iwpnd/flashgeotext\">flashgeotext</a> to extract the city mentions from the article text.</p><p>First we instantiate a Lookup with flashgeotext</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">flashgeotext.lookup</span> <span class=\"kn\">import</span> <span class=\"n\">LookupData</span><span class=\"n\">city_lookup</span> <span class=\"o\">=</span> <span class=\"n\">LookupData</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"city_names\"</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">lookup</span><span class=\"p\">,</span> <span class=\"n\">script</span><span class=\"o\">=</span><span class=\"s\">\"cyrillic\"</span><span class=\"p\">)</span></code></pre></div></div><p>By default <code class=\"language-plaintext highlighter-rouge\">LookupData</code> will expect you to look for words with latin characters. So we have to specificly add <code class=\"language-plaintext highlighter-rouge\">cyrillic</code> characters here. Then we instantiate an instance of <code class=\"language-plaintext highlighter-rouge\">GeoText</code> while explicitly not using the demo data that comes with it, but with our own data lookup.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">flashgeotext.geotext</span> <span class=\"kn\">import</span> <span class=\"n\">GeoText</span><span class=\"n\">geotext</span> <span class=\"o\">=</span> <span class=\"n\">GeoText</span><span class=\"p\">(</span><span class=\"n\">use_demo_data</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span><span class=\"n\">geotext</span><span class=\"p\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">lookup_city</span><span class=\"p\">)</span></code></pre></div></div><p>Now we use the <code class=\"language-plaintext highlighter-rouge\">extract</code> method of <code class=\"language-plaintext highlighter-rouge\">GeoText</code> to extract city mentions from a newspaper article.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">geotext</span><span class=\"p\">.</span><span class=\"n\">extract</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">span_info</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">{</span>    <span class=\"s\">'city_names'</span><span class=\"p\">:</span> <span class=\"p\">{</span>        <span class=\"s\">'Москва'</span><span class=\"p\">:</span> <span class=\"p\">{</span>            <span class=\"s\">'count'</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">,</span>            <span class=\"s\">'span_info'</span><span class=\"p\">:</span> <span class=\"p\">[(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">204</span><span class=\"p\">,</span> <span class=\"mi\">210</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">826</span><span class=\"p\">,</span> <span class=\"mi\">832</span><span class=\"p\">)]</span>            <span class=\"p\">}</span>        <span class=\"p\">}</span>    <span class=\"p\">}</span></code></pre></div></div><p>And there you have it. Moscow is mention three times in the article. Flashgeotext scales pretty great with increasing number of cities to look up (<a href=\"https://iwpnd.pw/articles/2020-02/flashgeotext-library\">read here</a>). So you could create a pipeline that would extract city mentions from a stream of newspaper articles in your framework of choice.</p>",
            "url": "https://iwpnd.pw//articles/2020-04/toponym-flashtext-newspaper",
            
            
            
            "tags": ["python","flashgeotext","toponym"],
            
            "date_published": "2020-04-29T11:37:00+00:00",
            "date_modified": "2020-04-29T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-03/apache-kafka-fastapi-geostream",
            "title": "Apache Kafka producer and consumer with FastAPI and aiokafka",
            "summary": null,
            "content_text": "After I finally published flashgeotext last month, it was time for the next item on the to-do list - Apache Kafka. After I somehow avoided the topic completely for the last years, aside from that one friend that wouldn’t shut up about it, I noticed that more often than not Apache Kafka knowledge is a requirement for a lot of positions in data engineering nowadays. So I did what I recommand everybody starting out - check out Tim Berglund’s introduction on Apache Kafka on YouTube and skinny-dip into the rabbit hole from there.tl;drFind the finished project on GitHub: geo-stream-kafkaWhat is Apache Kafka?Always great to read an introduction into a complex topic from somebody who just recently started out with it, right? What could go wrong. So in its core, Apache Kafka is a messaging system with somebody/something producing a message on the one side and a somebody/something consuming the message on the other side, and a lot of magic in between.Kafka core principlesTo zoom in on the magic part, when a producer sends a message, the message is pushed into Kafka topics. When a consumer consumes a message it is pulling the message from a Kafka topic. Kafka topics reside within a so-called broker (eg. Zookeeper). Zookeeper provides synchronization within distributed systems and in the case of Apache Kafka keeps track of the status of Kafka cluster nodes and Kafka topics.You can have multiple producers pushing messages into one topic, or you can have them push to different topics. Messages within topics can be retained indefinitly or be discarded after a certain time, depending on the needs. When a consumer starts consuming a message, it starts from the first message that has been pushed to the topic and continues from thereon. Kafka stores the so-called offset, basically a pointer telling the consumer which messages have been consumed and what is still left to indulge. Messages will stay within the topic, yet when the same consumer pulls messages from the topic it will only receive messages from the offset onwards.There is actually a lot more to dissect here, but if you want to understand as much Apache Kafka to be dangerous, I’d say we leave it at that and I direct you to more competent, no less handsome, people to tell you more.master builders assembleOkay, we have the theory let’s build something with it. This time, instead of a niche NLP problem I wanted to go back to my origins as a geospatial engineer. I figured I would build a producer/consumer architecture for geodata. We will have a FastAPI endpoint that will wrap the producer logic and a consumer FastAPI WebSocket endpoint that a leaflet.js map-application then can tap into. So far I have used Leaflet.js to display static data, so why not try and use it with dynamic data instead. What I had in mind was an architecture like that:Setup an Apache Kafka clusterHell no, I would never dare to even try to comprehend what’s necessary to build that up from scratch. BUT, we live in different times now and there is that handsome technology called Docker. We also can be thankful that respective members of the interwebs community like wurstmeister (german for Saugagemaster) prepare containers like Kafka-docker that are ready to be used for you local (maybe more?) development entertainment. Now I will say that even that container didn’t come without troubles for me. I just wasn’t able to connect to the Kafka broker from outside the container. There is wurstmeisters kafka connectivity guide that helped me to shed some light on the networking inside, but it didn’t help me to get a connection from the outside. The docs suggest that you addenvironment:      KAFKA_ADVERTISED_HOST_NAME: hostname      KAFKA_CREATE_TOPICS: \"geostream:1:1\"      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181to your docker-compose file. There are three things to unpack. We can create the Kafka Topic right here and give it 1 partition and 1 replica. We connect Kafka container to the zookeeper container. Now to the connection problem. So I thought that I wanted to advertise Kafka to localhost. Yet it took me some time to understand why that would not be possible. See, what I learned is that Docker on Mac sets up a VM (yaya old news blabla), so localhost for Kafka inside its container is not your localhost but the IP of the VM your Kafka is running in. So I added this to my .zshenv file:export DOCKER_KAFKA_HOST=$(ipconfig getifaddr en0)and was able to advertise the hostname like so:environment:      KAFKA_ADVERTISED_HOST_NAME: ${DOCKER_KAFKA_HOST}et voila, it’s alive!Test the Apache Kafka clusterWait, how do we know it’s alive and taking messages? We create a minimum viable Kafka producer and a minimum viable Kafka consumer, spin up to terminals and fire the up the producer and the consumer.Setup a minimum viable producerfrom pykafka import KafkaClientimport timeclient = KafkaClient(\"127.0.0.1:9092\")geostream = client.topcis[\"geostream\"]with geostream.get_sync_producer() as producer:    i = 0    for _ in range(10):        producer.produce((\"Kafka is not just an author \" + str(i).encode(\"ascii\"))        i += 1        time.sleep(1)Setup a minimum viable consumerfrom pykafka import KafkaClientclient = KafkaClient(hosts=\"127.0.0.1:9092\")def get_messages(topicname):    def events():        for message in client.topics[topicname].get_simple_consumer():            yield f\"i.value.decode()\"                return events()for x in get_messages(\"geostream\"):    print(x)If you see your consumer printing out what the producer is pushing, you’re good to go. If you set up your networking incorrectly, then you will notice it at this stage at the latest.FastAPI Apache Kafka producerAs shown in my sketch I want to wrap the producer into a FastAPI endpoint. This allows for more than one entity at a time to produce messages to a topic, but also enables me to flexibly change topics that I want to produce messages to with FastAPI endpoint path parameters. I opted to use aiokafka instead of pykafka to make use of FastAPIs async capabilities, but also to plague myself and get a better understanding of async programming (still pending).FastAPIs on_event(\"startup) and on_event(\"shutdown\") make the use of a aiokafka producer easy.loop = asyncio.get_event_loop()aioproducer = AIOKafkaProducer(    loop=loop, client_id=PROJECT_NAME, bootstrap_servers=KAFKA_INSTANCE)@app.on_event(\"startup\")async def startup_event():    await aioproducer.start()@app.on_event(\"shutdown\")async def shutdown_event():    await aioproducer.stop()Afterwards we can use the aioproducer in our application.@app.post(\"/producer/{topicname}\")async def kafka_produce(msg: ProducerMessage, topicname: str):    await aioproducer.send(topicname, json.dumps(msg.dict()).encode(\"ascii\"))    response = ProducerResponse(        name=msg.name, message_id=msg.message_id, topic=topicname    )    return responseFastAPI Apache Kafka consumerThe Apache Kafka consumer endpoint with FastAPI turned out to be a completely different beast. I wasn’t able to setup up something similar to the pykafka MVP. In Flask you can do something like and push server sent events to whoever is calling the /consumer/&lt;topicname&gt; endpoint:@app.route('/consumer/&lt;topicname&gt;')def get_messages(topicname):    def events():        for message in client.topics[topicname].get_simple_consumer():            yield f\"i.value.decode()\"        return Response(events(), mimetype=\"text/event-stream\")This way we can in the Leaflet application we can add an EventSource and an EventListener that would take incoming events and do something with it - exactly as we wanted it to be in the first place (see architecture).I was not able to do the same with FastAPI and an async generator. That is not to say it is not possible, but my dumb ass wasn’t able to figure out how and didn’t bother to open an issue with FastAPI.What I did figure out, however, is that there is a beautiful thing called Websockets and that FastAPI happily supports the likes. Since FastAPI is built on-top of starlette we can use class-basedd endpoints and especially the WebsocketEndpoint to handle incoming Websocket Sessions.@app.websocket_route(\"/consumer/{topicname}\")class WebsocketConsumer(WebSocketEndpoint):    async def on_connect(self, websocket: WebSocket) -&gt; None:        # get topicname from path until I have an alternative        topicname = websocket[\"path\"].split(\"/\")[2]         await websocket.accept()        await websocket.send_json({\"Message\": \"connected\"})        loop = asyncio.get_event_loop()        self.consumer = AIOKafkaConsumer(            topicname,            loop=loop,            client_id=PROJECT_NAME,            bootstrap_servers=KAFKA_INSTANCE,            enable_auto_commit=False,        )        await self.consumer.start()        self.consumer_task = asyncio.create_task(            self.send_consumer_message(websocket=websocket, topicname=topicname)        )        logger.info(\"connected\")    async def on_disconnect(self, websocket: WebSocket, close_code: int) -&gt; None:        self.consumer_task.cancel()        await self.consumer.stop()        logger.info(f\"counter: {self.counter}\")        logger.info(\"disconnected\")        logger.info(\"consumer stopped\")    async def on_receive(self, websocket: WebSocket, data: typing.Any) -&gt; None:        await websocket.send_json({\"Message\": data})    async def send_consumer_message(self, websocket: WebSocket, topicname: str) -&gt; None:        self.counter = 0        while True:            data = await consume(self.consumer, topicname)            response = ConsumerResponse(topic=topicname, **json.loads(data))            logger.info(response)            await websocket.send_text(f\"{response.json()}\")            self.counter = self.counter + 1When an application starts a websocket connection with out websocket endpoint we grab the event loop, use that to build and start the aiokafka consumer, start it and start a consumer task in the loop. Once this is set, everytime the consumer pulls a new message it is forwarded to the application through the websocket. When the leafletjs application either specifically closes the websocket or the browser is closed, we close the websocket, cancel the consumer_task and stop the consumer.Leaflet applicationI have to admit that it’s been some time that I have touched JavaScript at all, so bare with me here.First we will declare our map and websocket connection to the /consumer/{topicname} endpoint.var map = new L.Map('map');var linesLayer = new L.LayerGroup();var ws = new WebSocket(\"ws://127.0.0.1:8003/consumer/geostream\");var osmUrl = 'https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png',    osmAttribution = '&amp;copy; &lt;a href=\"https://www.openstreetmap.org/copyright\"&gt;OpenStreetMap&lt;/a&gt; contributors &amp;copy; &lt;a href=\"https://carto.com/attributions\"&gt;CARTO&lt;/a&gt;',    osm = new L.tileLayer(osmUrl, {maxZoom: 18, attribution: osmAttribution});var colors = [\"#8be9fd\", \"#50fa7b\", \"#ffb86c\", \"#ff79c6\", \"#bd93f9\", \"#ff5555\", \"#f1fa8c\"];lines = {};When client and backend established the silent agreement to use WebSockets, we can declare what we want to do, whenever the websocket receives a new message. Every message is an event. And every event consists of metadata and event.data that can be parsed with JSON.parse().ws.onmessage = function(event) {    console.log(event.data)    obj = JSON.parse(event.data)    [...]One of the requirements was to display more than one entity that pushes messages through the producer, Kafka and the consumer on the map as a live-event. For the leaflet application to associate an event to an entity, I hash events by the name of the entity that is sending them. If there is a new name in an event, it’ll be hashed into a dictionary and added as a new layer on the map. As I wanted every entity to be represented with a different color, the color will be randomly grabbed from the list of colors and hashed alongside the position of the event/entity.    [...]    if(!(obj.name in lines)) {      lines[obj.name] = {\"latlon\": []};      lines[obj.name][\"latlon\"].push([obj.lat, obj.lon]);      lines[obj.name][\"config\"] = {\"color\": colors[Math.floor(Math.random()*colors.length)]};    }    else {      lines[obj.name][\"latlon\"].push([obj.lat, obj.lon]);    }    line = L.polyline(lines[obj.name][\"latlon\"], {color: lines[obj.name][\"config\"][\"color\"]})    linesLayer.addLayer(line)    map.addLayer(linesLayer);};On thing to keep in mind is, that when you zoom on the map the bounds will be messed up and the events will not properly draw polylines along the trajectory of the entity. To fix this I added an zoomend trigger:map.on(\"zoomend\", function (e) { linesLayer.clearLayers() });that will clear the layers until the next event arrives and then continues to draw the trajectories.Result",
            "content_html": "<p>After I finally published <a href=\"https://iwpnd.pw/articles/2020-02/flashgeotext-library\">flashgeotext</a> last month, it was time for the next item on the to-do list - Apache Kafka. After I somehow avoided the topic completely for the last years, aside from that one friend that wouldn’t shut up about it, I noticed that more often than not Apache Kafka knowledge is a requirement for a lot of positions in data engineering nowadays. So I did what I recommand everybody starting out - check out <a href=\"https://www.youtube.com/watch?v=06iRM1Ghr1k\">Tim Berglund’s introduction on Apache Kafka</a> on YouTube and skinny-dip into the rabbit hole from there.</p><h1 id=\"tldr\">tl;dr</h1><p>Find the finished project on GitHub: <a href=\"https://github.com/iwpnd/geo-stream-kafka\">geo-stream-kafka</a></p><h1 id=\"what-is-apache-kafka\">What is Apache Kafka?</h1><p>Always great to read an introduction into a complex topic from somebody who just recently started out with it, right? What could go wrong. So in its core, Apache Kafka is a messaging system with somebody/something producing a message on the one side and a somebody/something consuming the message on the other side, and a lot of magic in between.</p><h2 id=\"kafka-core-principles\">Kafka core principles</h2><p>To zoom in on the magic part, when a producer sends a message, the message is pushed into Kafka topics. When a consumer consumes a message it is pulling the message from a Kafka topic. Kafka topics reside within a so-called broker (eg. Zookeeper). Zookeeper provides synchronization within distributed systems and in the case of Apache Kafka keeps track of the status of Kafka cluster nodes and Kafka topics.</p><p align=\"center\"><img src=\"/img/2020-03-geostream-kafka/kafka.png\" alt=\"setup geostream FastAPI aiokafka\" /></p><p>You can have multiple producers pushing messages into one topic, or you can have them push to different topics. Messages within topics can be retained indefinitly or be discarded after a certain time, depending on the needs. When a consumer starts consuming a message, it starts from the first message that has been pushed to the topic and continues from thereon. Kafka stores the so-called <em>offset</em>, basically a pointer telling the consumer which messages have been consumed and what is still left to indulge. Messages will stay within the topic, yet when the same consumer pulls messages from the topic it will only receive messages from the offset onwards.</p><p>There is actually a lot more to dissect here, but if you want to understand as much Apache Kafka to be dangerous, I’d say we leave it at that and I direct you to more competent, no less handsome, people to tell you more.</p><h1 id=\"master-builders-assemble\">master builders assemble</h1><p>Okay, we have the theory let’s build something with it. This time, instead of a <a href=\"https://iwpnd.pw/articles/2020-02/flashgeotext-library\">niche NLP problem</a> I wanted to go back to my origins as a geospatial engineer. I figured I would build a producer/consumer architecture for geodata. We will have a FastAPI endpoint that will wrap the producer logic and a consumer FastAPI WebSocket endpoint that a <a href=\"https://leafletjs.com/\">leaflet.js</a> map-application then can tap into. So far I have used Leaflet.js to display static data, so why not try and use it with dynamic data instead. What I had in mind was an architecture like that:</p><p align=\"center\"><img src=\"/img/2020-03-geostream-kafka/geostream-FastAPI-kafka.png\" alt=\"setup geostream FastAPI aiokafka\" /></p><h2 id=\"setup-an-apache-kafka-cluster\">Setup an Apache Kafka cluster</h2><p>Hell no, I would never dare to even try to comprehend what’s necessary to build that up from scratch. BUT, we live in different times now and there is that handsome technology called Docker. We also can be thankful that respective members of the interwebs community like <a href=\"https://github.com/wurstmeister/\">wurstmeister</a> (german for Saugagemaster) prepare containers like <a href=\"https://github.com/wurstmeister/kafka-docker\">Kafka-docker</a> that are ready to be used for you local (maybe more?) development entertainment. Now I will say that even that container didn’t come without troubles for me. I just wasn’t able to connect to the Kafka broker from outside the container. There is <a href=\"https://github.com/wurstmeister/kafka-docker/wiki\">wurstmeisters kafka connectivity guide</a> that helped me to shed some light on the networking inside, but it didn’t help me to get a connection from the outside. The docs suggest that you add</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>environment:      KAFKA_ADVERTISED_HOST_NAME: <span class=\"nb\">hostname      </span>KAFKA_CREATE_TOPICS: <span class=\"s2\">\"geostream:1:1\"</span>      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181</code></pre></div></div><p>to your docker-compose file. There are three things to unpack. We can create the Kafka Topic right here and give it 1 partition and 1 replica. We connect Kafka container to the zookeeper container. Now to the connection problem. So I thought that I wanted to advertise Kafka to <code class=\"language-plaintext highlighter-rouge\">localhost</code>. Yet it took me some time to understand why that would not be possible. See, what I learned is that Docker on Mac sets up a VM (yaya old news blabla), so <code class=\"language-plaintext highlighter-rouge\">localhost</code> for Kafka inside its container is not your localhost but the IP of the VM your Kafka is running in. So I added this to my <code class=\"language-plaintext highlighter-rouge\">.zshenv</code> file:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">DOCKER_KAFKA_HOST</span><span class=\"o\">=</span><span class=\"si\">$(</span>ipconfig getifaddr en0<span class=\"si\">)</span></code></pre></div></div><p>and was able to advertise the hostname like so:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>environment:      KAFKA_ADVERTISED_HOST_NAME: <span class=\"k\">${</span><span class=\"nv\">DOCKER_KAFKA_HOST</span><span class=\"k\">}</span></code></pre></div></div><p>et voila, it’s alive!</p><h2 id=\"test-the-apache-kafka-cluster\">Test the Apache Kafka cluster</h2><p>Wait, how do we know it’s alive and taking messages? We create a minimum viable Kafka producer and a minimum viable Kafka consumer, spin up to terminals and fire the up the producer and the consumer.</p><h3 id=\"setup-a-minimum-viable-producer\">Setup a minimum viable producer</h3><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">pykafka</span> <span class=\"kn\">import</span> <span class=\"n\">KafkaClient</span><span class=\"kn\">import</span> <span class=\"nn\">time</span><span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">KafkaClient</span><span class=\"p\">(</span><span class=\"s\">\"127.0.0.1:9092\"</span><span class=\"p\">)</span><span class=\"n\">geostream</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">topcis</span><span class=\"p\">[</span><span class=\"s\">\"geostream\"</span><span class=\"p\">]</span><span class=\"k\">with</span> <span class=\"n\">geostream</span><span class=\"p\">.</span><span class=\"n\">get_sync_producer</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">producer</span><span class=\"p\">:</span>    <span class=\"n\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>    <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">):</span>        <span class=\"n\">producer</span><span class=\"p\">.</span><span class=\"n\">produce</span><span class=\"p\">((</span><span class=\"s\">\"Kafka is not just an author \"</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">).</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s\">\"ascii\"</span><span class=\"p\">))</span>        <span class=\"n\">i</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>        <span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span></code></pre></div></div><h3 id=\"setup-a-minimum-viable-consumer\">Setup a minimum viable consumer</h3><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">pykafka</span> <span class=\"kn\">import</span> <span class=\"n\">KafkaClient</span><span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">KafkaClient</span><span class=\"p\">(</span><span class=\"n\">hosts</span><span class=\"o\">=</span><span class=\"s\">\"127.0.0.1:9092\"</span><span class=\"p\">)</span><span class=\"k\">def</span> <span class=\"nf\">get_messages</span><span class=\"p\">(</span><span class=\"n\">topicname</span><span class=\"p\">):</span>    <span class=\"k\">def</span> <span class=\"nf\">events</span><span class=\"p\">():</span>        <span class=\"k\">for</span> <span class=\"n\">message</span> <span class=\"ow\">in</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">topics</span><span class=\"p\">[</span><span class=\"n\">topicname</span><span class=\"p\">].</span><span class=\"n\">get_simple_consumer</span><span class=\"p\">():</span>            <span class=\"k\">yield</span> <span class=\"sa\">f</span><span class=\"s\">\"i.value.decode()\"</span>                <span class=\"k\">return</span> <span class=\"n\">events</span><span class=\"p\">()</span><span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">get_messages</span><span class=\"p\">(</span><span class=\"s\">\"geostream\"</span><span class=\"p\">):</span>    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span></code></pre></div></div><p>If you see your consumer printing out what the producer is pushing, you’re good to go. If you set up your networking incorrectly, then you will notice it at this stage at the latest.</p><h2 id=\"fastapi-apache-kafka-producer\">FastAPI Apache Kafka producer</h2><p>As shown in my sketch I want to wrap the producer into a <a href=\"https://FastAPI.tiangolo.com/\">FastAPI</a> endpoint. This allows for more than one entity at a time to produce messages to a topic, but also enables me to flexibly change topics that I want to produce messages to with <a href=\"https://FastAPI.tiangolo.com/tutorial/path-params-numeric-validations/\">FastAPI</a> endpoint path parameters. I opted to use <a href=\"https://github.com/aio-libs/aiokafka\">aiokafka</a> instead of pykafka to make use of FastAPIs async capabilities, but also to plague myself and get a better understanding of async programming (still pending).</p><p>FastAPIs <code class=\"language-plaintext highlighter-rouge\">on_event(\"startup)</code> and <code class=\"language-plaintext highlighter-rouge\">on_event(\"shutdown\")</code> make the use of a aiokafka producer easy.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">loop</span> <span class=\"o\">=</span> <span class=\"n\">asyncio</span><span class=\"p\">.</span><span class=\"n\">get_event_loop</span><span class=\"p\">()</span><span class=\"n\">aioproducer</span> <span class=\"o\">=</span> <span class=\"n\">AIOKafkaProducer</span><span class=\"p\">(</span>    <span class=\"n\">loop</span><span class=\"o\">=</span><span class=\"n\">loop</span><span class=\"p\">,</span> <span class=\"n\">client_id</span><span class=\"o\">=</span><span class=\"n\">PROJECT_NAME</span><span class=\"p\">,</span> <span class=\"n\">bootstrap_servers</span><span class=\"o\">=</span><span class=\"n\">KAFKA_INSTANCE</span><span class=\"p\">)</span><span class=\"o\">@</span><span class=\"n\">app</span><span class=\"p\">.</span><span class=\"n\">on_event</span><span class=\"p\">(</span><span class=\"s\">\"startup\"</span><span class=\"p\">)</span><span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">startup_event</span><span class=\"p\">():</span>    <span class=\"k\">await</span> <span class=\"n\">aioproducer</span><span class=\"p\">.</span><span class=\"n\">start</span><span class=\"p\">()</span><span class=\"o\">@</span><span class=\"n\">app</span><span class=\"p\">.</span><span class=\"n\">on_event</span><span class=\"p\">(</span><span class=\"s\">\"shutdown\"</span><span class=\"p\">)</span><span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">shutdown_event</span><span class=\"p\">():</span>    <span class=\"k\">await</span> <span class=\"n\">aioproducer</span><span class=\"p\">.</span><span class=\"n\">stop</span><span class=\"p\">()</span></code></pre></div></div><p>Afterwards we can use the <code class=\"language-plaintext highlighter-rouge\">aioproducer</code> in our application.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">@</span><span class=\"n\">app</span><span class=\"p\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s\">\"/producer/{topicname}\"</span><span class=\"p\">)</span><span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">kafka_produce</span><span class=\"p\">(</span><span class=\"n\">msg</span><span class=\"p\">:</span> <span class=\"n\">ProducerMessage</span><span class=\"p\">,</span> <span class=\"n\">topicname</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">):</span>    <span class=\"k\">await</span> <span class=\"n\">aioproducer</span><span class=\"p\">.</span><span class=\"n\">send</span><span class=\"p\">(</span><span class=\"n\">topicname</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"p\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"n\">msg</span><span class=\"p\">.</span><span class=\"nb\">dict</span><span class=\"p\">()).</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s\">\"ascii\"</span><span class=\"p\">))</span>    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">ProducerResponse</span><span class=\"p\">(</span>        <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"n\">msg</span><span class=\"p\">.</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">message_id</span><span class=\"o\">=</span><span class=\"n\">msg</span><span class=\"p\">.</span><span class=\"n\">message_id</span><span class=\"p\">,</span> <span class=\"n\">topic</span><span class=\"o\">=</span><span class=\"n\">topicname</span>    <span class=\"p\">)</span>    <span class=\"k\">return</span> <span class=\"n\">response</span></code></pre></div></div><h2 id=\"fastapi-apache-kafka-consumer\">FastAPI Apache Kafka consumer</h2><p>The Apache Kafka consumer endpoint with FastAPI turned out to be a completely different beast. I wasn’t able to setup up something similar to the <code class=\"language-plaintext highlighter-rouge\">pykafka</code> MVP. In <a href=\"\">Flask</a> you can do something like and push server sent events to whoever is calling the <code class=\"language-plaintext highlighter-rouge\">/consumer/&lt;topicname&gt;</code> endpoint:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">@</span><span class=\"n\">app</span><span class=\"p\">.</span><span class=\"n\">route</span><span class=\"p\">(</span><span class=\"s\">'/consumer/&lt;topicname&gt;'</span><span class=\"p\">)</span><span class=\"k\">def</span> <span class=\"nf\">get_messages</span><span class=\"p\">(</span><span class=\"n\">topicname</span><span class=\"p\">):</span>    <span class=\"k\">def</span> <span class=\"nf\">events</span><span class=\"p\">():</span>        <span class=\"k\">for</span> <span class=\"n\">message</span> <span class=\"ow\">in</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">topics</span><span class=\"p\">[</span><span class=\"n\">topicname</span><span class=\"p\">].</span><span class=\"n\">get_simple_consumer</span><span class=\"p\">():</span>            <span class=\"k\">yield</span> <span class=\"sa\">f</span><span class=\"s\">\"i.value.decode()\"</span>        <span class=\"k\">return</span> <span class=\"n\">Response</span><span class=\"p\">(</span><span class=\"n\">events</span><span class=\"p\">(),</span> <span class=\"n\">mimetype</span><span class=\"o\">=</span><span class=\"s\">\"text/event-stream\"</span><span class=\"p\">)</span></code></pre></div></div><p>This way we can in the Leaflet application we can add an <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events\">EventSource and an EventListener</a> that would take incoming events and do something with it - exactly as we wanted it to be in the first place (see architecture).I was not able to do the same with FastAPI and an async generator. That is not to say it is not possible, but my dumb ass wasn’t able to figure out how and didn’t bother to open an issue with FastAPI.</p><p>What I did figure out, however, is that there is a beautiful thing called <a href=\"https://FastAPI.tiangolo.com/advanced/websockets/\">Websockets</a> and that FastAPI happily supports the likes. Since FastAPI is built on-top of starlette we can use class-basedd endpoints and especially the <a href=\"https://www.starlette.io/endpoints/#websocketendpoint\">WebsocketEndpoint</a> to handle incoming <code class=\"language-plaintext highlighter-rouge\">Websocket</code> Sessions.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">@</span><span class=\"n\">app</span><span class=\"p\">.</span><span class=\"n\">websocket_route</span><span class=\"p\">(</span><span class=\"s\">\"/consumer/{topicname}\"</span><span class=\"p\">)</span><span class=\"k\">class</span> <span class=\"nc\">WebsocketConsumer</span><span class=\"p\">(</span><span class=\"n\">WebSocketEndpoint</span><span class=\"p\">):</span>    <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">on_connect</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">websocket</span><span class=\"p\">:</span> <span class=\"n\">WebSocket</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"bp\">None</span><span class=\"p\">:</span>        <span class=\"c1\"># get topicname from path until I have an alternative</span>        <span class=\"n\">topicname</span> <span class=\"o\">=</span> <span class=\"n\">websocket</span><span class=\"p\">[</span><span class=\"s\">\"path\"</span><span class=\"p\">].</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s\">\"/\"</span><span class=\"p\">)[</span><span class=\"mi\">2</span><span class=\"p\">]</span>         <span class=\"k\">await</span> <span class=\"n\">websocket</span><span class=\"p\">.</span><span class=\"n\">accept</span><span class=\"p\">()</span>        <span class=\"k\">await</span> <span class=\"n\">websocket</span><span class=\"p\">.</span><span class=\"n\">send_json</span><span class=\"p\">({</span><span class=\"s\">\"Message\"</span><span class=\"p\">:</span> <span class=\"s\">\"connected\"</span><span class=\"p\">})</span>        <span class=\"n\">loop</span> <span class=\"o\">=</span> <span class=\"n\">asyncio</span><span class=\"p\">.</span><span class=\"n\">get_event_loop</span><span class=\"p\">()</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">consumer</span> <span class=\"o\">=</span> <span class=\"n\">AIOKafkaConsumer</span><span class=\"p\">(</span>            <span class=\"n\">topicname</span><span class=\"p\">,</span>            <span class=\"n\">loop</span><span class=\"o\">=</span><span class=\"n\">loop</span><span class=\"p\">,</span>            <span class=\"n\">client_id</span><span class=\"o\">=</span><span class=\"n\">PROJECT_NAME</span><span class=\"p\">,</span>            <span class=\"n\">bootstrap_servers</span><span class=\"o\">=</span><span class=\"n\">KAFKA_INSTANCE</span><span class=\"p\">,</span>            <span class=\"n\">enable_auto_commit</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span>        <span class=\"p\">)</span>        <span class=\"k\">await</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">consumer</span><span class=\"p\">.</span><span class=\"n\">start</span><span class=\"p\">()</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">consumer_task</span> <span class=\"o\">=</span> <span class=\"n\">asyncio</span><span class=\"p\">.</span><span class=\"n\">create_task</span><span class=\"p\">(</span>            <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">send_consumer_message</span><span class=\"p\">(</span><span class=\"n\">websocket</span><span class=\"o\">=</span><span class=\"n\">websocket</span><span class=\"p\">,</span> <span class=\"n\">topicname</span><span class=\"o\">=</span><span class=\"n\">topicname</span><span class=\"p\">)</span>        <span class=\"p\">)</span>        <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s\">\"connected\"</span><span class=\"p\">)</span>    <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">on_disconnect</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">websocket</span><span class=\"p\">:</span> <span class=\"n\">WebSocket</span><span class=\"p\">,</span> <span class=\"n\">close_code</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"bp\">None</span><span class=\"p\">:</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">consumer_task</span><span class=\"p\">.</span><span class=\"n\">cancel</span><span class=\"p\">()</span>        <span class=\"k\">await</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">consumer</span><span class=\"p\">.</span><span class=\"n\">stop</span><span class=\"p\">()</span>        <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"counter: </span><span class=\"si\">{</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">counter</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>        <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s\">\"disconnected\"</span><span class=\"p\">)</span>        <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s\">\"consumer stopped\"</span><span class=\"p\">)</span>    <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">on_receive</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">websocket</span><span class=\"p\">:</span> <span class=\"n\">WebSocket</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">typing</span><span class=\"p\">.</span><span class=\"n\">Any</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"bp\">None</span><span class=\"p\">:</span>        <span class=\"k\">await</span> <span class=\"n\">websocket</span><span class=\"p\">.</span><span class=\"n\">send_json</span><span class=\"p\">({</span><span class=\"s\">\"Message\"</span><span class=\"p\">:</span> <span class=\"n\">data</span><span class=\"p\">})</span>    <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">send_consumer_message</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">websocket</span><span class=\"p\">:</span> <span class=\"n\">WebSocket</span><span class=\"p\">,</span> <span class=\"n\">topicname</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"bp\">None</span><span class=\"p\">:</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">counter</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>        <span class=\"k\">while</span> <span class=\"bp\">True</span><span class=\"p\">:</span>            <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">consume</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">consumer</span><span class=\"p\">,</span> <span class=\"n\">topicname</span><span class=\"p\">)</span>            <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">ConsumerResponse</span><span class=\"p\">(</span><span class=\"n\">topic</span><span class=\"o\">=</span><span class=\"n\">topicname</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">json</span><span class=\"p\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">))</span>            <span class=\"n\">logger</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">)</span>            <span class=\"k\">await</span> <span class=\"n\">websocket</span><span class=\"p\">.</span><span class=\"n\">send_text</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"</span><span class=\"si\">{</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">json</span><span class=\"p\">()</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>            <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">counter</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">counter</span> <span class=\"o\">+</span> <span class=\"mi\">1</span></code></pre></div></div><p>When an application starts a websocket connection with out websocket endpoint we grab the event loop, use that to build and start the aiokafka consumer, start it and start a consumer task in the loop. Once this is set, everytime the consumer pulls a new message it is forwarded to the application through the websocket. When the leafletjs application either specifically closes the websocket or the browser is closed, we close the websocket, cancel the consumer_task and stop the consumer.</p><h2 id=\"leaflet-application\">Leaflet application</h2><p>I have to admit that it’s been some time that I have touched JavaScript at all, so bare with me here.</p><p>First we will declare our map and websocket connection to the <code class=\"language-plaintext highlighter-rouge\">/consumer/{topicname}</code> endpoint.</p><div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">var</span> <span class=\"nx\">map</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">L</span><span class=\"p\">.</span><span class=\"nb\">Map</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">map</span><span class=\"dl\">'</span><span class=\"p\">);</span><span class=\"kd\">var</span> <span class=\"nx\">linesLayer</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">L</span><span class=\"p\">.</span><span class=\"nx\">LayerGroup</span><span class=\"p\">();</span><span class=\"kd\">var</span> <span class=\"nx\">ws</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">WebSocket</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">ws://127.0.0.1:8003/consumer/geostream</span><span class=\"dl\">\"</span><span class=\"p\">);</span><span class=\"kd\">var</span> <span class=\"nx\">osmUrl</span> <span class=\"o\">=</span> <span class=\"dl\">'</span><span class=\"s1\">https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png</span><span class=\"dl\">'</span><span class=\"p\">,</span>    <span class=\"nx\">osmAttribution</span> <span class=\"o\">=</span> <span class=\"dl\">'</span><span class=\"s1\">&amp;copy; &lt;a href=\"https://www.openstreetmap.org/copyright\"&gt;OpenStreetMap&lt;/a&gt; contributors &amp;copy; &lt;a href=\"https://carto.com/attributions\"&gt;CARTO&lt;/a&gt;</span><span class=\"dl\">'</span><span class=\"p\">,</span>    <span class=\"nx\">osm</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">L</span><span class=\"p\">.</span><span class=\"nx\">tileLayer</span><span class=\"p\">(</span><span class=\"nx\">osmUrl</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"na\">maxZoom</span><span class=\"p\">:</span> <span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"na\">attribution</span><span class=\"p\">:</span> <span class=\"nx\">osmAttribution</span><span class=\"p\">});</span><span class=\"kd\">var</span> <span class=\"nx\">colors</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"dl\">\"</span><span class=\"s2\">#8be9fd</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"dl\">\"</span><span class=\"s2\">#50fa7b</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"dl\">\"</span><span class=\"s2\">#ffb86c</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"dl\">\"</span><span class=\"s2\">#ff79c6</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"dl\">\"</span><span class=\"s2\">#bd93f9</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"dl\">\"</span><span class=\"s2\">#ff5555</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"dl\">\"</span><span class=\"s2\">#f1fa8c</span><span class=\"dl\">\"</span><span class=\"p\">];</span><span class=\"nx\">lines</span> <span class=\"o\">=</span> <span class=\"p\">{};</span></code></pre></div></div><p>When client and backend established the silent agreement to use WebSockets, we can declare what we want to do, whenever the <a href=\"https://javascript.info/websocket\">websocket receives a new message</a>. Every message is an <code class=\"language-plaintext highlighter-rouge\">event</code>. And every <code class=\"language-plaintext highlighter-rouge\">event</code> consists of metadata and <code class=\"language-plaintext highlighter-rouge\">event.data</code> that can be parsed with <code class=\"language-plaintext highlighter-rouge\">JSON.parse()</code>.</p><div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">ws</span><span class=\"p\">.</span><span class=\"nx\">onmessage</span> <span class=\"o\">=</span> <span class=\"kd\">function</span><span class=\"p\">(</span><span class=\"nx\">event</span><span class=\"p\">)</span> <span class=\"p\">{</span>    <span class=\"nx\">console</span><span class=\"p\">.</span><span class=\"nx\">log</span><span class=\"p\">(</span><span class=\"nx\">event</span><span class=\"p\">.</span><span class=\"nx\">data</span><span class=\"p\">)</span>    <span class=\"nx\">obj</span> <span class=\"o\">=</span> <span class=\"nx\">JSON</span><span class=\"p\">.</span><span class=\"nx\">parse</span><span class=\"p\">(</span><span class=\"nx\">event</span><span class=\"p\">.</span><span class=\"nx\">data</span><span class=\"p\">)</span>    <span class=\"p\">[...]</span></code></pre></div></div><p>One of the requirements was to display more than one entity that pushes messages through the producer, Kafka and the consumer on the map as a live-event. For the leaflet application to associate an event to an entity, I hash events by the name of the entity that is sending them. If there is a new <code class=\"language-plaintext highlighter-rouge\">name</code> in an event, it’ll be hashed into a dictionary and added as a new layer on the map. As I wanted every entity to be represented with a different color, the color will be randomly grabbed from the list of <code class=\"language-plaintext highlighter-rouge\">colors</code> and hashed alongside the position of the event/entity.</p><div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>    <span class=\"p\">[...]</span>    <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"o\">!</span><span class=\"p\">(</span><span class=\"nx\">obj</span><span class=\"p\">.</span><span class=\"nx\">name</span> <span class=\"k\">in</span> <span class=\"nx\">lines</span><span class=\"p\">))</span> <span class=\"p\">{</span>      <span class=\"nx\">lines</span><span class=\"p\">[</span><span class=\"nx\">obj</span><span class=\"p\">.</span><span class=\"nx\">name</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"dl\">\"</span><span class=\"s2\">latlon</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"p\">[]};</span>      <span class=\"nx\">lines</span><span class=\"p\">[</span><span class=\"nx\">obj</span><span class=\"p\">.</span><span class=\"nx\">name</span><span class=\"p\">][</span><span class=\"dl\">\"</span><span class=\"s2\">latlon</span><span class=\"dl\">\"</span><span class=\"p\">].</span><span class=\"nx\">push</span><span class=\"p\">([</span><span class=\"nx\">obj</span><span class=\"p\">.</span><span class=\"nx\">lat</span><span class=\"p\">,</span> <span class=\"nx\">obj</span><span class=\"p\">.</span><span class=\"nx\">lon</span><span class=\"p\">]);</span>      <span class=\"nx\">lines</span><span class=\"p\">[</span><span class=\"nx\">obj</span><span class=\"p\">.</span><span class=\"nx\">name</span><span class=\"p\">][</span><span class=\"dl\">\"</span><span class=\"s2\">config</span><span class=\"dl\">\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"dl\">\"</span><span class=\"s2\">color</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"nx\">colors</span><span class=\"p\">[</span><span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">floor</span><span class=\"p\">(</span><span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">random</span><span class=\"p\">()</span><span class=\"o\">*</span><span class=\"nx\">colors</span><span class=\"p\">.</span><span class=\"nx\">length</span><span class=\"p\">)]};</span>    <span class=\"p\">}</span>    <span class=\"k\">else</span> <span class=\"p\">{</span>      <span class=\"nx\">lines</span><span class=\"p\">[</span><span class=\"nx\">obj</span><span class=\"p\">.</span><span class=\"nx\">name</span><span class=\"p\">][</span><span class=\"dl\">\"</span><span class=\"s2\">latlon</span><span class=\"dl\">\"</span><span class=\"p\">].</span><span class=\"nx\">push</span><span class=\"p\">([</span><span class=\"nx\">obj</span><span class=\"p\">.</span><span class=\"nx\">lat</span><span class=\"p\">,</span> <span class=\"nx\">obj</span><span class=\"p\">.</span><span class=\"nx\">lon</span><span class=\"p\">]);</span>    <span class=\"p\">}</span>    <span class=\"nx\">line</span> <span class=\"o\">=</span> <span class=\"nx\">L</span><span class=\"p\">.</span><span class=\"nx\">polyline</span><span class=\"p\">(</span><span class=\"nx\">lines</span><span class=\"p\">[</span><span class=\"nx\">obj</span><span class=\"p\">.</span><span class=\"nx\">name</span><span class=\"p\">][</span><span class=\"dl\">\"</span><span class=\"s2\">latlon</span><span class=\"dl\">\"</span><span class=\"p\">],</span> <span class=\"p\">{</span><span class=\"na\">color</span><span class=\"p\">:</span> <span class=\"nx\">lines</span><span class=\"p\">[</span><span class=\"nx\">obj</span><span class=\"p\">.</span><span class=\"nx\">name</span><span class=\"p\">][</span><span class=\"dl\">\"</span><span class=\"s2\">config</span><span class=\"dl\">\"</span><span class=\"p\">][</span><span class=\"dl\">\"</span><span class=\"s2\">color</span><span class=\"dl\">\"</span><span class=\"p\">]})</span>    <span class=\"nx\">linesLayer</span><span class=\"p\">.</span><span class=\"nx\">addLayer</span><span class=\"p\">(</span><span class=\"nx\">line</span><span class=\"p\">)</span>    <span class=\"nx\">map</span><span class=\"p\">.</span><span class=\"nx\">addLayer</span><span class=\"p\">(</span><span class=\"nx\">linesLayer</span><span class=\"p\">);</span><span class=\"p\">};</span></code></pre></div></div><p>On thing to keep in mind is, that when you zoom on the map the <code class=\"language-plaintext highlighter-rouge\">bounds</code> will be messed up and the events will not properly draw polylines along the trajectory of the entity. To fix this I added an <code class=\"language-plaintext highlighter-rouge\">zoomend</code> trigger:</p><div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">map</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">zoomend</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"kd\">function</span> <span class=\"p\">(</span><span class=\"nx\">e</span><span class=\"p\">)</span> <span class=\"p\">{</span> <span class=\"nx\">linesLayer</span><span class=\"p\">.</span><span class=\"nx\">clearLayers</span><span class=\"p\">()</span> <span class=\"p\">});</span></code></pre></div></div><p>that will clear the layers until the next event arrives and then continues to draw the trajectories.</p><h2 id=\"result\">Result</h2><p align=\"center\"><img src=\"/img/2020-03-geostream-kafka/geostream.gif\" alt=\"geostream leaflet\" /></p>",
            "url": "https://iwpnd.pw//articles/2020-03/apache-kafka-fastapi-geostream",
            
            
            
            "tags": ["python","kafka","FastAPI"],
            
            "date_published": "2020-03-11T11:37:00+00:00",
            "date_modified": "2020-03-11T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-02/flashgeotext-library",
            "title": "Introducing flashgeotext&#58; extract city and country names from text",
            "summary": null,
            "content_text": "Say you are faced with the problem of extracting the name of a location from a text and count their occurrences. How would you do it in the simplest of manners, if you:  Have a list of names that you want to extract (10.000+)  Don’t care about any other name in your text, because you know what you are looking for  Are on a budget  Want to optimize for execution timetl;drGo to https://flashgeotext.iwpnd.pw, ignore all that and use it right away.Let’s brute force itLet’s brute for this problem and check every word in a lookup against an input text.lookup = ['Berlin', 'Hamburg', 'London', 'New York', [...]]input_text = \"Berlin is a great city. Berlin is not as nice as Erlangen, but will do for now.\"# check if lookup in input_textisin_text = [city in input_text for city in lookup]print(isin_text)&gt;&gt; [True, False, False, True]Now you know that Berlin and New York are mentioned in the text. But you don’t know how often. So maybe you think of something like:lookup = ['Berlin', 'Hamburg', 'London', 'New York']input_text = \"Berlin is a great city. Berlin is not as nice as New York, but will do for now.\"extract = {}for city in lookup:    for word in input_text.split():        if word == city:            if city not in extract:                extract[city] = {\"count\": 1}            else:                extract[city][\"count\"] = extract[city][\"count\"] + 1                print(extract)&gt;&gt; {'Berlin': {'count': 2}}If your eyes are already bleeding, wipe that away and stick with me some more. Now we have a count for the occurrences of Berlinbut are missing New York entirely because we’re iterating over a sequence of unigrams but to match New York we would have to check bigrams of the words in the input text. If we were to look up something like Costa del Sol, it would take trigrams and so on. With some recursive function you might be able to abuse the find() method of Python strings to avoid the use of n-grams, but before we go down that route we cut the conclusion short and say that brute-forcing is not the way to go. At best you’re left with M * N calculations - that is when you’re checking for unigrams. That means every word in the lookup, will be checked against every word of the input text.What now? Named Entity Extraction!To reduce the number of calculations and to avoid the use of n-grams, we have to reduce either M (the number of words in the lookup) or reduce N the words in the input text. Reducing M is off the table, because that’s what we’re looking for. Reducing N is something we can do though. Cities, countries, and districts are all named entities. And Named Entities can be extracted, reducing N in the process.spaCyimport spacynlp = spacy.load(\"en_core_web_sm\")doc = nlp(\"Berlin is an awesome city. It is also great to live in London though.\")for ent in doc.ents:    print(ent.text, ent.start_char, ent.end_char, ent.label_)&gt;&gt; Berlin 0 6 GPE&gt;&gt; London 55 61 GPEspaCy is awesome. It supports a lot of languages. It’s easy to learn, hard to master. It’s super well documented and embedded into a great community and a great ecosystem of plugins/addons/spinoffs. It has a lot of functionality aside from named entity extraction. And it does what it is supposed to do and more. However, named entity extraction with spaCy is still based on a trained model prediction, and even though the core models perform well, they are not 100% accurate. On top of that spaCy is build with C dependencies, which can be a problem in some environments. Also, you are limited to the pre-trained models, unless you want to train your language model from scratch.Using spaCy limits us to a limited amount of languages that are supported. Locations that are parsed from an input text and returned by spaCy would still need to be checked against a lookup. Slight variations in the input text would drastically change the result.import spacynlp = spacy.load(\"en_core_web_sm\")doc = nlp(\"Berlin city is awesome. But now imagine you live in London city.\")for ent in doc.ents:    print(ent.text, ent.start_char, ent.end_char, ent.label_)&gt;&gt; Berlin city 0 11 GPE&gt;&gt; London city 52 63 GPEAlso, there would have to be a process to calculate the occurrences. Spacys dependencies make it harder (not impossible) to deploy in a low-cost environment such as AWS Lambda.GeoTextfrom geotext import GeoTextplaces = GeoText(\"London is a great city\")print(places.cities)&gt;&gt; \"London\"GeoText relies on a single regex search pattern to extract named entities from an input text. Afterward, GeoText tries to match every single one of the entities found to a collection of city and country names one by one. This approach is fast for the 22.000 cities that come with the library, but do not scale well with longer texts and more cities/keywords in a lookup file. GeoText also does not make it easy to bring your data. Also, synonyms are not in the scope of GeoText. Another problem is the regex search pattern that extracts named entities. It is a fine line between matching correctly and matching too much, and it gets even harder to match when city names contain more than a couple of words. Have fun matching something like Friendly Village of Crooked Creek.Nonetheless, GeoText comes very close to what I had in mind. GeoText provides named entity recognition, even though it has its flaws. It provides some sample data from Geonames that can work as a lookup. It is a native python implementation and will run anywhere. Where GeoText struggles is that it comes batteries included but doesn’t provide you an opening to bring your data. Also, the regex statement has its limits.ConclusionBy extracting named entities we reduce the number of computations necessary to look up keywords in a text tremendously. However reducing the number of computations is of no great use, if we can’t match the lookup against the extracted entities. Spacy is a great framework, but not the tool for the job. GeoText looks like the tool for the job but is flawed when it comes to entity extraction and flexibility.Aaand now?What if instead of reducing N the number of words in the text to check against a lookup explicitly? What if we would be able to go over the input text, character by character, in one go and would only perform an action if the character isa) the start of a word, ergo follows a non-word character and b) the character is even present in our list of keywords to lookup?FlashText an Aho-Corasick implementationFlashText (see paper) is loosely based on the  Aho-Corasick Algorithm used in string-searching. Using the keywords in the lookup data a Trie dictionary is built. The tree is grown by adding edges down from node to node where edges are comprised of letters in each of the strings being added.source: banay.meThis allows it to iterate over the input text character by character, and only if the character matches the trie dictionary from start to finish a match is found and stored separately.from flashtext import KeywordProcessorlookup = ['Berlin', 'Hamburg', 'London', 'New York']input_text = \"Berlin is a great city. Berlin is not as nice as New York, but will do for now.\"processor = KeywordProcessor(case_sensitive=True)processor.add_keywords_from_list(lookup)processor.extract_keywords(input_text, span_info=True)&gt;&gt; [('Berlin', 0, 6), ('Berlin', 24, 30), ('New York', 49, 57)]This makes FlashText super fast compared to other approaches.source: github.com/vi3k6i5/flashtext/Flashgeotextflashgeotext is my approach in making something good like FlashText, even better by adding some quality of life add-ons to it. It is also intended as a homage to GeoText following a discussion late last year.Featuresbatteries included lookupflashgeotext comes with batteries included. Just like GeoText, you can add city names from geonames.org as a lookup by default and start going right away.from flashgeotext.geotext import GeoTextgeotext = GeoText(use_demo_data=True)input_text = '''Shanghai. The Chinese Ministry of Finance in Shanghai said that China plans                to cut tariffs on $75 billion worth of goods that the country                imports from the US. Washington welcomes the decision.'''geotext.extract(input_text=input_text, span_info=True)&gt;&gt; {    'cities': {        'Shanghai': {            'count': 2,            'span_info': [(0, 8), (45, 53)]            },        'Washington, D.C.': {            'count': 1,            'span_info': [(175, 185)]            }        },    'countries': {        'China': {            'count': 1,            'span_info': [(64, 69)]            },        'United States': {            'count': 1,            'span_info': [(171, 173)]            }        }    }improved data handlingThe idea is to provide a way to add additional data to the already provided data from Geonames or use your data for the lookup entirely. This is achieved by utilizing a quasi composite pattern. You instantiate LookupData with your data (see example) and add that instance to a LookupDataPool, which is just a collection of flashtext.KeywordProcessor’s. On extract() for every LookupData in the pool there is one passing and extraction over the text, which amounts to the complexity of O(N * number of LookupData). The extracted data is parsed, occurrences are counted, and span information stored and returned.small footprint and no dependenciesJust like FlashText, flashgeotext does not have any big dependencies which make it easy to use in AWS Lambda and/or AWS Step functions. Once instantiated on a warm AWS Lambda/Stepfunction, it can be reused again and again. You can easily put every city name on earth into a trie dictionary and run it inside an AWS Lambda’s 3072mb of memory.non-word-boundariesFlashText’s non-word boundaries that are essential in during keyword extraction are comprised ofnon_word_boundaries = set(string.digits + string.ascii_letters + '_')print(non_word_boundaries)&gt;&gt; {'k', '6', 's', 'M', 'i', 'S', 'm', 'E', 'r', 'W', 'v', 'l', 'R', 'f', 'e', 'X', '7', '3', 'q', 'w', '0', 'x', 'V', 'C', 'n', 'I', '4', 'D', 'z', 'G', 'L', '2', 'T', 'U', '_', 'B', 't', 'Q', 'd', '9', 'h', 'o', 'c', 'u', 'P', 'K', 'Y', 'p', 'A', 'J', 'O', 'N', 'H', 'j', 'a', 'Z', '5', '1', 'b', 'y', 'F', '8', 'g'}This has been are-occurring issue in the past. The fact that there are a lot of characters missing, makes it, that FlashText is unreliable at best for every other language than English. I’m currently working on handling different alphabets within flashgeotext to make it possible to reliably extract keywords from a text written in Cyrillic or Greek characters also.ConclusionFlashgeotext can help you to extract city and country names in your text processing pipelines. It gives you the ability to add your data instead of or on top of the demo data provided. It is incredibly fast, reliable, written in native Python and easy to set up in an AWS Lambda/Stepfunction environment or as Airflow / Luigi / Prefect directed acyclic graph.",
            "content_html": "<p>Say you are faced with the problem of extracting the name of a location from a text and count their occurrences. How would you do it in the simplest of manners, if you:</p><ol>  <li>Have a list of names that you want to extract (10.000+)</li>  <li>Don’t care about any other name in your text, because you know what you are looking for</li>  <li>Are on a budget</li>  <li>Want to optimize for execution time</li></ol><h1 id=\"tldr\">tl;dr</h1><p>Go to <a href=\"https://flashgeotext.iwpnd.pw\">https://flashgeotext.iwpnd.pw</a>, ignore all that and use it right away.</p><h2 id=\"lets-brute-force-it\">Let’s brute force it</h2><p>Let’s brute for this problem and check every word in a lookup against an input text.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">lookup</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">'Berlin'</span><span class=\"p\">,</span> <span class=\"s\">'Hamburg'</span><span class=\"p\">,</span> <span class=\"s\">'London'</span><span class=\"p\">,</span> <span class=\"s\">'New York'</span><span class=\"p\">,</span> <span class=\"p\">[...]]</span><span class=\"n\">input_text</span> <span class=\"o\">=</span> <span class=\"s\">\"Berlin is a great city. Berlin is not as nice as Erlangen, but will do for now.\"</span><span class=\"c1\"># check if lookup in input_text</span><span class=\"n\">isin_text</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">city</span> <span class=\"ow\">in</span> <span class=\"n\">input_text</span> <span class=\"k\">for</span> <span class=\"n\">city</span> <span class=\"ow\">in</span> <span class=\"n\">lookup</span><span class=\"p\">]</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">isin_text</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">[</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"bp\">False</span><span class=\"p\">,</span> <span class=\"bp\">False</span><span class=\"p\">,</span> <span class=\"bp\">True</span><span class=\"p\">]</span></code></pre></div></div><p>Now you know that Berlin and New York are mentioned in the text. But you don’t know how often. So maybe you think of something like:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">lookup</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">'Berlin'</span><span class=\"p\">,</span> <span class=\"s\">'Hamburg'</span><span class=\"p\">,</span> <span class=\"s\">'London'</span><span class=\"p\">,</span> <span class=\"s\">'New York'</span><span class=\"p\">]</span><span class=\"n\">input_text</span> <span class=\"o\">=</span> <span class=\"s\">\"Berlin is a great city. Berlin is not as nice as New York, but will do for now.\"</span><span class=\"n\">extract</span> <span class=\"o\">=</span> <span class=\"p\">{}</span><span class=\"k\">for</span> <span class=\"n\">city</span> <span class=\"ow\">in</span> <span class=\"n\">lookup</span><span class=\"p\">:</span>    <span class=\"k\">for</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">input_text</span><span class=\"p\">.</span><span class=\"n\">split</span><span class=\"p\">():</span>        <span class=\"k\">if</span> <span class=\"n\">word</span> <span class=\"o\">==</span> <span class=\"n\">city</span><span class=\"p\">:</span>            <span class=\"k\">if</span> <span class=\"n\">city</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">extract</span><span class=\"p\">:</span>                <span class=\"n\">extract</span><span class=\"p\">[</span><span class=\"n\">city</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"count\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">}</span>            <span class=\"k\">else</span><span class=\"p\">:</span>                <span class=\"n\">extract</span><span class=\"p\">[</span><span class=\"n\">city</span><span class=\"p\">][</span><span class=\"s\">\"count\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">extract</span><span class=\"p\">[</span><span class=\"n\">city</span><span class=\"p\">][</span><span class=\"s\">\"count\"</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>                <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">extract</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">{</span><span class=\"s\">'Berlin'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s\">'count'</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">}}</span></code></pre></div></div><p>If your eyes are already bleeding, wipe that away and stick with me some more. Now we have a count for the occurrences of <em>Berlin</em>but are missing <em>New York</em> entirely because we’re iterating over a sequence of <a href=\"https://en.wikipedia.org/wiki/N-gram\">unigrams</a> but to match <em>New York</em> we would have to check <a href=\"https://en.wikipedia.org/wiki/N-gram\">bigrams</a> of the words in the input text. If we were to look up something like <em>Costa del Sol</em>, it would take <a href=\"https://en.wikipedia.org/wiki/N-gram\">trigrams</a> and so on. With some recursive function you might be able to abuse the <code class=\"language-plaintext highlighter-rouge\">find()</code> method of Python strings to avoid the use of <a href=\"https://en.wikipedia.org/wiki/N-gram\">n-grams</a>, but before we go down that route we cut the conclusion short and say that brute-forcing is not the way to go. At best you’re left with <strong><code class=\"language-plaintext highlighter-rouge\">M * N</code></strong> calculations - that is when you’re checking for unigrams. That means every word in the lookup, will be checked against every word of the input text.</p><h2 id=\"what-now-named-entity-extraction\">What now? Named Entity Extraction!</h2><p>To reduce the number of calculations and to avoid the use of n-grams, we have to reduce either <strong><code class=\"language-plaintext highlighter-rouge\">M</code></strong> (the number of words in the lookup) or reduce <strong><code class=\"language-plaintext highlighter-rouge\">N</code></strong> the words in the input text. Reducing <strong><code class=\"language-plaintext highlighter-rouge\">M</code></strong> is off the table, because that’s what we’re looking for. Reducing <strong><code class=\"language-plaintext highlighter-rouge\">N</code></strong> is something we can do though. Cities, countries, and districts are all <a href=\"https://en.wikipedia.org/wiki/Named_entity\">named entities</a>. And Named Entities can be extracted, reducing <strong><code class=\"language-plaintext highlighter-rouge\">N</code></strong> in the process.</p><h3 id=\"spacy\">spaCy</h3><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">spacy</span><span class=\"n\">nlp</span> <span class=\"o\">=</span> <span class=\"n\">spacy</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s\">\"en_core_web_sm\"</span><span class=\"p\">)</span><span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"p\">(</span><span class=\"s\">\"Berlin is an awesome city. It is also great to live in London though.\"</span><span class=\"p\">)</span><span class=\"k\">for</span> <span class=\"n\">ent</span> <span class=\"ow\">in</span> <span class=\"n\">doc</span><span class=\"p\">.</span><span class=\"n\">ents</span><span class=\"p\">:</span>    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">ent</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">ent</span><span class=\"p\">.</span><span class=\"n\">start_char</span><span class=\"p\">,</span> <span class=\"n\">ent</span><span class=\"p\">.</span><span class=\"n\">end_char</span><span class=\"p\">,</span> <span class=\"n\">ent</span><span class=\"p\">.</span><span class=\"n\">label_</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"n\">Berlin</span> <span class=\"mi\">0</span> <span class=\"mi\">6</span> <span class=\"n\">GPE</span><span class=\"o\">&gt;&gt;</span> <span class=\"n\">London</span> <span class=\"mi\">55</span> <span class=\"mi\">61</span> <span class=\"n\">GPE</span></code></pre></div></div><p><a href=\"https://github.com/explosion/spaCy\">spaCy</a> is awesome. It supports a lot of languages. It’s easy to learn, hard to master. It’s super well documented and embedded into a great community and a great ecosystem of plugins/addons/spinoffs. It has a lot of functionality aside from named entity extraction. And it does what it is supposed to do and more. However, named entity extraction with spaCy is still based on a trained model prediction, and even though the core models perform well, they are not 100% accurate. On top of that spaCy is build with C dependencies, which can be a problem in some environments. Also, you are limited to the pre-trained models, unless you want to train your language model from scratch.</p><p>Using spaCy limits us to a limited amount of languages that are supported. Locations that are parsed from an input text and returned by spaCy would still need to be checked against a lookup. Slight variations in the input text would drastically change the result.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">spacy</span><span class=\"n\">nlp</span> <span class=\"o\">=</span> <span class=\"n\">spacy</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s\">\"en_core_web_sm\"</span><span class=\"p\">)</span><span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"p\">(</span><span class=\"s\">\"Berlin city is awesome. But now imagine you live in London city.\"</span><span class=\"p\">)</span><span class=\"k\">for</span> <span class=\"n\">ent</span> <span class=\"ow\">in</span> <span class=\"n\">doc</span><span class=\"p\">.</span><span class=\"n\">ents</span><span class=\"p\">:</span>    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">ent</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">ent</span><span class=\"p\">.</span><span class=\"n\">start_char</span><span class=\"p\">,</span> <span class=\"n\">ent</span><span class=\"p\">.</span><span class=\"n\">end_char</span><span class=\"p\">,</span> <span class=\"n\">ent</span><span class=\"p\">.</span><span class=\"n\">label_</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"n\">Berlin</span> <span class=\"n\">city</span> <span class=\"mi\">0</span> <span class=\"mi\">11</span> <span class=\"n\">GPE</span><span class=\"o\">&gt;&gt;</span> <span class=\"n\">London</span> <span class=\"n\">city</span> <span class=\"mi\">52</span> <span class=\"mi\">63</span> <span class=\"n\">GPE</span></code></pre></div></div><p>Also, there would have to be a process to calculate the occurrences. Spacys dependencies make it harder (not impossible) to deploy in a low-cost environment such as AWS Lambda.</p><h3 id=\"geotext\">GeoText</h3><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">geotext</span> <span class=\"kn\">import</span> <span class=\"n\">GeoText</span><span class=\"n\">places</span> <span class=\"o\">=</span> <span class=\"n\">GeoText</span><span class=\"p\">(</span><span class=\"s\">\"London is a great city\"</span><span class=\"p\">)</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">places</span><span class=\"p\">.</span><span class=\"n\">cities</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"s\">\"London\"</span></code></pre></div></div><p><a href=\"https://github.com/elyase/geotext\">GeoText</a> relies on a single regex search pattern to extract named entities from an input text. Afterward, GeoText tries to match every single one of the entities found to a collection of city and country names one by one. This approach is fast for the 22.000 cities that come with the library, but do not scale well with longer texts and more cities/keywords in a lookup file. <a href=\"https://github.com/elyase/geotext\">GeoText</a> also does not make it easy to bring your data. Also, synonyms are not in the scope of <a href=\"https://github.com/elyase/geotext\">GeoText</a>. Another problem is the regex search pattern that extracts named entities. It is a fine line between matching correctly and matching too much, and it gets even harder to match when city names contain more than a couple of words. Have fun matching something like <em>Friendly Village of Crooked Creek</em>.</p><p>Nonetheless, <a href=\"https://github.com/elyase/geotext\">GeoText</a> comes very close to what I had in mind. <a href=\"https://github.com/elyase/geotext\">GeoText</a> provides named entity recognition, even though it has its flaws. It provides some sample data from Geonames that can work as a lookup. It is a native python implementation and will run anywhere. Where <a href=\"https://github.com/elyase/geotext\">GeoText</a> struggles is that it comes batteries included but doesn’t provide you an opening to bring your data. Also, the regex statement has its limits.</p><h3 id=\"conclusion\">Conclusion</h3><p>By extracting named entities we reduce the number of computations necessary to look up keywords in a text tremendously. However reducing the number of computations is of no great use, if we can’t match the lookup against the extracted entities. Spacy is a great framework, but not the tool for the job. GeoText looks like the tool for the job but is flawed when it comes to entity extraction and flexibility.</p><h2 id=\"aaand-now\">Aaand now?</h2><p>What if instead of reducing <strong><code class=\"language-plaintext highlighter-rouge\">N</code></strong> the number of words in the text to check against a lookup explicitly? What if we would be able to go over the input text, character by character, in one go and would only perform an action if the character is</p><p>a) the start of a word, ergo follows a non-word character and <br />b) the character is even present in our list of keywords to lookup?</p><h3 id=\"flashtext-an-aho-corasick-implementation\">FlashText an Aho-Corasick implementation</h3><p><a href=\"https://github.com/vi3k6i5/flashtext/\">FlashText</a> (see <a href=\"https://arxiv.org/pdf/1711.00046.pdf\">paper</a>) is loosely based on the  <a href=\"https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm\">Aho-Corasick Algorithm</a> used in string-searching. Using the keywords in the lookup data a Trie dictionary is built. The tree is grown by adding edges down from node to node where edges are comprised of letters in each of the strings being added.</p><p align=\"center\"><img src=\"/img/2020-02-flashgeotext/aho-random-trie.png\" alt=\"aho random trie\" />source: <a href=\"https://banay.me/post/aho-corasick/\">banay.me</a></p><p>This allows it to iterate over the input text character by character, and only if the character matches the trie dictionary from start to finish a match is found and stored separately.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">flashtext</span> <span class=\"kn\">import</span> <span class=\"n\">KeywordProcessor</span><span class=\"n\">lookup</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">'Berlin'</span><span class=\"p\">,</span> <span class=\"s\">'Hamburg'</span><span class=\"p\">,</span> <span class=\"s\">'London'</span><span class=\"p\">,</span> <span class=\"s\">'New York'</span><span class=\"p\">]</span><span class=\"n\">input_text</span> <span class=\"o\">=</span> <span class=\"s\">\"Berlin is a great city. Berlin is not as nice as New York, but will do for now.\"</span><span class=\"n\">processor</span> <span class=\"o\">=</span> <span class=\"n\">KeywordProcessor</span><span class=\"p\">(</span><span class=\"n\">case_sensitive</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span><span class=\"n\">processor</span><span class=\"p\">.</span><span class=\"n\">add_keywords_from_list</span><span class=\"p\">(</span><span class=\"n\">lookup</span><span class=\"p\">)</span><span class=\"n\">processor</span><span class=\"p\">.</span><span class=\"n\">extract_keywords</span><span class=\"p\">(</span><span class=\"n\">input_text</span><span class=\"p\">,</span> <span class=\"n\">span_info</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">[(</span><span class=\"s\">'Berlin'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">'Berlin'</span><span class=\"p\">,</span> <span class=\"mi\">24</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">'New York'</span><span class=\"p\">,</span> <span class=\"mi\">49</span><span class=\"p\">,</span> <span class=\"mi\">57</span><span class=\"p\">)]</span></code></pre></div></div><p>This makes FlashText super fast compared to other approaches.</p><p align=\"center\"><img src=\"/img/2020-02-flashgeotext/flashtext-vs-regex.png\" alt=\"flashtext vs regex\" />source: <a href=\"https://github.com/vi3k6i5/flashtext/\">github.com/vi3k6i5/flashtext/</a></p><h2 id=\"flashgeotext\">Flashgeotext</h2><p><a href=\"https://github.com/iwpnd/flashgeotext\">flashgeotext</a> is my approach in making something good like FlashText, even better by adding some quality of life add-ons to it. It is also intended as a homage to <a href=\"https://github.com/elyase/geotext\">GeoText</a> following a <a href=\"https://github.com/elyase/geotext/issues/22\">discussion</a> late last year.</p><h3 id=\"features\">Features</h3><h4 id=\"batteries-included-lookup\">batteries included lookup</h4><p><a href=\"https://github.com/iwpnd/flashgeotext\">flashgeotext</a> comes with batteries included. Just like GeoText, you can add city names from <a href=\"https://geonames.org\">geonames.org</a> as a lookup by default and start going right away.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">flashgeotext.geotext</span> <span class=\"kn\">import</span> <span class=\"n\">GeoText</span><span class=\"n\">geotext</span> <span class=\"o\">=</span> <span class=\"n\">GeoText</span><span class=\"p\">(</span><span class=\"n\">use_demo_data</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span><span class=\"n\">input_text</span> <span class=\"o\">=</span> <span class=\"s\">'''Shanghai. The Chinese Ministry of Finance in Shanghai said that China plans                to cut tariffs on $75 billion worth of goods that the country                imports from the US. Washington welcomes the decision.'''</span><span class=\"n\">geotext</span><span class=\"p\">.</span><span class=\"n\">extract</span><span class=\"p\">(</span><span class=\"n\">input_text</span><span class=\"o\">=</span><span class=\"n\">input_text</span><span class=\"p\">,</span> <span class=\"n\">span_info</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">{</span>    <span class=\"s\">'cities'</span><span class=\"p\">:</span> <span class=\"p\">{</span>        <span class=\"s\">'Shanghai'</span><span class=\"p\">:</span> <span class=\"p\">{</span>            <span class=\"s\">'count'</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>            <span class=\"s\">'span_info'</span><span class=\"p\">:</span> <span class=\"p\">[(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">45</span><span class=\"p\">,</span> <span class=\"mi\">53</span><span class=\"p\">)]</span>            <span class=\"p\">},</span>        <span class=\"s\">'Washington, D.C.'</span><span class=\"p\">:</span> <span class=\"p\">{</span>            <span class=\"s\">'count'</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>            <span class=\"s\">'span_info'</span><span class=\"p\">:</span> <span class=\"p\">[(</span><span class=\"mi\">175</span><span class=\"p\">,</span> <span class=\"mi\">185</span><span class=\"p\">)]</span>            <span class=\"p\">}</span>        <span class=\"p\">},</span>    <span class=\"s\">'countries'</span><span class=\"p\">:</span> <span class=\"p\">{</span>        <span class=\"s\">'China'</span><span class=\"p\">:</span> <span class=\"p\">{</span>            <span class=\"s\">'count'</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>            <span class=\"s\">'span_info'</span><span class=\"p\">:</span> <span class=\"p\">[(</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">69</span><span class=\"p\">)]</span>            <span class=\"p\">},</span>        <span class=\"s\">'United States'</span><span class=\"p\">:</span> <span class=\"p\">{</span>            <span class=\"s\">'count'</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>            <span class=\"s\">'span_info'</span><span class=\"p\">:</span> <span class=\"p\">[(</span><span class=\"mi\">171</span><span class=\"p\">,</span> <span class=\"mi\">173</span><span class=\"p\">)]</span>            <span class=\"p\">}</span>        <span class=\"p\">}</span>    <span class=\"p\">}</span></code></pre></div></div><h4 id=\"improved-data-handling\">improved data handling</h4><p align=\"center\"><img src=\"/img/2020-02-flashgeotext/flashgeotext-diagram-notext.png\" alt=\"flashtext structure\" /></p><p>The idea is to provide a way to add additional data to the already provided data from Geonames or use your data for the lookup entirely. This is achieved by utilizing a quasi <a href=\"https://github.com/faif/python-patterns/blob/master/patterns/structural/composite.py\">composite pattern</a>. You instantiate <code class=\"language-plaintext highlighter-rouge\">LookupData</code> with your data (see <a href=\"https://flashgeotext.iwpnd.pw/usage/#bring-your-own-data\">example</a>) and add that instance to a <code class=\"language-plaintext highlighter-rouge\">LookupDataPool</code>, which is just a collection of <code class=\"language-plaintext highlighter-rouge\">flashtext.KeywordProcessor</code>’s. On <code class=\"language-plaintext highlighter-rouge\">extract()</code> for every <code class=\"language-plaintext highlighter-rouge\">LookupData</code> in the pool there is one passing and extraction over the text, which amounts to the complexity of O(N * number of LookupData). The extracted data is parsed, occurrences are counted, and span information stored and returned.</p><h4 id=\"small-footprint-and-no-dependencies\">small footprint and no dependencies</h4><p>Just like FlashText, flashgeotext does not have any big dependencies which make it easy to use in AWS Lambda and/or AWS Step functions. Once instantiated on a warm AWS Lambda/Stepfunction, it can be reused again and again. You can easily put every city name on earth into a trie dictionary and run it inside an AWS Lambda’s 3072mb of memory.</p><h4 id=\"non-word-boundaries\">non-word-boundaries</h4><p>FlashText’s non-word boundaries that are essential in during keyword extraction are comprised of</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">non_word_boundaries</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">string</span><span class=\"p\">.</span><span class=\"n\">digits</span> <span class=\"o\">+</span> <span class=\"n\">string</span><span class=\"p\">.</span><span class=\"n\">ascii_letters</span> <span class=\"o\">+</span> <span class=\"s\">'_'</span><span class=\"p\">)</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">non_word_boundaries</span><span class=\"p\">)</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">{</span><span class=\"s\">'k'</span><span class=\"p\">,</span> <span class=\"s\">'6'</span><span class=\"p\">,</span> <span class=\"s\">'s'</span><span class=\"p\">,</span> <span class=\"s\">'M'</span><span class=\"p\">,</span> <span class=\"s\">'i'</span><span class=\"p\">,</span> <span class=\"s\">'S'</span><span class=\"p\">,</span> <span class=\"s\">'m'</span><span class=\"p\">,</span> <span class=\"s\">'E'</span><span class=\"p\">,</span> <span class=\"s\">'r'</span><span class=\"p\">,</span> <span class=\"s\">'W'</span><span class=\"p\">,</span> <span class=\"s\">'v'</span><span class=\"p\">,</span> <span class=\"s\">'l'</span><span class=\"p\">,</span> <span class=\"s\">'R'</span><span class=\"p\">,</span> <span class=\"s\">'f'</span><span class=\"p\">,</span> <span class=\"s\">'e'</span><span class=\"p\">,</span> <span class=\"s\">'X'</span><span class=\"p\">,</span> <span class=\"s\">'7'</span><span class=\"p\">,</span> <span class=\"s\">'3'</span><span class=\"p\">,</span> <span class=\"s\">'q'</span><span class=\"p\">,</span> <span class=\"s\">'w'</span><span class=\"p\">,</span> <span class=\"s\">'0'</span><span class=\"p\">,</span> <span class=\"s\">'x'</span><span class=\"p\">,</span> <span class=\"s\">'V'</span><span class=\"p\">,</span> <span class=\"s\">'C'</span><span class=\"p\">,</span> <span class=\"s\">'n'</span><span class=\"p\">,</span> <span class=\"s\">'I'</span><span class=\"p\">,</span> <span class=\"s\">'4'</span><span class=\"p\">,</span> <span class=\"s\">'D'</span><span class=\"p\">,</span> <span class=\"s\">'z'</span><span class=\"p\">,</span> <span class=\"s\">'G'</span><span class=\"p\">,</span> <span class=\"s\">'L'</span><span class=\"p\">,</span> <span class=\"s\">'2'</span><span class=\"p\">,</span> <span class=\"s\">'T'</span><span class=\"p\">,</span> <span class=\"s\">'U'</span><span class=\"p\">,</span> <span class=\"s\">'_'</span><span class=\"p\">,</span> <span class=\"s\">'B'</span><span class=\"p\">,</span> <span class=\"s\">'t'</span><span class=\"p\">,</span> <span class=\"s\">'Q'</span><span class=\"p\">,</span> <span class=\"s\">'d'</span><span class=\"p\">,</span> <span class=\"s\">'9'</span><span class=\"p\">,</span> <span class=\"s\">'h'</span><span class=\"p\">,</span> <span class=\"s\">'o'</span><span class=\"p\">,</span> <span class=\"s\">'c'</span><span class=\"p\">,</span> <span class=\"s\">'u'</span><span class=\"p\">,</span> <span class=\"s\">'P'</span><span class=\"p\">,</span> <span class=\"s\">'K'</span><span class=\"p\">,</span> <span class=\"s\">'Y'</span><span class=\"p\">,</span> <span class=\"s\">'p'</span><span class=\"p\">,</span> <span class=\"s\">'A'</span><span class=\"p\">,</span> <span class=\"s\">'J'</span><span class=\"p\">,</span> <span class=\"s\">'O'</span><span class=\"p\">,</span> <span class=\"s\">'N'</span><span class=\"p\">,</span> <span class=\"s\">'H'</span><span class=\"p\">,</span> <span class=\"s\">'j'</span><span class=\"p\">,</span> <span class=\"s\">'a'</span><span class=\"p\">,</span> <span class=\"s\">'Z'</span><span class=\"p\">,</span> <span class=\"s\">'5'</span><span class=\"p\">,</span> <span class=\"s\">'1'</span><span class=\"p\">,</span> <span class=\"s\">'b'</span><span class=\"p\">,</span> <span class=\"s\">'y'</span><span class=\"p\">,</span> <span class=\"s\">'F'</span><span class=\"p\">,</span> <span class=\"s\">'8'</span><span class=\"p\">,</span> <span class=\"s\">'g'</span><span class=\"p\">}</span></code></pre></div></div><p>This has been are-occurring <a href=\"https://github.com/vi3k6i5/flashtext/issues/87\">issue</a> in the past. The fact that there are a lot of characters missing, makes it, that FlashText is unreliable at best for every other language than English. I’m currently working on handling different alphabets within flashgeotext to make it possible to reliably extract keywords from a text written in Cyrillic or Greek characters also.</p><h1 id=\"conclusion-1\">Conclusion</h1><p>Flashgeotext can help you to extract city and country names in your text processing pipelines. It gives you the ability to add your data instead of or on top of the demo data provided. It is incredibly fast, reliable, written in native Python and easy to set up in an AWS Lambda/Stepfunction environment or as <a href=\"https://airflow.apache.org/\">Airflow</a> / <a href=\"https://github.com/spotify/luigi\">Luigi</a> / <a href=\"https://www.prefect.io/\">Prefect</a> directed acyclic graph.</p>",
            "url": "https://iwpnd.pw//articles/2020-02/flashgeotext-library",
            
            
            
            "tags": ["python","pytest","flashgeotext"],
            
            "date_published": "2020-02-25T08:13:37+00:00",
            "date_modified": "2020-02-25T08:13:37+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-02/pytest-fixtures",
            "title": "Stop killing kittens. Or, how to use pytest.fixtures to remove redundancy in your test suite",
            "summary": null,
            "content_text": "Last week I wrote about how pytest.mark.parametrizing can be used to remove some redundancy in your test suite. Today let’s talk pytest.fixture and how it helps you to clean up the mess that is your test suite with reusable variables, connections and/or objects.stop killing kittens with pytest.fixtureIf you obey the testing goat like you should, you practice Test-Driven-Development. Therefore you make sure to code in small incremental steps. During the refactoring phase, you will notice that repetition is omnipresent. You always pass the same data to the tests and you often instantiate the same objects. However, instead of running the same code for every test, you can attach so-called fixture functions to the test, that run and return the data to the test when needed in a reliable, consistent and repeatable manner.Let’s try this in an example.class MyClass:    def __init__(self, name: str, foo: int, bar: int) -&gt; None:        self.name = name        self.foo = foo        self.bar = barSo, you have a class MyClass and you always use the same instance of the class in your test suite.# test_myclass.pyimport MyClassdef test_myclass_1():    myclass = MyClass(name=\"Panda\", foo=13, bar=37)        assert myclass.foo + 24 == 37def test_myclass_2():    myclass = MyClass(name=\"Panda\", foo=13, bar=37)        assert myclass.bar - 24 == 13Instead of instantiating MyClass on every test, you can add a pytest.fixture and pass it as an argument to every test. Fixture functions are registered by marking them with @pytest.fixture.# test_myclass.pyimport MyClassimport pytest@pytest.fixturedef myclass():    myclass = MyClass(name=\"Panda\", foo=13, bar=37)    return myclassdef test_myclass_1(myclass):    assert myclass.foo + 24 == 37def test_myclass_2(myclass):    assert myclass.bar - 24 == 13Now pytest finds the test_myclass_1 and test_myclass_2 test functions, because of the test_ prefix. The test functions need a function argument named myclass. A matching fixture function is discovered by looking for a fixture-marked function named myclass. Pytest calls myclass() to create an instance of MyClass and returns the instance to either test function.Defining the fixture within test_myclass.py comes with the trade-off that you limit the scope of your fixture to the test_myclass.py test file - it cannot be used in another test file that way. So what do you do? Define the same fixture in another class? No, that would cause code repetition again. Luckily pytest has a solution for that.conftestTo make fixtures available to your entire test suite you use a conftest.py file in your tests folder. In this file you store the fixtures you plan to use across your tests.# conftest.pyimport pytest@pytest.fixturedef myclass():    myclass = MyClass(name=\"Panda\", foo=13, bar=37)    return myclass└── tests    ├── conftest.py    ├── integration    │   └── test_integration.py    └── unit        ├── test_myclass_1.py        └── test_myclass_2.pyNow that conftest.py is present. For every test function that has myclass, pytest will pass the return value of myclass() fixture to the test function.conclusionStop killing kittens.Instead leverage fixtures and the concept of dependency injection, which means that an object (the fixture) supplies the dependencies to another object (the test function). This concept makes for a very modular, better manageable and repetition-free test suites.",
            "content_html": "<p>Last week I wrote about how <a href=\"https://iwpnd.pw/articles/2020-02/pytest-parametrize\">pytest.mark.parametrizing</a> can be used to remove some redundancy in your test suite. Today let’s talk <code class=\"language-plaintext highlighter-rouge\">pytest.fixture</code> and how it helps you to clean up the mess that is your test suite with reusable variables, connections and/or objects.</p><p align=\"center\"><img src=\"/img/2020-02-pytest-fixtures/duplication-kills-kittens.jpg\" alt=\"duplication kills kittens\" /></p><h2 id=\"stop-killing-kittens-with-pytestfixture\">stop killing kittens with pytest.fixture</h2><p>If you <a href=\"https://www.obeythetestinggoat.com/\">obey the testing goat</a> like you should, you practice Test-Driven-Development. Therefore you make sure to code in small incremental steps. During the refactoring phase, you will notice that repetition is omnipresent. You always pass the same data to the tests and you often instantiate the same objects. However, instead of running the same code for every test, you can attach so-called fixture functions to the test, that run and return the data to the test when needed in a reliable, consistent and repeatable manner.</p><p>Let’s try this in an example.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">class</span> <span class=\"nc\">MyClass</span><span class=\"p\">:</span>    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">foo</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">bar</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"bp\">None</span><span class=\"p\">:</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"n\">name</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">foo</span> <span class=\"o\">=</span> <span class=\"n\">foo</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">bar</span> <span class=\"o\">=</span> <span class=\"n\">bar</span></code></pre></div></div><p>So, you have a class <code class=\"language-plaintext highlighter-rouge\">MyClass</code> and you always use the same instance of the class in your test suite.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># test_myclass.py</span><span class=\"kn\">import</span> <span class=\"nn\">MyClass</span><span class=\"k\">def</span> <span class=\"nf\">test_myclass_1</span><span class=\"p\">():</span>    <span class=\"n\">myclass</span> <span class=\"o\">=</span> <span class=\"n\">MyClass</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"Panda\"</span><span class=\"p\">,</span> <span class=\"n\">foo</span><span class=\"o\">=</span><span class=\"mi\">13</span><span class=\"p\">,</span> <span class=\"n\">bar</span><span class=\"o\">=</span><span class=\"mi\">37</span><span class=\"p\">)</span>        <span class=\"k\">assert</span> <span class=\"n\">myclass</span><span class=\"p\">.</span><span class=\"n\">foo</span> <span class=\"o\">+</span> <span class=\"mi\">24</span> <span class=\"o\">==</span> <span class=\"mi\">37</span><span class=\"k\">def</span> <span class=\"nf\">test_myclass_2</span><span class=\"p\">():</span>    <span class=\"n\">myclass</span> <span class=\"o\">=</span> <span class=\"n\">MyClass</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"Panda\"</span><span class=\"p\">,</span> <span class=\"n\">foo</span><span class=\"o\">=</span><span class=\"mi\">13</span><span class=\"p\">,</span> <span class=\"n\">bar</span><span class=\"o\">=</span><span class=\"mi\">37</span><span class=\"p\">)</span>        <span class=\"k\">assert</span> <span class=\"n\">myclass</span><span class=\"p\">.</span><span class=\"n\">bar</span> <span class=\"o\">-</span> <span class=\"mi\">24</span> <span class=\"o\">==</span> <span class=\"mi\">13</span></code></pre></div></div><p>Instead of instantiating MyClass on every test, you can add a <code class=\"language-plaintext highlighter-rouge\">pytest.fixture</code> and pass it as an argument to every test. Fixture functions are registered by marking them with <code class=\"language-plaintext highlighter-rouge\">@pytest.fixture</code>.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># test_myclass.py</span><span class=\"kn\">import</span> <span class=\"nn\">MyClass</span><span class=\"kn\">import</span> <span class=\"nn\">pytest</span><span class=\"o\">@</span><span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"n\">fixture</span><span class=\"k\">def</span> <span class=\"nf\">myclass</span><span class=\"p\">():</span>    <span class=\"n\">myclass</span> <span class=\"o\">=</span> <span class=\"n\">MyClass</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"Panda\"</span><span class=\"p\">,</span> <span class=\"n\">foo</span><span class=\"o\">=</span><span class=\"mi\">13</span><span class=\"p\">,</span> <span class=\"n\">bar</span><span class=\"o\">=</span><span class=\"mi\">37</span><span class=\"p\">)</span>    <span class=\"k\">return</span> <span class=\"n\">myclass</span><span class=\"k\">def</span> <span class=\"nf\">test_myclass_1</span><span class=\"p\">(</span><span class=\"n\">myclass</span><span class=\"p\">):</span>    <span class=\"k\">assert</span> <span class=\"n\">myclass</span><span class=\"p\">.</span><span class=\"n\">foo</span> <span class=\"o\">+</span> <span class=\"mi\">24</span> <span class=\"o\">==</span> <span class=\"mi\">37</span><span class=\"k\">def</span> <span class=\"nf\">test_myclass_2</span><span class=\"p\">(</span><span class=\"n\">myclass</span><span class=\"p\">):</span>    <span class=\"k\">assert</span> <span class=\"n\">myclass</span><span class=\"p\">.</span><span class=\"n\">bar</span> <span class=\"o\">-</span> <span class=\"mi\">24</span> <span class=\"o\">==</span> <span class=\"mi\">13</span></code></pre></div></div><p>Now pytest finds the <code class=\"language-plaintext highlighter-rouge\">test_myclass_1</code> and <code class=\"language-plaintext highlighter-rouge\">test_myclass_2</code> test functions, because of the <code class=\"language-plaintext highlighter-rouge\">test_</code> prefix. The test functions need a function argument named <code class=\"language-plaintext highlighter-rouge\">myclass</code>. A matching fixture function is discovered by looking for a fixture-marked function named <code class=\"language-plaintext highlighter-rouge\">myclass</code>. Pytest calls <code class=\"language-plaintext highlighter-rouge\">myclass()</code> to create an instance of <code class=\"language-plaintext highlighter-rouge\">MyClass</code> and returns the instance to either test function.</p><p>Defining the fixture within <code class=\"language-plaintext highlighter-rouge\">test_myclass.py</code> comes with the trade-off that you limit the scope of your fixture to the <code class=\"language-plaintext highlighter-rouge\">test_myclass.py</code> test file - it cannot be used in another test file that way. So what do you do? Define the same fixture in another class? No, that would cause code repetition again. Luckily <code class=\"language-plaintext highlighter-rouge\">pytest</code> has a solution for that.</p><h2 id=\"conftest\">conftest</h2><p>To make fixtures available to your entire test suite you use a <code class=\"language-plaintext highlighter-rouge\">conftest.py</code> file in your tests folder. In this file you store the fixtures you plan to use across your tests.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># conftest.py</span><span class=\"kn\">import</span> <span class=\"nn\">pytest</span><span class=\"o\">@</span><span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"n\">fixture</span><span class=\"k\">def</span> <span class=\"nf\">myclass</span><span class=\"p\">():</span>    <span class=\"n\">myclass</span> <span class=\"o\">=</span> <span class=\"n\">MyClass</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"Panda\"</span><span class=\"p\">,</span> <span class=\"n\">foo</span><span class=\"o\">=</span><span class=\"mi\">13</span><span class=\"p\">,</span> <span class=\"n\">bar</span><span class=\"o\">=</span><span class=\"mi\">37</span><span class=\"p\">)</span>    <span class=\"k\">return</span> <span class=\"n\">myclass</span></code></pre></div></div><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>└── tests    ├── conftest.py    ├── integration    │   └── test_integration.py    └── unit        ├── test_myclass_1.py        └── test_myclass_2.py</code></pre></div></div><p>Now that <code class=\"language-plaintext highlighter-rouge\">conftest.py</code> is present. For every test function that has <code class=\"language-plaintext highlighter-rouge\">myclass</code>, pytest will pass the return value of <code class=\"language-plaintext highlighter-rouge\">myclass()</code> fixture to the test function.</p><h2 id=\"conclusion\">conclusion</h2><p>Stop killing kittens.</p><p>Instead leverage fixtures and the concept of dependency injection, which means that an object (the fixture) supplies the dependencies to another object (the test function). This concept makes for a very modular, better manageable and repetition-free test suites.</p>",
            "url": "https://iwpnd.pw//articles/2020-02/pytest-fixtures",
            
            
            
            "tags": ["python","pytest"],
            
            "date_published": "2020-02-20T08:13:37+00:00",
            "date_modified": "2020-02-20T08:13:37+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-02/pytest-parametrize",
            "title": "use pytest.mark.parametrize to remove redundancy in your test suite",
            "summary": null,
            "content_text": "The further you go down the rabbit hole that is programming, the more you will get exposed to a thing called unit testing, or testing in general and eventually to test-driven development. There is a couple of things you need to get started. You need to understand  what testing is, and why you need to test your code and you will have to face the hard reality:After that, you will pick your test framework and get to work. I opted for pytest for no particular reason at first. I noticed it was used in other teams at work, and I found it to be mentioned a little more often at conferences than other frameworks. I got the hang on it fairly quickly, yet I’m sure to not tap into its full potential and I’m still discovering things the further I get that make testing even more efficient. One of those things is the parametrizing of test functions.example functionSay you have a simple function that takes two float or int values and adds them together and throws a custom TypeError if the values your function received are not int or float.def add(a: Union[float, int], b: Union[float, int]) -&gt; Union[float, int]:    if any([isinstance(a, _type) for _type in [int, float]]) and any(        [isinstance(b, _type) for _type in [int, float]]    ):        return a + b    else:        raise TypeError(\"a and b can either be int or float\")Now you want to test that function using pytestdef test_example_success():    result = add(a=2, b=3)    assert result == 5and it passes. No big surprise here.redundancySo what you want to do now is to check whether TypeError is ever raised, if the input is something other than an int or float.My naive understanding was, that I would just create a couple of test functions, that would test different types of inputs and see if pytest.raises would raise that TypeError.def test_example_add_fails_string():    with pytest.raises(TypeError):        result = add(a=\"2\", b=\"3\")def test_example_add_fails_list():    with pytest.raises(TypeError):        result = add(a=[2], b=[3])While that works just fine, there is quite some redundancy in here, even if we are just testing that simple add() function. So to avoid that we can use the pytest.mark.parametrize decorator that allows us to test a multitude of arguments against the same test functions, without having to duplicate the test function itself.parametrizing@pytest.mark.parametrize(    \"a, b, expectation\", # parameter for the test function    [        (\"3\", 4, pytest.raises(TypeError)),        (3, \"4\", pytest.raises(TypeError)),        ([\"3\"], [\"4\"], pytest.raises(TypeError)),        (\"3\", \"4\", pytest.raises(TypeError)),         # [...]    ])def test_example_add_raises(a, b, expectation):    with expectation:        result = add(a,b)As you can see pytest runs the same test function but separately with the parameters that you set in the decorator which essentially removes code duplication in your unit tests. And since every set of input parameters is appended in brackets in the pytest log, you can easily spot which input does not behave as intended. Of course, you can use the same idea to test the input that you want to succeed.@pytest.mark.parametrize(    \"a, b, expectation\",     [        (1, 3, 4),        (1.3, 3.7, 5),        (-1, 4, 3)    ])def test_example_success(a, b, expectation):    result = add(a=a, b=b)    assert result == expectationThat’s it for a quick introduction. There’s a lot more to learn on parametrize-basics and even more when you went past the basics.Next up, fixtures.",
            "content_html": "<p>The further you go down the rabbit hole that is programming, the more you will get exposed to a thing called unit testing, or testing in general and eventually to test-driven development. There is a couple of things you need to get started. You need to understand  <a href=\"https://jeffknupp.com/blog/2013/12/09/improve-your-python-understanding-unit-testing/\">what testing is, and why you need to test</a> your code and you will have to face the hard reality:</p><p align=\"center\"><img src=\"/img/2020-02-pytest/no.png\" alt=\"just no\" /></p><p>After that, you will pick your test framework and get to work. I opted for <a href=\"https://docs.pytest.org/en/latest/\">pytest</a> for no particular reason at first. I noticed it was used in other teams at work, and I found it to be mentioned a little more often at conferences than other frameworks. I got the hang on it fairly quickly, yet I’m sure to not tap into its full potential and I’m still discovering things the further I get that make testing even more efficient. One of those things is the <a href=\"http://doc.pytest.org/en/latest/parametrize.html#parametrize-basics\">parametrizing of test functions</a>.</p><h2 id=\"example-function\">example function</h2><p>Say you have a simple function that takes two <code class=\"language-plaintext highlighter-rouge\">float</code> or <code class=\"language-plaintext highlighter-rouge\">int</code> values and adds them together and throws a custom <code class=\"language-plaintext highlighter-rouge\">TypeError</code> if the values your function received are not int or float.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">],</span> <span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">])</span> <span class=\"o\">-&gt;</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]:</span>    <span class=\"k\">if</span> <span class=\"nb\">any</span><span class=\"p\">([</span><span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">_type</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">_type</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]])</span> <span class=\"ow\">and</span> <span class=\"nb\">any</span><span class=\"p\">(</span>        <span class=\"p\">[</span><span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">_type</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">_type</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>    <span class=\"p\">):</span>        <span class=\"k\">return</span> <span class=\"n\">a</span> <span class=\"o\">+</span> <span class=\"n\">b</span>    <span class=\"k\">else</span><span class=\"p\">:</span>        <span class=\"k\">raise</span> <span class=\"nb\">TypeError</span><span class=\"p\">(</span><span class=\"s\">\"a and b can either be int or float\"</span><span class=\"p\">)</span></code></pre></div></div><p>Now you want to test that function using pytest</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">test_example_success</span><span class=\"p\">():</span>    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>    <span class=\"k\">assert</span> <span class=\"n\">result</span> <span class=\"o\">==</span> <span class=\"mi\">5</span></code></pre></div></div><p><br /></p><p align=\"center\"><img src=\"/img/2020-02-pytest/screenshot_test.png\" alt=\"pytest output simple test\" /></p><p><br /></p><p>and it passes. No big surprise here.</p><h3 id=\"redundancy\">redundancy</h3><p>So what you want to do now is to check whether <code class=\"language-plaintext highlighter-rouge\">TypeError</code> is ever raised, if the input is something other than an <code class=\"language-plaintext highlighter-rouge\">int</code> or <code class=\"language-plaintext highlighter-rouge\">float</code>.My naive understanding was, that I would just create a couple of test functions, that would test different types of inputs and see if <code class=\"language-plaintext highlighter-rouge\">pytest.raises</code> would raise that <code class=\"language-plaintext highlighter-rouge\">TypeError</code>.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">test_example_add_fails_string</span><span class=\"p\">():</span>    <span class=\"k\">with</span> <span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"n\">raises</span><span class=\"p\">(</span><span class=\"nb\">TypeError</span><span class=\"p\">):</span>        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"o\">=</span><span class=\"s\">\"2\"</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"o\">=</span><span class=\"s\">\"3\"</span><span class=\"p\">)</span><span class=\"k\">def</span> <span class=\"nf\">test_example_add_fails_list</span><span class=\"p\">():</span>    <span class=\"k\">with</span> <span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"n\">raises</span><span class=\"p\">(</span><span class=\"nb\">TypeError</span><span class=\"p\">):</span>        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"n\">b</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">])</span></code></pre></div></div><p><br /></p><p align=\"center\"><img src=\"/img/2020-02-pytest/screenshot_redundancy.png\" alt=\"pytest output redundant tests\" /></p><p><br /></p><p>While that works just fine, there is quite some redundancy in here, even if we are just testing that simple <code class=\"language-plaintext highlighter-rouge\">add()</code> function. So to avoid that we can use the <code class=\"language-plaintext highlighter-rouge\">pytest.mark.parametrize</code> decorator that allows us to test a multitude of arguments against the same test functions, without having to duplicate the test function itself.</p><h3 id=\"parametrizing\">parametrizing</h3><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">@</span><span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"n\">mark</span><span class=\"p\">.</span><span class=\"n\">parametrize</span><span class=\"p\">(</span>    <span class=\"s\">\"a, b, expectation\"</span><span class=\"p\">,</span> <span class=\"c1\"># parameter for the test function</span>    <span class=\"p\">[</span>        <span class=\"p\">(</span><span class=\"s\">\"3\"</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"n\">raises</span><span class=\"p\">(</span><span class=\"nb\">TypeError</span><span class=\"p\">)),</span>        <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s\">\"4\"</span><span class=\"p\">,</span> <span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"n\">raises</span><span class=\"p\">(</span><span class=\"nb\">TypeError</span><span class=\"p\">)),</span>        <span class=\"p\">([</span><span class=\"s\">\"3\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s\">\"4\"</span><span class=\"p\">],</span> <span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"n\">raises</span><span class=\"p\">(</span><span class=\"nb\">TypeError</span><span class=\"p\">)),</span>        <span class=\"p\">(</span><span class=\"s\">\"3\"</span><span class=\"p\">,</span> <span class=\"s\">\"4\"</span><span class=\"p\">,</span> <span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"n\">raises</span><span class=\"p\">(</span><span class=\"nb\">TypeError</span><span class=\"p\">)),</span>         <span class=\"c1\"># [...]</span>    <span class=\"p\">]</span><span class=\"p\">)</span><span class=\"k\">def</span> <span class=\"nf\">test_example_add_raises</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">expectation</span><span class=\"p\">):</span>    <span class=\"k\">with</span> <span class=\"n\">expectation</span><span class=\"p\">:</span>        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span><span class=\"n\">b</span><span class=\"p\">)</span></code></pre></div></div><p><br /></p><p align=\"center\"><img src=\"/img/2020-02-pytest/screenshot_result_parametrize.png\" alt=\"pytest output parametrized test\" /></p><p><br /></p><p>As you can see pytest runs the same test function but separately with the parameters that you set in the <code class=\"language-plaintext highlighter-rouge\">decorator</code> which essentially removes code duplication in your unit tests. And since every set of input parameters is appended in brackets in the pytest log, you can easily spot which input does not behave as intended. Of course, you can use the same idea to test the input that you want to succeed.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">@</span><span class=\"n\">pytest</span><span class=\"p\">.</span><span class=\"n\">mark</span><span class=\"p\">.</span><span class=\"n\">parametrize</span><span class=\"p\">(</span>    <span class=\"s\">\"a, b, expectation\"</span><span class=\"p\">,</span>     <span class=\"p\">[</span>        <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">),</span>        <span class=\"p\">(</span><span class=\"mf\">1.3</span><span class=\"p\">,</span> <span class=\"mf\">3.7</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span>        <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span>    <span class=\"p\">]</span><span class=\"p\">)</span><span class=\"k\">def</span> <span class=\"nf\">test_example_success</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">expectation</span><span class=\"p\">):</span>    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"o\">=</span><span class=\"n\">b</span><span class=\"p\">)</span>    <span class=\"k\">assert</span> <span class=\"n\">result</span> <span class=\"o\">==</span> <span class=\"n\">expectation</span></code></pre></div></div><p><br /></p><p align=\"center\"><img src=\"/img/2020-02-pytest/screenshot_parametrize-success.png\" alt=\"pytest output parametrized test success\" /></p><p><br /></p><p>That’s it for a quick introduction. There’s a lot more to learn on <a href=\"http://doc.pytest.org/en/latest/parametrize.html#parametrize-basics\">parametrize-basics</a> and even more when you went <a href=\"http://doc.pytest.org/en/latest/example/parametrize.html\">past the basics</a>.</p><p>Next up, <code class=\"language-plaintext highlighter-rouge\">fixtures</code>.</p>",
            "url": "https://iwpnd.pw//articles/2020-02/pytest-parametrize",
            
            
            
            "tags": ["python","pytest"],
            
            "date_published": "2020-02-12T08:13:37+00:00",
            "date_modified": "2020-02-12T08:13:37+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-02/streamlit-application-development-inside-docker",
            "title": "rapidly build and deploy applications with streamlit and docker",
            "summary": null,
            "content_text": "Ever since May 2019, funny enough right AFTER my wedding, I decided to lose weight. Spoiler, the endeavor has been successful, but this is hardly blog-worthy. So, I was looking to apply what I’ve learned during my last sessions with FastAPI on a new project, preferably one that made use of a database this time, and that I could use daily if I wanted to. The result is weightwhat, a CRUD API using both FastAPI and PostgreSQL to document my weight-loss journey. A day’s work and I had this thing up and running, so far so good, and even learned a couple of new things about docker-compose. Now, while I already talked about a big plus of FastAPI, the automatic Swagger UI that is created with your application, I felt that some kind of frontend with some visualizations to show the progress wouldn’t hurt. While in the past I did something like that with Flask and Jinja, I feel that visually and functionally those efforts were underwhelming to say the least. Adding some frontend development skills to my skillset is eventually on the list, but for now, I wanted to have a solution that is in a more immediate reach.Enter streamlit  “Streamlit is an open-source app framework for Machine Learning and Data Science teams. Create beautiful data apps in hours, not weeks. All in pure Python. All for free.” streamlitOkay, that sounds promising.I like me some sprinkleFrom the documentation, it says to “sprinkle a few Streamlit commands” into a Python script and run the script.import streamlit as stname = \"Ben\"st.title(f\"Hello {name}\")st.subheader(\"How are you?\")$ streamlit run app/main.py&gt; You can now view your Streamlit app in your browser. Local URL: http://localhost:8502 Network URL: http://192.168.178.21:8502That’s it? That’s it! Once the script is executed streamlit will spawn a server in the background that takes your script and renders the output. You can reach it via localhost in your home network or your companies network to quickly share it with your colleagues or to check how the application looks on your mobile device without going through the effort of deploying it.Let’s add a little more to it.import streamlit as stname = st.text_input(\"What's your name?\")if name: st.title(f\"Hello {name}\") st.subheader(\"How are you?\")So what can we unpack here? We have some interactivity, and we have conditions for the elements that are displayed. All that is possible with some magic. Whenever there is a variable on a single line, or streamlit notices one of its commands, they will be published in your app. So, defining a variable does not print the variable, however putting it on its own, will prompt streamlit to show its value.Now what?Adrien Treuille summed streamlit up in a great blog post on Medium where he goes into more detail on the architecture and some more examples.I think one can immediately see why streamlit excites me. Never could you create an application faster than now with streamlit. You do not have to rely on your frontend engineers, you don’t have to wait for them to allocate some time to your little project. Nor do you have to wait until somebody else can use your model for inference, see your visualization without handing out files or explore your datasets. Sure, you could have that interactivity and visualization in a jupyter notebook, but the fun stops when you have to version control that beast. I’m hooked, are you?Developing streamlit inside a containerUntil I have tested to deploy a streamlit application in AWS Lambda, I assume that the easiest way is to develop it in a container and ship that, to avoid shipping my machine. For that purpose, I created a simple template for you to try it out.# DockerfileFROM python:3.7WORKDIR /usr/src/appENV PYTHONDONTWRITEBYTECODE 1ENV PYTHONUNBUFFERED 1COPY ./requirements.txt /usr/src/app/requirements.txt # atleast: streamlitRUN pip install --upgrade pip setuptools wheel \\ &amp;&amp; pip install -r requirements.txt \\ &amp;&amp; rm -rf /root/.cache/pipCOPY ./ /usr/src/app# docker-compose.ymlversion: '3.7'services: app: build: ./ command: streamlit run app/main.py --server.port 8501 volumes: - ./:/usr/src/app ports: - 8501:8501 image: yourstreamlitapp:latestThis tells docker to create a service, from the dockerfile, mount your folder to the container, expose the ports and tag that image.Now bring that service up withdocker-compose -d up --buildThis will run the container in -d detached mode in the background and build the image if it isn’t present on your local machine. You can now reach your application at http://localhost:8502. If that doesn’t work check if your service is up and running docker-compose ps, and if it is running but the application is not displayed you can check the logs like so docker-compose logs --follow app.Since docker-compose is mounting your current directory, your container will always have your current application main.py, therefor streamlit will also re-run, if it detects a change made to your application. This way you can incrementally build, test and try your application.Once you’re done:docker-compose downThat’s it. :ship:Weightwhat?Now, back to that CRUD api that I needed a frontend for. It turns out streamlit is compatible with a lot of the major libraries and frameworks that we all love. I used that chance to finally learn a little about declarative visualization with Altair. Altair is not only highly customizable but also very well documented and allows for easy to implement interactivity, which is a nice-to-have on an application evolving around data exploration. You can check it out here.",
            "content_html": "<p>Ever since May 2019, funny enough right AFTER my wedding, I decided to lose weight. Spoiler, the endeavor has been successful, but this is hardly blog-worthy. So, I was looking to apply what I’ve learned during my last sessions with <a href=\"https://iwpnd.pw/articles/2020-01/deploy-FastAPI-to-aws-lambda\">FastAPI</a> on a new project, preferably one that made use of a database this time, and that I could use daily if I wanted to. The result is <a href=\"https://github.com/iwpnd/weightwhat\">weightwhat</a>, a CRUD API using both FastAPI and PostgreSQL to document my weight-loss journey. A day’s work and I had this thing up and running, so far so good, and even learned a couple of new things about docker-compose. Now, while I already talked about a big plus of FastAPI, the automatic <a href=\"https://iwpnd.pw/articles/2020-01/opinion-on-FastAPI\">Swagger UI that is created with your application</a>, I felt that some kind of frontend with some visualizations to show the progress wouldn’t hurt. While in the past I did something like that with <a href=\"https://github.com/pallets/flask\">Flask</a> and <a href=\"https://github.com/pallets/jinja\">Jinja</a>, I feel that visually and functionally those efforts were <a href=\"https://github.com/iwpnd/geojsonconverter\">underwhelming</a> to say the least. Adding some frontend development skills to my skillset is eventually on the list, but for now, I wanted to have a solution that is in a more immediate reach.</p><h2 id=\"enter-streamlit\">Enter streamlit</h2><blockquote>  <p>“Streamlit is an open-source app framework for Machine Learning and Data Science teams. Create beautiful data apps in hours, not weeks. All in pure Python. All for free.” <a href=\"https://www.streamlit.io/\">streamlit</a></p></blockquote><p>Okay, that sounds promising.</p><h3 id=\"i-like-me-some-sprinkle\">I like me some sprinkle</h3><p>From the documentation, it says to “sprinkle a few Streamlit commands” into a Python script and run the script.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">streamlit</span> <span class=\"k\">as</span> <span class=\"n\">st</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s\">\"Ben\"</span><span class=\"n\">st</span><span class=\"p\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Hello </span><span class=\"si\">{</span><span class=\"n\">name</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span><span class=\"n\">st</span><span class=\"p\">.</span><span class=\"n\">subheader</span><span class=\"p\">(</span><span class=\"s\">\"How are you?\"</span><span class=\"p\">)</span></code></pre></div></div><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>streamlit run app/main.py<span class=\"o\">&gt;</span> You can now view your Streamlit app <span class=\"k\">in </span>your browser. Local URL: http://localhost:8502 Network URL: http://192.168.178.21:8502</code></pre></div></div><p align=\"center\"><img src=\"/img/2020-02-streamlit/streamlit1.png\" alt=\"streamlit in browser\" /></p><p>That’s it? That’s it! Once the script is executed streamlit will spawn a server in the background that takes your script and renders the output. You can reach it via <code class=\"language-plaintext highlighter-rouge\">localhost</code> in your home network or your companies network to quickly share it with your colleagues or to check how the application looks on your mobile device without going through the effort of deploying it.</p><p>Let’s add a little more to it.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">streamlit</span> <span class=\"k\">as</span> <span class=\"n\">st</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"n\">st</span><span class=\"p\">.</span><span class=\"n\">text_input</span><span class=\"p\">(</span><span class=\"s\">\"What's your name?\"</span><span class=\"p\">)</span><span class=\"k\">if</span> <span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"n\">st</span><span class=\"p\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Hello </span><span class=\"si\">{</span><span class=\"n\">name</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span> <span class=\"n\">st</span><span class=\"p\">.</span><span class=\"n\">subheader</span><span class=\"p\">(</span><span class=\"s\">\"How are you?\"</span><span class=\"p\">)</span></code></pre></div></div><p align=\"center\"><img src=\"/img/2020-02-streamlit/screenrecording1.gif\" alt=\"streamlit in browser\" /></p><p>So what can we unpack here? We have some interactivity, and we have conditions for the elements that are displayed. All that is possible with some <a href=\"https://docs.streamlit.io/api.html#magic-commands\">magic</a>. Whenever there is a variable on a single line, or streamlit notices one of its commands, they will be published in your app. So, defining a variable does not print the variable, however putting it on its own, will prompt streamlit to show its value.</p><h3 id=\"now-what\">Now what?</h3><p>Adrien Treuille summed streamlit up in a great <a href=\"https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace\">blog post</a> on Medium where he goes into more detail on the architecture and some more examples.</p><p>I think one can immediately see why streamlit excites me. Never could you create an application faster than now with <a href=\"https://www.streamlit.io/\">streamlit</a>. You do not have to rely on your frontend engineers, you don’t have to wait for them to allocate some time to your little project. Nor do you have to wait until somebody else can use your model for inference, see your visualization without handing out files or explore your datasets. Sure, you could have that interactivity and visualization in a jupyter notebook, but the fun stops when you have to version control that beast. I’m hooked, are you?</p><h2 id=\"developing-streamlit-inside-a-container\">Developing streamlit inside a container</h2><p>Until I have tested to deploy a streamlit application in AWS Lambda, I assume that the easiest way is to develop it in a container and ship that, to <a href=\"https://pics.me.me/it-works-on-my-machine-then-well-ship-your-machine-62072263.png\">avoid shipping my machine</a>. For that purpose, I created a <a href=\"https://github.com/iwpnd/streamlit-docker-example\">simple template</a> for you to try it out.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Dockerfile</span>FROM python:3.7WORKDIR /usr/src/appENV PYTHONDONTWRITEBYTECODE 1ENV PYTHONUNBUFFERED 1COPY ./requirements.txt /usr/src/app/requirements.txt <span class=\"c\"># atleast: streamlit</span>RUN pip <span class=\"nb\">install</span> <span class=\"nt\">--upgrade</span> pip setuptools wheel <span class=\"se\">\\</span> <span class=\"o\">&amp;&amp;</span> pip <span class=\"nb\">install</span> <span class=\"nt\">-r</span> requirements.txt <span class=\"se\">\\</span> <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">rm</span> <span class=\"nt\">-rf</span> /root/.cache/pipCOPY ./ /usr/src/app</code></pre></div></div><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># docker-compose.yml</span>version: <span class=\"s1\">'3.7'</span>services: app: build: ./ <span class=\"nb\">command</span>: streamlit run app/main.py <span class=\"nt\">--server</span>.port 8501 volumes: - ./:/usr/src/app ports: - 8501:8501 image: yourstreamlitapp:latest</code></pre></div></div><p>This tells docker to create a service, from the <code class=\"language-plaintext highlighter-rouge\">dockerfile</code>, mount your folder to the container, expose the ports and tag that image.Now bring that service up with</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>docker-compose <span class=\"nt\">-d</span> up <span class=\"nt\">--build</span></code></pre></div></div><p>This will run the container in <code class=\"language-plaintext highlighter-rouge\">-d</code> detached mode in the background and build the image if it isn’t present on your local machine. You can now reach your application at <a href=\"\">http://localhost:8502</a>. If that doesn’t work check if your service is up and running <code class=\"language-plaintext highlighter-rouge\">docker-compose ps</code>, and if it is running but the application is not displayed you can check the logs like so <code class=\"language-plaintext highlighter-rouge\">docker-compose logs --follow app</code>.</p><p>Since docker-compose is mounting your current directory, your container will always have your current application <code class=\"language-plaintext highlighter-rouge\">main.py</code>, therefor streamlit will also re-run, if it detects a change made to your application. This way you can incrementally build, test and try your application.Once you’re done:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>docker-compose down</code></pre></div></div><p>That’s it. :ship:</p><h2 id=\"weightwhat\">Weightwhat?</h2><p>Now, back to that CRUD api that I needed a frontend for. It turns out streamlit is compatible with a lot of the major libraries and frameworks that we all love. I used that chance to finally learn a little about declarative visualization with <a href=\"https://altair-viz.github.io/\">Altair</a>. Altair is not only highly customizable but also very well documented and allows for easy to implement interactivity, which is a nice-to-have on an application evolving around data exploration. You can check it out <a href=\"https://github.com/iwpnd/weightwhat\">here</a>.</p><p align=\"center\"><img src=\"/img/2020-02-streamlit/weightwhat.png\" alt=\"weightwhat\" /></p>",
            "url": "https://iwpnd.pw//articles/2020-02/streamlit-application-development-inside-docker",
            
            
            
            "tags": ["python","streamlit","docker"],
            
            "date_published": "2020-02-04T08:37:00+00:00",
            "date_modified": "2020-02-04T08:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-01/grammarly-in-vscode",
            "title": "Spellchecking with Grammarly in VSCode",
            "summary": null,
            "content_text": "My last article on FastAPI was the most-viewed article by now, but at the same time the article I had to update the most due to typos and grammatical errors. Sure you can write you texts in something like Pages, or Word, and copy them over to markdown afterward, or save them as markdown (apparently this is a thing now). I for one like to write directly in my editor of choice which is VSCode, even though it does not come with spell-checking batteries included. So you could resort back to Grammarly to do the spell-checking for you, but you would still have to copy and paste the text over to from Grammarly and back into your editor. Naah!Luckily for people like me, there are people out there that have our back on this. Folks like @znck who felt the same, reverse-engineered the Grammarly client and build a VSCode extension to fulfill my every spell-checking need.All you as a user have to do is to register an account with Grammarly, install the extension from the marketplace and add    {        \"grammarly.username\": \"you@typo.com\",        \"grammarly.password\": \"typ0\"    }to your settings.json in VSCode or manually in the VSCode settings via Settings &gt; Extensions &gt; Grammarly.There you go, full Grammarly support in your code editor.Mistakes are presented like your code linting errors, which is pretty neat if you’re debugging through the list anyways. You can then either choose to correct them from there like so:Another possibility is to hover over the text in your editor directly:Say you know that you never make mistakes in your code,.. because, you know, who does, right? Then you can simply exclude certain parts of your markdown from spellchecking.    {        \"grammarly.username\": \"you@typo.com\",        \"grammarly.password\": \"typ0\",        \"grammarly.diagnostics\":             {            \"[markdown]\": {                \"ignore\": [\"inlineCode\", \"code\"]            }        }Now if that piqued your interest, go ahead and try it out yourselves. And if you’re interested in how the extension was brought to life, there is an extensive blog post of the author about that. Let’s hope that this text is free from typos, at least.",
            "content_html": "<p>My last <a href=\"https://iwpnd.pw/articles/2020-01/deploy-FastAPI-to-aws-lambda\">article on FastAPI</a> was the most-viewed article by now, but at the same time the article I had to update the most due to typos and grammatical errors. Sure you can write you texts in something like Pages, or Word, and copy them over to markdown afterward, or save them as markdown (apparently <a href=\"http://www.writage.com/\">this</a> is a thing now). I for one like to write directly in my editor of choice which is VSCode, even though it does not come with spell-checking batteries included. So you could resort back to Grammarly to do the spell-checking for you, but you would still have to copy and paste the text over to from Grammarly and back into your editor. Naah!</p><p>Luckily for people like me, there are people out there that have our back on this. Folks like <a href=\"https://znck.dev/\">@znck</a> who felt the same, reverse-engineered the Grammarly client and build a <a href=\"https://marketplace.visualstudio.com/items?itemName=znck.grammarly\">VSCode extension</a> to fulfill my every spell-checking need.</p><p>All you as a user have to do is to register an account with <a href=\"https://app.grammarly.com/\">Grammarly</a>, install the extension from the marketplace and add</p><div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"w\">    </span><span class=\"p\">{</span><span class=\"w\">        </span><span class=\"nl\">\"grammarly.username\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"you@typo.com\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"grammarly.password\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"typ0\"</span><span class=\"w\">    </span><span class=\"p\">}</span><span class=\"w\"></span></code></pre></div></div><p>to your <code class=\"language-plaintext highlighter-rouge\">settings.json</code> in VSCode or manually in the VSCode settings via <code class=\"language-plaintext highlighter-rouge\">Settings &gt; Extensions &gt; Grammarly</code>.</p><p>There you go, full Grammarly support in your code editor.</p><p align=\"center\"><img src=\"/img/2020-01-28-grammarly-vscode-1.png\" alt=\"grammarly in vscode\" /></p><p>Mistakes are presented like your code linting errors, which is pretty neat if you’re debugging through the list anyways. You can then either choose to correct them from there like so:</p><p align=\"center\"><img src=\"/img/2020-01-28-grammarly-vscode-2.png\" alt=\"grammarly in vscode\" /></p><p>Another possibility is to hover over the text in your editor directly:</p><p align=\"center\"><img src=\"/img/2020-01-28-grammarly-vscode-3.png\" alt=\"grammarly in vscode\" /></p><p>Say you know that you never make mistakes in your code,.. because, you know, who does, right? Then you can simply exclude certain parts of your markdown from spellchecking.</p><div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"w\">    </span><span class=\"p\">{</span><span class=\"w\">        </span><span class=\"nl\">\"grammarly.username\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"you@typo.com\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"grammarly.password\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"typ0\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"grammarly.diagnostics\"</span><span class=\"p\">:</span><span class=\"w\">             </span><span class=\"p\">{</span><span class=\"w\">            </span><span class=\"nl\">\"[markdown]\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">                </span><span class=\"nl\">\"ignore\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"s2\">\"inlineCode\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">\"code\"</span><span class=\"p\">]</span><span class=\"w\">            </span><span class=\"p\">}</span><span class=\"w\">        </span><span class=\"p\">}</span><span class=\"w\"></span></code></pre></div></div><p>Now if that piqued your interest, go ahead and try it out yourselves. And if you’re interested in how the extension was brought to life, there is an extensive <a href=\"https://znck.dev/blog/2019-grammarly-in-code\">blog post</a> of the author about that. Let’s hope that this text is free from typos, at least.</p>",
            "url": "https://iwpnd.pw//articles/2020-01/grammarly-in-vscode",
            
            
            
            "tags": ["python","vscode","markdown"],
            
            "date_published": "2020-01-28T11:37:00+00:00",
            "date_modified": "2020-01-28T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-01/deploy-fastapi-to-aws-lambda",
            "title": "How to continuously deploy a FastAPI to AWS Lambda with AWS SAM",
            "summary": null,
            "content_text": "My last article about FastAPI was supposed to be an article about how to deploy a FastAPI on a budget, but instead turned out to be an opinion on FastAPI and I left it at that. Let’s change that.FastAPIs documentation is exhaustive on all accounts. It gets you started real quick, takes you by the hand if it gets more complicated and even describes features in detail when it doesn’t have to. I like it. Where it falls short, however, is when it comes to deployment. That’s probably because there are gazillions of ways to deploy an API. The documentation proposes to use Docker and while I understand that this is the way to go for most companies and most applications, I don’t see why I would want to deploy a small application with a few undemanding endpoints to a Docker Swarm or even Kubernetes cluster. Those come with a (hefty) price tag and are not interesting for the private person who just wants to get his hand dirty on FastAPI a little.So let’s use this article to start over and learn how to set up a basic continuous deployment pipeline for a FastAPI app on a budget. We will be using AWS API Gateway, AWS Lambda the serverless computing services by AWS and Travis. Both AWS Lambda and AWS API Gateway are billed per API call and by the amount of data that you transfer. If you’re still eligible for the free tier of AWS you can use those two services completely free for the scope of this tutorial. If not refer to the pricing example of AWS API Gateway and AWS Lambda. Take a particularly detailed look at the AWS Lambda pricing as this depends on the time your application is running and the memory size that is provisioned to the AWS Lambda. Luckily AWS finally shed some transparency on that matter with a proper calculator.prerequisitesLet’s assume that you already have registered an AWS account, set up a user other than root and have installed and configured AWS CLI properly. The latter is not a requirement, as you do everything we do in the AWS console, yet it’s not a bad idea to learn the AWS CLI anyways.Setup a new role for AWS Lambda to assumeFirst, we set up a role that the AWS Lambda will assume within our AWS account. Roles are AWSs way to enforce the principle of least privilege when it comes to the resources an AWS Lambda can execute or have access to. Go to the IAM console, select Roles and Create. That’ll take you into the Role creation screen where you will choose an AWS service as the trusted entity and select Lambda as the service that will use the role we’re going to create. Press next and it’ll take you to the Permissions tab. Here you can either create a permission policy from scratch or select one of the existing ones. Keep in mind that the permissions vary depending on the purpose of the Lambda function. We’ll take an existing permission for now. Search for lambda and select the AWSLambdaBasicExecutionRole which only allows our Lambda to write logs to AWS Cloudwatch. Press next, give it a tag or don’t and press Review. Now you’re prompted to give it a name. Choose a meaningful name (e.g. fastapilambdarole) and continue to create the role, which takes you back to Roles where you click your newly created role and mark down the Role ARN (Amazon Resource Name).Setup an AWS S3 bucketFor small applications that only use vanilla python without external libraries, one could quickly copy and paste the code into the AWS Lambda console. Bigger applications that use third-party libraries, however, will either be uploaded as a zip file or in our case, we will provide the location of our deployment package as an AWS S3 bucket. If you have AWS CLI properly setup you can create a bucket with:aws s3api create-bucket \\--bucket my-travis-deployment-bucket \\--region eu-west-1 \\--create-bucket-configuration LocationConstraint=eu-west-1Otherwise, you navigate to the S3 console and create one there.Setup a Travis User in AWSNext up we create a new AWS user that Travis can use to perform actions on AWS resources on our behalf. We could just use our encrypted credentials and secret, but a) spider-senses intensify b) we follow the principle of least privilege again. If Travis does not need the rights to summon the mighty p3dn.24xlarge, Travis will stick to creating AWS Lambdas. First, we will create a new policy. This time we have to be more specific because we need Travis to have access to AWS S3 for the deployment package, AWS Cloudformation to build our API stack, API Gateway, and AWS Lambda. You can do it in the AWS console or with AWS CLI (see here). Use this policy for the example:      { \"Version\": \"2012-10-17\", \"Statement\": [    {    \"Sid\": \"AllowListBucket\",    \"Effect\": \"Allow\",    \"Action\": [    \"s3:ListBucket\"    ], [...]        { \"Version\": \"2012-10-17\",\"Statement\": [    {        \"Sid\": \"AllowListBucket\",        \"Effect\": \"Allow\",        \"Action\": [            \"s3:ListBucket\"            ],        \"Resource\": [            \"arn:aws:s3:::my-travis-deployment-bucket\"        ]    },    {        \"Sid\": \"AllowPassLambdaRole\",        \"Effect\": \"Allow\",        \"Action\": [            \"iam:PassRole\"            ],        \"Resource\": [            \"arn:aws:iam::&lt;your-account-id-here&gt;:role/fastapilambdarole\"            ]        },    {        \"Sid\": \"AllowS3Actions\",        \"Effect\": \"Allow\",        \"Action\": [            \"s3:PutObject\",            \"s3:GetObjectAcl\",            \"s3:GetObject\",            \"s3:DeleteObject\",            \"s3:PutObjectAcl\"            ],        \"Resource\": \"arn:aws:s3:::my-travis-deployment-bucket/*\"        },    {        \"Sid\": \"AllowLambda\",        \"Effect\": \"Allow\",        \"Action\": [            \"lambda:*\"            ],        \"Resource\": \"*\"        },    {        \"Sid\": \"AllowListPolicies\",        \"Effect\": \"Allow\",        \"Action\": [            \"iam:ListPolicies\"            ],        \"Resource\": \"*\"        },    {        \"Sid\": \"AllowApiGateway\",        \"Effect\": \"Allow\",        \"Action\": [            \"apigateway:*\"            ],        \"Resource\": \"*\"        },    {        \"Sid\": \"AllowCloudFormation\",        \"Effect\": \"Allow\",        \"Action\": [            \"cloudformation:*\"        ],        \"Resource\": \"*\"    }    ]}   Continuing, you go to the IAM console, but this time you will create a user with programmatic access and a meaningful name such as travisdeploymentuser or the likes. Now we attach the policy we just created to the Travis user. We get prompted with the AWS-ACCESS-KEY-ID and the AWS-SECRET-ACCESS-KEY of the user. Note those down for now. Done.The example applicationFor the sake of this tutorial, I created a Github repository with an example application that you can use as a first step and to/or built on top.Application structureInspired by the FastAPI-realworld-example-app, I neatly separated the pydantic models, the configuration, the endpoints, and the routers..├── Dockerfile├── LICENSE├── README.md├── example_app│ ├── __init__.py│ ├── api│ │ ├── __init__.py│ │ └── api_v1│ │ ├── __init__.py│ │ ├── api.py│ │ └── endpoints│ │ ├── __init__.py│ │ └── example.py│ ├── core│ │ ├── __init__.py│ │ ├── config.py│ │ └── models│ │ ├── input.py│ │ └── output.py│ └── main.py├── requirements.txt├── scripts│ └── example.ipynb├── setup.py├── template.yml├── .travis.yml├── .pre-commit-config.yaml├── tests│ ├── __init__.py│ ├── test_example_endpoint.py│ └── test_ping.pyFor simplicities sake, we have exactly two endpoints. One is /ping in main.py and the other is /api/v1/example that takes two integer values and returns their product. If you want you can expand this functionality with your pedantic models and additional routes. I also already included a .pre-commit-configuration.yaml for you to start using pre-commit right away. It comes with pre-configured hook for black.Test the application locallyTo test the example application locally we have a couple of options. One is by cloning the repository and starting it locally with uvicorn. The other is to build a docker image from the Dockerfile in the repository and expose the app from within a container.git clone https://github.com/iwpnd/FastAPI-aws-lambda-examplecd FastAPI-aws-lambda-example# create and activate a virtual environmentpip install -e .pip install uvicorn # or anything else that can handle ASGIpytest . -vuvicorn example_app.main:app --host 0.0.0.0 --port 8080 --reloadOrdocker build -t example_app_image .docker run -p 8080:8080 -name example-app-container example_app_imageNo matter what you choose you can now go to your browser and check the documentation of the application via http://localhost:8080/docs and test the API through the Swagger UI right there.Wrap the application with MangumFor this application to run with AWS Lambda &amp; AWS API Gateway, we have to wrap it with Mangum. Mangum works as an adapter for ASGI applications like the ones you can create with FastAPI so that they can send and receive information from API Gateway to Lambda and vice versa.from FastAPI import FastAPIfrom example_app.api.api_v1.api import router as api_routerfrom example_app.core.config import API_V1_STR, PROJECT_NAMEfrom mangum import Mangumapp = FastAPI(title=PROJECT_NAME)app.include_router(api_router, prefix=API_V1_STR)@app.get(\"/ping\")def pong():    \"\"\"    Sanity check.    This will let the user know that the service is operational.    And this path operation will:    * show a life sign    \"\"\"    return {\"ping\": \"pong!\"}handler = Mangum(app, enable_lifespan=False)The AWS Lambda handlerThe handler is necessary for AWS Lambda for it is the function that AWS Lambda can invoke when the service executes your code. It follows a simple syntax:from your_module import Yourclassfrom your_database import databasedb = database.connect()def handler(event, context):    msg = Yourclass(        text=event[\"message\"],        connection=db.connection        )    msg.build()    return msg.transformedYou can import your libraries, like your_module or your_database and you can create variables or database connections. Everything outside of the handler function will execute when the AWS Lambda is provisioned. After that, you can use it within the handler that AWS Lambda will use on consecutive calls. The event is what AWS Lambda uses to pass in event data to the handler. The context on the other hand provides information about the invocation, function, and execution environment (see docs for more details).Mangum as the handler for event and contextA FastAPI application does not have a handler, so that’s what Mangum is for. It wraps the app, therefore it will receive event and context in an AWS Lambda execution environment and will pass those on to the app itself. For this to work we have to setup AWS API Gateway proxy integration to pass the raw request to the AWS Lambda, and let the app decide on how to process the information and what to return, including 404s, etc. This is what allows this setup in the first place.Deploy with AWS SAMTo deploy the AWS Lambda function we have now built, we will use the AWS Serverless Application Model (AWS SAM, an open-source framework to build serverless applications. As an extension to AWS Cloudformation it integrates nicely with all the other AWS services we need and lets us build our infrastructure from code - the template.yml in the repository.AWSTemplateFormatVersion: '2010-09-09'Transform: AWS::Serverless-2016-10-31Description: &gt;    FastAPI aws lambda exampleResources:    FastapiExampleLambda:        Type: AWS::Serverless::Function        Properties:            Events:                ApiEvent:                    Properties:                        RestApiId:                            Ref: FastapiExampleGateway                        Path: /{proxy+}                        Method: ANY                    Type: Api            FunctionName: FastAPI-lambda-example            CodeUri: ./            Handler: example_app.main.handler            Runtime: python3.7            Timeout: 300 # timeout of your lambda function            MemorySize: 128 # memory size of your lambda function            Description: FastAPI aws lambda example            # other options, see -&gt;            # https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy-globals.html#sam-specification-template-anatomy-globals-supported-resources-and-properties            Role: !Sub arn:aws:iam::${AWS::AccountId}:role/fastapilambdarole    FastapiExampleGateway:        Type: AWS::Serverless::Api        Properties:            StageName: prod            OpenApiVersion: '3.0.0'There are some things we have to unpack here. What we do is, we tell AWS Cloudformation to provision resources on our behalf and to deploy them in a stack. In the Resources section you see the API Gateway first, then the Lambda function we want to build from the code in the CodeUri with the handler in Handler. We define the Runtime of the Lambda, as well as MemorySize and Timeout. You need to attach the proper role in the Role section, that we have created earlier. In the Events section we tell AWS Cloudformation to use FastapiExampleGateway as the API Gateway with {proxy+} integration, because as you recall that’s what makes this setup work in the first place.Check out the official Template Anatomy to get a better understanding of other options available.Once set up, you can now deploy the FastAPI-aws-lambda-example application from your local machine, through the template that tells AWS Cloudformation to build a stack and provision the resources necessary.1. Stage: Validate the SAM templateThe first thing we do is to validate the SAM template to check if the YAML we provide is valid.sam validate2020-01-21 10:28:39 Found credentials in environment variables./path/to/FastAPI-aws-lambda-example/template.yml is a valid SAM Template2. Stage: Build the deployment packageNext up, we build the deployment package. If your application depends on packages that have natively compiled programs you pass --use-container and SAM will attempt to build the application in a Docker container using based on LambCI. Optionally you can see what’s happening in the container if you also pass the --debug flag.sam build --use-container --debugThis will build the deployment package and store it in .aws-sam/build along with a new template.yaml that now also contains the values we !Sub ‘ed or substituted like so ${AWS::AccountId}, in the initial template.Starting Build inside a containerBuilding resource 'FastapiExampleLambda'Fetching lambci/lambda:build-python3.7 Docker container image......Mounting /path/to/FastAPI-aws-lambda-example as /tmp/samcli/source:ro,delegated inside runtime containerBuild SucceededBuilt Artifacts : .aws-sam/buildBuilt Template : .aws-sam/build/template.yamlCommands you can use next=========================[*] Invoke Function: sam local invoke[*] Package: sam package --s3-bucket &lt;yourbucket&gt;Running PythonPipBuilder:ResolveDependenciesRunning PythonPipBuilder:CopySource3. Stage: Package the applicationUp next, packaging. As stated in the beginning, when an application in an AWS Lambda exceeds a certain size or has dependencies, we have to package the application and either upload it in the AWS console or prepare an intermediate AWS S3 bucket and let AWS Lambda get the application package from there. AWS SAM requires you to do the latter, or better, does it for you if you provision a bucket and pass it with --s3-bucket my-travis-deployment-bucket.sam package --s3-bucket my-travis-deployment-bucket --output-template-file out.yml --region eu-west-1Which returns:2020-01-21 10:48:12 Found credentials in environment variables.Uploading to 2adfa5ddb62b541b7cf323cda43ee394 8523862 / 8523862.0 (100.00%) Successfully packaged artifacts and wrote output template to file out.yml.Execute the following command to deploy the packaged templatesam deploy --template-file /path/to/FastAPI-aws-lambda-example/out.yml --stack-name &lt;YOUR STACK NAME&gt;Now the application is packaged and a final template has been created that will now be used to tell AWS Cloudformation where the package is.4. Stage: Deploy the applicationsam deploy --template-file out.yml --stack-name example-stack-name --region eu-west-1 --no-fail-on-empty-changeset --capabilities CAPABILITY_IAMWaiting for stack create/update to completeSuccessfully created/updated stack - example-stack-nameThis last step will finally deploy the application --stack-name to a --region. Important to note here is the option --no-fail-on-empty-changeset. Deploying a new version of your application code does not change the stack itself. So running the command without this option will fail. With this, you can, however, push consecutive updates of your codes to the same stack.If you now go to the API Gateway Console you will see your API deployed to the prod stage at https://xxxxxxxxxx.execute-api.eu-west-1.amazonaws.com/prod.Continuous deployment with TravisNow for the last part, the continuous deployment through Github and Travis. The idea is that any code commit that passes an automated testing phase is automatically released into the production environment, and is accessible by the user. This means that the stages we laid out above, will no longer be executed manually by you, but instead in a Travis CI pipeline. Aight, let’se go.  Go to https://travis-ci.com/ and sign up with your GitHub account.  Accept the Authorization of Travis CI and you’ll be redirected to GitHub.  Click on your profile picture in the top right of your Travis Dashboard, click the green Activate button, and select the repositories you want to use with Travis CI.  Select the repository of your application  Encrypt your AWS-ACCESS-KEY-ID and your AWS-SECRET-ACCESS-KEY of the travisdeploymentuser we created at the beginning like this, and put those in the .travis.yml file  Commit the .travis.yml file to your repository and check your build process in the Travis dashboard.language: pythoncache: pippython:- '3.7'install:- pip install awscli- pip install aws-sam-clijobs:  include:    - stage: test      script:        - pip install pytest        - pip install -e .        - pytest . -v    - stage: deploy      script:        - sam validate        - sam build --debug        - sam package --s3-bucket my-travis-deployment-bucket --output-template-file out.yml --region eu-west-1        - sam deploy --template-file out.yml --stack-name example-stack-name --region eu-west-1 --no-fail-on-empty-changeset --capabilities CAPABILITY_IAM      skip_cleanup: true      if: branch = masternotifications:  email:    on_failure: alwaysenv:  global:  - AWS_DEFAULT_REGION=eu-west-1  - secure: your-encrypted-aws-access-key-id  - secure: your-encrypted-aws-secret-access-keyFrom now on every time you commit changes to your repository master branch, it will be immediately be deployed to AWS Lambda.conclusion  We learned how to create a new user and policies in AWS IAM  We now have a basic application we can build upon  We learned about AWS API Gateway and AWS Lambda  We learned how Mangum works  We learned about AWS SAM and how to deploy an application from your machine  We learned how to use Travis for continuous deployment of the said application instead of doing it manuallyIf you have any questions feel free to reach out to me in the example repository or via mail.",
            "content_html": "<p>My last <a href=\"https://iwpnd.pw/articles/2020-01/opinion-on-FastAPI\">article about FastAPI</a> was supposed to be an article about how to deploy a FastAPI on a budget, but instead turned out to be an opinion on FastAPI and I left it at that. Let’s change that.</p><p>FastAPIs <a href=\"https://FastAPI.tiangolo.com/\">documentation</a> is exhaustive on all accounts. It gets you started real quick, takes you by the hand if it gets more complicated and even describes features in detail when it doesn’t have to. I like it. Where it falls short, however, is when it comes to <a href=\"https://FastAPI.tiangolo.com/deployment/\">deployment</a>. That’s probably because there are gazillions of ways to deploy an API. The documentation proposes to use <a href=\"https://docker.com/\">Docker</a> and while I understand that this is the way to go for most companies and most applications, I don’t see why I would want to deploy a small application with a few undemanding endpoints to a Docker Swarm or even Kubernetes cluster. Those come with a (hefty) price tag and are not interesting for the private person who just wants to get his hand dirty on FastAPI a little.</p><p>So let’s use this article to start over and learn how to set up a basic continuous deployment pipeline for a FastAPI app on a budget. We will be using <a href=\"https://aws.amazon.com/api-gateway/\">AWS API Gateway</a>, <a href=\"https://aws.amazon.com/lambda/\">AWS Lambda</a> the serverless computing services by AWS and <a href=\"https://travis-ci.com/\">Travis</a>. Both AWS Lambda and AWS API Gateway are billed per API call and by the amount of data that you transfer. If you’re still eligible for the free tier of AWS you can use those two services completely free for the scope of this tutorial. If not refer to the pricing example of <a href=\"https://aws.amazon.com/api-gateway/pricing/#Pricing_Examples\">AWS API Gateway</a> and <a href=\"https://aws.amazon.com/lambda/pricing/\">AWS Lambda</a>. Take a particularly detailed look at the AWS Lambda pricing as this depends on the time your application is running and the memory size that is provisioned to the AWS Lambda. Luckily AWS finally shed some transparency on that matter with a proper <a href=\"https://aws.amazon.com/lambda/pricing/#Calculator\">calculator</a>.</p><h2 id=\"prerequisites\">prerequisites</h2><p>Let’s assume that you already have registered an <a href=\"https://aws.amazon.com/free\">AWS account</a>, set up a user other than <code class=\"language-plaintext highlighter-rouge\">root</code> and have installed and configured <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\">AWS CLI</a> properly. The latter is not a requirement, as you do everything we do in the AWS console, yet it’s not a bad idea to learn the AWS CLI anyways.</p><h3 id=\"setup-a-new-role-for-aws-lambda-to-assume\">Setup a new role for AWS Lambda to assume</h3><p>First, we set up a role that the AWS Lambda will assume within our AWS account. Roles are AWSs way to enforce the <a href=\"https://en.wikipedia.org/wiki/Principle_of_least_privilege\">principle of least privilege</a> when it comes to the resources an AWS Lambda can execute or have access to. Go to the <a href=\"https://console.aws.amazon.com/iam/\">IAM console</a>, select <em>Roles</em> and <em>Create</em>. That’ll take you into the <em>Role creation</em> screen where you will choose an <em>AWS service</em> as the trusted entity and select <em>Lambda</em> as the service that will use the role we’re going to create. Press next and it’ll take you to the <em>Permissions</em> tab. Here you can either create a permission policy from scratch or select one of the existing ones. Keep in mind that the permissions vary depending on the purpose of the Lambda function. We’ll take an existing permission for now. Search for <code class=\"language-plaintext highlighter-rouge\">lambda</code> and select the <code class=\"language-plaintext highlighter-rouge\">AWSLambdaBasicExecutionRole</code> which only allows our Lambda to write logs to AWS Cloudwatch. Press next, give it a tag or don’t and press <em>Review</em>. Now you’re prompted to give it a name. Choose a meaningful name (e.g. fastapilambdarole) and continue to create the role, which takes you back to <em>Roles</em> where you click your newly created role and mark down the Role ARN (<a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\">Amazon Resource Name</a>).</p><h3 id=\"setup-an-aws-s3-bucket\">Setup an AWS S3 bucket</h3><p>For small applications that only use vanilla python without external libraries, one could quickly copy and paste the code into the AWS Lambda console. Bigger applications that use third-party libraries, however, will either be uploaded as a zip file or in our case, we will provide the location of our deployment package as an AWS S3 bucket. If you have AWS CLI properly setup you can create a bucket with:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>aws s3api create-bucket \\--bucket my-travis-deployment-bucket \\--region eu-west-1 \\--create-bucket-configuration LocationConstraint=eu-west-1</code></pre></div></div><p>Otherwise, you navigate to the <a href=\"https://s3.console.aws.amazon.com/s3/\">S3 console</a> and create one there.</p><h3 id=\"setup-a-travis-user-in-aws\">Setup a Travis User in AWS</h3><p>Next up we create a new AWS user that Travis can use to perform actions on AWS resources on our behalf. We could just use our encrypted credentials and secret, but a) spider-senses intensify b) we follow the principle of least privilege again. If Travis does not need the rights to summon the mighty <a href=\"https://aws.amazon.com/ec2/instance-types/p3/\">p3dn.24xlarge</a>, Travis will stick to creating AWS Lambdas. First, we will create a new policy. This time we have to be more specific because we need Travis to have access to AWS S3 for the deployment package, AWS Cloudformation to build our API stack, API Gateway, and AWS Lambda. You can do it in the AWS console or with AWS CLI (<a href=\"https://docs.aws.amazon.com/cli/latest/reference/iam/create-policy.html\">see here</a>). Use this policy for the example:</p><details>  <summary>    <div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nl\">\"Version\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"2012-10-17\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nl\">\"Statement\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">    </span><span class=\"p\">{</span><span class=\"w\">    </span><span class=\"nl\">\"Sid\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"AllowListBucket\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"Effect\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Allow\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"Action\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">    </span><span class=\"s2\">\"s3:ListBucket\"</span><span class=\"w\">    </span><span class=\"p\">],</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"err\">...</span><span class=\"p\">]</span><span class=\"w\"></span></code></pre></div>    </div>  </summary>  <div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nl\">\"Version\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"2012-10-17\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"Statement\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">    </span><span class=\"p\">{</span><span class=\"w\">        </span><span class=\"nl\">\"Sid\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"AllowListBucket\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Effect\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Allow\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Action\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">            </span><span class=\"s2\">\"s3:ListBucket\"</span><span class=\"w\">            </span><span class=\"p\">],</span><span class=\"w\">        </span><span class=\"nl\">\"Resource\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">            </span><span class=\"s2\">\"arn:aws:s3:::my-travis-deployment-bucket\"</span><span class=\"w\">        </span><span class=\"p\">]</span><span class=\"w\">    </span><span class=\"p\">},</span><span class=\"w\">    </span><span class=\"p\">{</span><span class=\"w\">        </span><span class=\"nl\">\"Sid\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"AllowPassLambdaRole\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Effect\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Allow\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Action\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">            </span><span class=\"s2\">\"iam:PassRole\"</span><span class=\"w\">            </span><span class=\"p\">],</span><span class=\"w\">        </span><span class=\"nl\">\"Resource\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">            </span><span class=\"s2\">\"arn:aws:iam::&lt;your-account-id-here&gt;:role/fastapilambdarole\"</span><span class=\"w\">            </span><span class=\"p\">]</span><span class=\"w\">        </span><span class=\"p\">},</span><span class=\"w\">    </span><span class=\"p\">{</span><span class=\"w\">        </span><span class=\"nl\">\"Sid\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"AllowS3Actions\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Effect\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Allow\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Action\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">            </span><span class=\"s2\">\"s3:PutObject\"</span><span class=\"p\">,</span><span class=\"w\">            </span><span class=\"s2\">\"s3:GetObjectAcl\"</span><span class=\"p\">,</span><span class=\"w\">            </span><span class=\"s2\">\"s3:GetObject\"</span><span class=\"p\">,</span><span class=\"w\">            </span><span class=\"s2\">\"s3:DeleteObject\"</span><span class=\"p\">,</span><span class=\"w\">            </span><span class=\"s2\">\"s3:PutObjectAcl\"</span><span class=\"w\">            </span><span class=\"p\">],</span><span class=\"w\">        </span><span class=\"nl\">\"Resource\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"arn:aws:s3:::my-travis-deployment-bucket/*\"</span><span class=\"w\">        </span><span class=\"p\">},</span><span class=\"w\">    </span><span class=\"p\">{</span><span class=\"w\">        </span><span class=\"nl\">\"Sid\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"AllowLambda\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Effect\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Allow\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Action\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">            </span><span class=\"s2\">\"lambda:*\"</span><span class=\"w\">            </span><span class=\"p\">],</span><span class=\"w\">        </span><span class=\"nl\">\"Resource\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"*\"</span><span class=\"w\">        </span><span class=\"p\">},</span><span class=\"w\">    </span><span class=\"p\">{</span><span class=\"w\">        </span><span class=\"nl\">\"Sid\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"AllowListPolicies\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Effect\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Allow\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Action\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">            </span><span class=\"s2\">\"iam:ListPolicies\"</span><span class=\"w\">            </span><span class=\"p\">],</span><span class=\"w\">        </span><span class=\"nl\">\"Resource\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"*\"</span><span class=\"w\">        </span><span class=\"p\">},</span><span class=\"w\">    </span><span class=\"p\">{</span><span class=\"w\">        </span><span class=\"nl\">\"Sid\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"AllowApiGateway\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Effect\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Allow\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Action\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">            </span><span class=\"s2\">\"apigateway:*\"</span><span class=\"w\">            </span><span class=\"p\">],</span><span class=\"w\">        </span><span class=\"nl\">\"Resource\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"*\"</span><span class=\"w\">        </span><span class=\"p\">},</span><span class=\"w\">    </span><span class=\"p\">{</span><span class=\"w\">        </span><span class=\"nl\">\"Sid\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"AllowCloudFormation\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Effect\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Allow\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"Action\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">            </span><span class=\"s2\">\"cloudformation:*\"</span><span class=\"w\">        </span><span class=\"p\">],</span><span class=\"w\">        </span><span class=\"nl\">\"Resource\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"*\"</span><span class=\"w\">    </span><span class=\"p\">}</span><span class=\"w\">    </span><span class=\"p\">]</span><span class=\"w\"></span><span class=\"p\">}</span><span class=\"w\"> </span></code></pre></div>  </div></details><p>Continuing, you go to the <a href=\"https://console.aws.amazon.com/iam/\">IAM console</a>, but this time you will create a user with <em>programmatic access</em> and a meaningful name such as <code class=\"language-plaintext highlighter-rouge\">travisdeploymentuser</code> or the likes. Now we attach the policy we just created to the Travis user. We get prompted with the <code class=\"language-plaintext highlighter-rouge\">AWS-ACCESS-KEY-ID</code> and the <code class=\"language-plaintext highlighter-rouge\">AWS-SECRET-ACCESS-KEY</code> of the user. Note those down for now. Done.</p><h2 id=\"the-example-application\">The example application</h2><p>For the sake of this tutorial, I created a <a href=\"https://github.com/iwpnd/FastAPI-aws-lambda-example\">Github repository</a> with an example application that you can use as a first step and to/or built on top.</p><h3 id=\"application-structure\">Application structure</h3><p>Inspired by the <a href=\"https://github.com/nsidnev/FastAPI-realworld-example-app\">FastAPI-realworld-example-app</a>, I neatly separated the pydantic models, the configuration, the endpoints, and the routers.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>.├── Dockerfile├── LICENSE├── README.md├── example_app│ ├── __init__.py│ ├── api│ │ ├── __init__.py│ │ └── api_v1│ │ ├── __init__.py│ │ ├── api.py│ │ └── endpoints│ │ ├── __init__.py│ │ └── example.py│ ├── core│ │ ├── __init__.py│ │ ├── config.py│ │ └── models│ │ ├── input.py│ │ └── output.py│ └── main.py├── requirements.txt├── scripts│ └── example.ipynb├── setup.py├── template.yml├── .travis.yml├── .pre-commit-config.yaml├── tests│ ├── __init__.py│ ├── test_example_endpoint.py│ └── test_ping.py</code></pre></div></div><p>For simplicities sake, we have exactly two endpoints. One is <code class=\"language-plaintext highlighter-rouge\">/ping</code> in <code class=\"language-plaintext highlighter-rouge\">main.py</code> and the other is <code class=\"language-plaintext highlighter-rouge\">/api/v1/example</code> that takes two integer values and returns their product. If you want you can expand this functionality with your pedantic models and additional routes. I also already included a <code class=\"language-plaintext highlighter-rouge\">.pre-commit-configuration.yaml</code> for you to start using <a href=\"https://iwpnd.pw/articles/2020-01/pre-commit-to-the-rescue\">pre-commit</a> right away. It comes with pre-configured hook for <a href=\"https://iwpnd.pw/articles/2020-01/black-python-code-formatter\">black</a>.</p><h3 id=\"test-the-application-locally\">Test the application locally</h3><p>To test the example application locally we have a couple of options. One is by cloning the <a href=\"https://github.com/iwpnd/FastAPI-aws-lambda-example\">repository</a> and starting it locally with uvicorn. The other is to build a docker image from the <code class=\"language-plaintext highlighter-rouge\">Dockerfile</code> in the repository and expose the app from within a container.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>git clone https://github.com/iwpnd/FastAPI-aws-lambda-examplecd FastAPI-aws-lambda-example# create and activate a virtual environmentpip install -e .pip install uvicorn # or anything else that can handle ASGIpytest . -vuvicorn example_app.main:app --host 0.0.0.0 --port 8080 --reload</code></pre></div></div><p>Or</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>docker build -t example_app_image .docker run -p 8080:8080 -name example-app-container example_app_image</code></pre></div></div><p>No matter what you choose you can now go to your browser and check the documentation of the application via <a href=\"http://localhost:8000/docs\">http://localhost:8080/docs</a> and test the API through the Swagger UI right there.</p><h2 id=\"wrap-the-application-with-mangum\">Wrap the application with Mangum</h2><p>For this application to run with AWS Lambda &amp; AWS API Gateway, we have to wrap it with <a href=\"https://github.com/erm/mangum\">Mangum</a>. Mangum works as an adapter for <a href=\"https://asgi.readthedocs.io/en/latest/introduction.html\">ASGI applications</a> like the ones you can create with FastAPI so that they can send and receive information from API Gateway to Lambda and vice versa.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">FastAPI</span> <span class=\"kn\">import</span> <span class=\"n\">FastAPI</span><span class=\"kn\">from</span> <span class=\"nn\">example_app.api.api_v1.api</span> <span class=\"kn\">import</span> <span class=\"n\">router</span> <span class=\"k\">as</span> <span class=\"n\">api_router</span><span class=\"kn\">from</span> <span class=\"nn\">example_app.core.config</span> <span class=\"kn\">import</span> <span class=\"n\">API_V1_STR</span><span class=\"p\">,</span> <span class=\"n\">PROJECT_NAME</span><span class=\"kn\">from</span> <span class=\"nn\">mangum</span> <span class=\"kn\">import</span> <span class=\"n\">Mangum</span><span class=\"n\">app</span> <span class=\"o\">=</span> <span class=\"n\">FastAPI</span><span class=\"p\">(</span><span class=\"n\">title</span><span class=\"o\">=</span><span class=\"n\">PROJECT_NAME</span><span class=\"p\">)</span><span class=\"n\">app</span><span class=\"p\">.</span><span class=\"n\">include_router</span><span class=\"p\">(</span><span class=\"n\">api_router</span><span class=\"p\">,</span> <span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"n\">API_V1_STR</span><span class=\"p\">)</span><span class=\"o\">@</span><span class=\"n\">app</span><span class=\"p\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s\">\"/ping\"</span><span class=\"p\">)</span><span class=\"k\">def</span> <span class=\"nf\">pong</span><span class=\"p\">():</span>    <span class=\"s\">\"\"\"    Sanity check.    This will let the user know that the service is operational.    And this path operation will:    * show a life sign    \"\"\"</span>    <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s\">\"ping\"</span><span class=\"p\">:</span> <span class=\"s\">\"pong!\"</span><span class=\"p\">}</span><span class=\"n\">handler</span> <span class=\"o\">=</span> <span class=\"n\">Mangum</span><span class=\"p\">(</span><span class=\"n\">app</span><span class=\"p\">,</span> <span class=\"n\">enable_lifespan</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span></code></pre></div></div><h3 id=\"the-aws-lambda-handler\">The AWS Lambda handler</h3><p>The handler is necessary for AWS Lambda for it is the function that AWS Lambda can invoke when the service executes your code. It follows a simple syntax:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">your_module</span> <span class=\"kn\">import</span> <span class=\"n\">Yourclass</span><span class=\"kn\">from</span> <span class=\"nn\">your_database</span> <span class=\"kn\">import</span> <span class=\"n\">database</span><span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">database</span><span class=\"p\">.</span><span class=\"n\">connect</span><span class=\"p\">()</span><span class=\"k\">def</span> <span class=\"nf\">handler</span><span class=\"p\">(</span><span class=\"n\">event</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">):</span>    <span class=\"n\">msg</span> <span class=\"o\">=</span> <span class=\"n\">Yourclass</span><span class=\"p\">(</span>        <span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">event</span><span class=\"p\">[</span><span class=\"s\">\"message\"</span><span class=\"p\">],</span>        <span class=\"n\">connection</span><span class=\"o\">=</span><span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">connection</span>        <span class=\"p\">)</span>    <span class=\"n\">msg</span><span class=\"p\">.</span><span class=\"n\">build</span><span class=\"p\">()</span>    <span class=\"k\">return</span> <span class=\"n\">msg</span><span class=\"p\">.</span><span class=\"n\">transformed</span></code></pre></div></div><p>You can import your libraries, like <code class=\"language-plaintext highlighter-rouge\">your_module</code> or <code class=\"language-plaintext highlighter-rouge\">your_database</code> and you can create variables or database connections. Everything outside of the <code class=\"language-plaintext highlighter-rouge\">handler</code> function will execute when the AWS Lambda is provisioned. After that, you can use it within the <code class=\"language-plaintext highlighter-rouge\">handler</code> that AWS Lambda will use on consecutive calls. The <code class=\"language-plaintext highlighter-rouge\">event</code> is what AWS Lambda uses to pass in event data to the <code class=\"language-plaintext highlighter-rouge\">handler</code>. The <code class=\"language-plaintext highlighter-rouge\">context</code> on the other hand provides information about the invocation, function, and execution environment (see <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/python-context-object.html\">docs</a> for more details).</p><h3 id=\"mangum-as-the-handler-for-event-and-context\">Mangum as the handler for event and context</h3><p>A FastAPI application does not have a handler, so that’s what <a href=\"https://github.com/erm/mangum\">Mangum</a> is for. It wraps the <code class=\"language-plaintext highlighter-rouge\">app</code>, therefore it will receive <code class=\"language-plaintext highlighter-rouge\">event</code> and <code class=\"language-plaintext highlighter-rouge\">context</code> in an AWS Lambda execution environment and will pass those on to the <code class=\"language-plaintext highlighter-rouge\">app</code> itself. For this to work we have to setup AWS API Gateway proxy integration to pass the raw request to the AWS Lambda, and let the <code class=\"language-plaintext highlighter-rouge\">app</code> decide on how to process the information and what to return, including 404s, etc. This is what allows this setup in the first place.</p><h2 id=\"deploy-with-aws-sam\">Deploy with AWS SAM</h2><p>To deploy the AWS Lambda function we have now built, we will use the AWS Serverless Application Model (<a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html\">AWS SAM</a>, an open-source framework to build serverless applications. As an extension to <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html\">AWS Cloudformation</a> it integrates nicely with all the other AWS services we need and lets us build our infrastructure from code - the <code class=\"language-plaintext highlighter-rouge\">template.yml</code> in the <a href=\"[repository](https://github.com/iwpnd/FastAPI-aws-lambda-example)\">repository</a>.</p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">AWSTemplateFormatVersion</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">2010-09-09'</span><span class=\"na\">Transform</span><span class=\"pi\">:</span> <span class=\"s\">AWS::Serverless-2016-10-31</span><span class=\"na\">Description</span><span class=\"pi\">:</span> <span class=\"pi\">&gt;</span>    <span class=\"s\">FastAPI aws lambda example</span><span class=\"na\">Resources</span><span class=\"pi\">:</span>    <span class=\"na\">FastapiExampleLambda</span><span class=\"pi\">:</span>        <span class=\"na\">Type</span><span class=\"pi\">:</span> <span class=\"s\">AWS::Serverless::Function</span>        <span class=\"na\">Properties</span><span class=\"pi\">:</span>            <span class=\"na\">Events</span><span class=\"pi\">:</span>                <span class=\"na\">ApiEvent</span><span class=\"pi\">:</span>                    <span class=\"na\">Properties</span><span class=\"pi\">:</span>                        <span class=\"na\">RestApiId</span><span class=\"pi\">:</span>                            <span class=\"na\">Ref</span><span class=\"pi\">:</span> <span class=\"s\">FastapiExampleGateway</span>                        <span class=\"na\">Path</span><span class=\"pi\">:</span> <span class=\"s\">/{proxy+}</span>                        <span class=\"na\">Method</span><span class=\"pi\">:</span> <span class=\"s\">ANY</span>                    <span class=\"na\">Type</span><span class=\"pi\">:</span> <span class=\"s\">Api</span>            <span class=\"na\">FunctionName</span><span class=\"pi\">:</span> <span class=\"s\">FastAPI-lambda-example</span>            <span class=\"na\">CodeUri</span><span class=\"pi\">:</span> <span class=\"s\">./</span>            <span class=\"na\">Handler</span><span class=\"pi\">:</span> <span class=\"s\">example_app.main.handler</span>            <span class=\"na\">Runtime</span><span class=\"pi\">:</span> <span class=\"s\">python3.7</span>            <span class=\"na\">Timeout</span><span class=\"pi\">:</span> <span class=\"m\">300</span> <span class=\"c1\"># timeout of your lambda function</span>            <span class=\"na\">MemorySize</span><span class=\"pi\">:</span> <span class=\"m\">128</span> <span class=\"c1\"># memory size of your lambda function</span>            <span class=\"na\">Description</span><span class=\"pi\">:</span> <span class=\"s\">FastAPI aws lambda example</span>            <span class=\"c1\"># other options, see -&gt;</span>            <span class=\"c1\"># https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy-globals.html#sam-specification-template-anatomy-globals-supported-resources-and-properties</span>            <span class=\"na\">Role</span><span class=\"pi\">:</span> <span class=\"kt\">!Sub</span> <span class=\"s\">arn:aws:iam::${AWS::AccountId}:role/fastapilambdarole</span>    <span class=\"na\">FastapiExampleGateway</span><span class=\"pi\">:</span>        <span class=\"na\">Type</span><span class=\"pi\">:</span> <span class=\"s\">AWS::Serverless::Api</span>        <span class=\"na\">Properties</span><span class=\"pi\">:</span>            <span class=\"na\">StageName</span><span class=\"pi\">:</span> <span class=\"s\">prod</span>            <span class=\"na\">OpenApiVersion</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">3.0.0'</span></code></pre></div></div><p>There are some things we have to unpack here. What we do is, we tell AWS Cloudformation to provision resources on our behalf and to deploy them in a stack. In the <em>Resources</em> section you see the API Gateway first, then the Lambda function we want to build from the code in the <em>CodeUri</em> with the <code class=\"language-plaintext highlighter-rouge\">handler</code> in <em>Handler</em>. We define the <em>Runtime</em> of the Lambda, as well as <em>MemorySize</em> and <em>Timeout</em>. You need to attach the proper role in the <em>Role</em> section, that we have created earlier. In the <em>Events</em> section we tell AWS Cloudformation to use <code class=\"language-plaintext highlighter-rouge\">FastapiExampleGateway</code> as the API Gateway with <code class=\"language-plaintext highlighter-rouge\">{proxy+}</code> integration, because as you recall that’s what makes this setup work in the first place.Check out the official <a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html\">Template Anatomy</a> to get a better understanding of other options available.</p><p>Once set up, you can now deploy the <a href=\"[repository](https://github.com/iwpnd/FastAPI-aws-lambda-example)\">FastAPI-aws-lambda-example application</a> from your local machine, through the <code class=\"language-plaintext highlighter-rouge\">template</code> that tells AWS Cloudformation to build a stack and provision the resources necessary.</p><h3 id=\"1-stage-validate-the-sam-template\">1. Stage: Validate the SAM template</h3><p>The first thing we do is to validate the SAM template to check if the YAML we provide is valid.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>sam validate</code></pre></div></div><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>2020-01-21 10:28:39 Found credentials in environment variables./path/to/FastAPI-aws-lambda-example/template.yml is a valid SAM Template</code></pre></div></div><h3 id=\"2-stage-build-the-deployment-package\">2. Stage: Build the deployment package</h3><p>Next up, we build the deployment package. If your application depends on packages that have natively compiled programs you pass <code class=\"language-plaintext highlighter-rouge\">--use-container</code> and SAM will attempt to build the application in a Docker container using based on <a href=\"https://github.com/lambci\">LambCI</a>. Optionally you can see what’s happening in the container if you also pass the <code class=\"language-plaintext highlighter-rouge\">--debug</code> flag.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>sam build --use-container --debug</code></pre></div></div><p>This will build the deployment package and store it in <code class=\"language-plaintext highlighter-rouge\">.aws-sam/build</code> along with a new <code class=\"language-plaintext highlighter-rouge\">template.yaml</code> that now also contains the values we <code class=\"language-plaintext highlighter-rouge\">!Sub</code> ‘ed or substituted like so <code class=\"language-plaintext highlighter-rouge\">${AWS::AccountId}</code>, in the initial template.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Starting Build inside a containerBuilding resource 'FastapiExampleLambda'Fetching lambci/lambda:build-python3.7 Docker container image......Mounting /path/to/FastAPI-aws-lambda-example as /tmp/samcli/source:ro,delegated inside runtime containerBuild SucceededBuilt Artifacts : .aws-sam/buildBuilt Template : .aws-sam/build/template.yamlCommands you can use next=========================[*] Invoke Function: sam local invoke[*] Package: sam package --s3-bucket &lt;yourbucket&gt;Running PythonPipBuilder:ResolveDependenciesRunning PythonPipBuilder:CopySource</code></pre></div></div><h3 id=\"3-stage-package-the-application\">3. Stage: Package the application</h3><p>Up next, packaging. As stated in the beginning, when an application in an AWS Lambda exceeds a certain size or has dependencies, we have to package the application and either upload it in the AWS console or prepare an intermediate AWS S3 bucket and let AWS Lambda get the application package from there. AWS SAM requires you to do the latter, or better, does it for you if you provision a bucket and pass it with <code class=\"language-plaintext highlighter-rouge\">--s3-bucket my-travis-deployment-bucket</code>.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>sam package --s3-bucket my-travis-deployment-bucket --output-template-file out.yml --region eu-west-1</code></pre></div></div><p>Which returns:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>2020-01-21 10:48:12 Found credentials in environment variables.Uploading to 2adfa5ddb62b541b7cf323cda43ee394 8523862 / 8523862.0 (100.00%) Successfully packaged artifacts and wrote output template to file out.yml.Execute the following command to deploy the packaged templatesam deploy --template-file /path/to/FastAPI-aws-lambda-example/out.yml --stack-name &lt;YOUR STACK NAME&gt;</code></pre></div></div><p>Now the application is packaged and a final template has been created that will now be used to tell AWS Cloudformation where the package is.</p><h3 id=\"4-stage-deploy-the-application\">4. Stage: Deploy the application</h3><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>sam deploy --template-file out.yml --stack-name example-stack-name --region eu-west-1 --no-fail-on-empty-changeset --capabilities CAPABILITY_IAM</code></pre></div></div><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Waiting for stack create/update to completeSuccessfully created/updated stack - example-stack-name</code></pre></div></div><p>This last step will finally deploy the application <code class=\"language-plaintext highlighter-rouge\">--stack-name</code> to a <code class=\"language-plaintext highlighter-rouge\">--region</code>. Important to note here is the option <code class=\"language-plaintext highlighter-rouge\">--no-fail-on-empty-changeset</code>. Deploying a new version of your application code does not change the stack itself. So running the command without this option will fail. With this, you can, however, push consecutive updates of your codes to the same stack.</p><p>If you now go to the <a href=\"https://eu-west-1.console.aws.amazon.com/apigateway/main/apis?region=eu-west-1\">API Gateway Console</a> you will see your API deployed to the <code class=\"language-plaintext highlighter-rouge\">prod</code> stage at <a href=\"\">https://xxxxxxxxxx.execute-api.eu-west-1.amazonaws.com/prod</a>.</p><h2 id=\"continuous-deployment-with-travis\">Continuous deployment with Travis</h2><p>Now for the last part, the continuous deployment through Github and Travis. The idea is that any code commit that passes an automated testing phase is automatically released into the production environment, and is accessible by the user. This means that the stages we laid out above, will no longer be executed manually by you, but instead in a Travis CI pipeline. Aight, let’se go.</p><ol>  <li>Go to <a href=\"https://travis-ci.com/\">https://travis-ci.com/</a> and sign up with your GitHub account.</li>  <li>Accept the Authorization of Travis CI and you’ll be redirected to GitHub.</li>  <li>Click on your profile picture in the top right of your Travis Dashboard, click the green Activate button, and select the repositories you want to use with Travis CI.</li>  <li>Select the repository of your application</li>  <li>Encrypt your <code class=\"language-plaintext highlighter-rouge\">AWS-ACCESS-KEY-ID</code> and your <code class=\"language-plaintext highlighter-rouge\">AWS-SECRET-ACCESS-KEY</code> of the <code class=\"language-plaintext highlighter-rouge\">travisdeploymentuser</code> we created at the beginning like <a href=\"https://docs.travis-ci.com/user/encryption-keys#usage\">this</a>, and put those in the .travis.yml file</li>  <li>Commit the .travis.yml file to your repository and check your build process in the <a href=\"https://travis-ci.com/dashboard\">Travis dashboard</a>.</li></ol><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">language</span><span class=\"pi\">:</span> <span class=\"s\">python</span><span class=\"na\">cache</span><span class=\"pi\">:</span> <span class=\"s\">pip</span><span class=\"na\">python</span><span class=\"pi\">:</span><span class=\"pi\">-</span> <span class=\"s1\">'</span><span class=\"s\">3.7'</span><span class=\"na\">install</span><span class=\"pi\">:</span><span class=\"pi\">-</span> <span class=\"s\">pip install awscli</span><span class=\"pi\">-</span> <span class=\"s\">pip install aws-sam-cli</span><span class=\"na\">jobs</span><span class=\"pi\">:</span>  <span class=\"na\">include</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span> <span class=\"na\">stage</span><span class=\"pi\">:</span> <span class=\"s\">test</span>      <span class=\"na\">script</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span> <span class=\"s\">pip install pytest</span>        <span class=\"pi\">-</span> <span class=\"s\">pip install -e .</span>        <span class=\"pi\">-</span> <span class=\"s\">pytest . -v</span>    <span class=\"pi\">-</span> <span class=\"na\">stage</span><span class=\"pi\">:</span> <span class=\"s\">deploy</span>      <span class=\"na\">script</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span> <span class=\"s\">sam validate</span>        <span class=\"pi\">-</span> <span class=\"s\">sam build --debug</span>        <span class=\"pi\">-</span> <span class=\"s\">sam package --s3-bucket my-travis-deployment-bucket --output-template-file out.yml --region eu-west-1</span>        <span class=\"pi\">-</span> <span class=\"s\">sam deploy --template-file out.yml --stack-name example-stack-name --region eu-west-1 --no-fail-on-empty-changeset --capabilities CAPABILITY_IAM</span>      <span class=\"na\">skip_cleanup</span><span class=\"pi\">:</span> <span class=\"no\">true</span>      <span class=\"na\">if</span><span class=\"pi\">:</span> <span class=\"s\">branch = master</span><span class=\"na\">notifications</span><span class=\"pi\">:</span>  <span class=\"na\">email</span><span class=\"pi\">:</span>    <span class=\"na\">on_failure</span><span class=\"pi\">:</span> <span class=\"s\">always</span><span class=\"na\">env</span><span class=\"pi\">:</span>  <span class=\"na\">global</span><span class=\"pi\">:</span>  <span class=\"pi\">-</span> <span class=\"s\">AWS_DEFAULT_REGION=eu-west-1</span>  <span class=\"pi\">-</span> <span class=\"na\">secure</span><span class=\"pi\">:</span> <span class=\"s\">your-encrypted-aws-access-key-id</span>  <span class=\"pi\">-</span> <span class=\"na\">secure</span><span class=\"pi\">:</span> <span class=\"s\">your-encrypted-aws-secret-access-key</span></code></pre></div></div><p>From now on every time you commit changes to your repository <code class=\"language-plaintext highlighter-rouge\">master</code> branch, it will be immediately be deployed to AWS Lambda.</p><h2 id=\"conclusion\">conclusion</h2><ol>  <li>We learned how to create a new user and policies in AWS IAM</li>  <li>We now have a basic application we can build upon</li>  <li>We learned about AWS API Gateway and AWS Lambda</li>  <li>We learned how <a href=\"https://github.com/erm/mangum\">Mangum</a> works</li>  <li>We learned about AWS SAM and how to deploy an application from your machine</li>  <li>We learned how to use Travis for continuous deployment of the said application instead of doing it manually</li></ol><p>If you have any questions feel free to reach out to me in the <a href=\"https://github.com/iwpnd/FastAPI-aws-lambda-example\">example repository</a> or via mail.</p>",
            "url": "https://iwpnd.pw//articles/2020-01/deploy-fastapi-to-aws-lambda",
            
            
            
            "tags": ["python","FastAPI","aws","aws lambda","aws sam"],
            
            "date_published": "2020-01-21T11:37:00+00:00",
            "date_modified": "2020-01-21T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-01/directory-as-tree",
            "title": "Directory tree asci representation",
            "summary": null,
            "content_text": "I was looking for an easy way to build a tree-structure from an input dictionary that I could use in documentation e.g. on github and stumbled upon tree, a homebrew formulae.Setup on macOS is as easy as:brew install treeNow if you cd into a directory that you want to map into a visually representation in ascii, then you would just typetree . &gt;&gt; output.txtwhich would return:├── Dockerfile├── LICENSE├── README.md├── app│   ├── __init__.py│   ├── api│   │   ├── __init__.py│   │   └── api_v1│   │       ├── __init__.py│   │       ├── api.py│   │       └── endpoints│   │           ├── __init__.py│   │           └── endpoint.py│   ├── core│   │   ├── __init__.py│   │   ├── config.py│   │   └── models│   └── main.py├── pyproject.toml├── requirements.txt├── scripts├── template.yml├── tests│   └── __init__.py8 directories, 17 filesSomething similar could be achieved in Python with this answer on Stackoverflow.",
            "content_html": "<p>I was looking for an easy way to build a tree-structure from an input dictionary that I could use in documentation e.g. on github and stumbled upon <a href=\"http://mama.indstate.edu/users/ice/tree/\">tree</a>, a homebrew formulae.</p><p>Setup on macOS is as easy as:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>brew install tree</code></pre></div></div><p>Now if you <code class=\"language-plaintext highlighter-rouge\">cd</code> into a directory that you want to map into a visually representation in ascii, then you would just type</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>tree . &gt;&gt; output.txt</code></pre></div></div><p>which would return:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>├── Dockerfile├── LICENSE├── README.md├── app│   ├── __init__.py│   ├── api│   │   ├── __init__.py│   │   └── api_v1│   │       ├── __init__.py│   │       ├── api.py│   │       └── endpoints│   │           ├── __init__.py│   │           └── endpoint.py│   ├── core│   │   ├── __init__.py│   │   ├── config.py│   │   └── models│   └── main.py├── pyproject.toml├── requirements.txt├── scripts├── template.yml├── tests│   └── __init__.py8 directories, 17 files</code></pre></div></div><p>Something similar could be achieved in Python with this <a href=\"https://stackoverflow.com/a/49912639\">answer on Stackoverflow</a>.</p>",
            "url": "https://iwpnd.pw//articles/2020-01/directory-as-tree",
            
            
            
            "tags": ["homebrew","documentation"],
            
            "date_published": "2020-01-14T11:37:00+00:00",
            "date_modified": "2020-01-14T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-01/opinion-on-fastapi",
            "title": "On FastAPI",
            "summary": null,
            "content_text": "Whenever I wanted people to interface some module I wrote or let them do inference on a model I resorted to Flask. After all it is lightweight, widely used, well documented and there are gazillion tutorials that help you out on getting started, e.g. the famous Flask Mega Tutorial.the right tool for the jobI didn’t know that I needed something else until somebody told me about FastAPI. See, you can build APIs with Flask, but it was not specifically intended with that task in mind, but with the ability to construct complex web applications. While that is not specifically excluding, there’s still that “right tool for the job” mentality that I want to keep in mind when I build things.things I like about FastAPII’m not one to tell you about the nitty-gritty details of both frameworks, and this is certainly not a Flask vs FastAPI discussion. I let other people talk about the differences between all of those available frameworks. One example would be a great talk by Chris Withers. What I can do, however, is tell you what I like about FastAPI, you take a look at it and decide for yourself.      documentation: I kind of judge documentation by the times I have to look up things outside of the actual documentation. In this case, it was (almost) none. FastAPI has an outstanding documentation with a great tutorial section that gets you started in no-time. Once you got started, it holds a section for everything you would love to do a deep-dive on.        it’s fast: It’s fast to code, and fast. Not that I will come anywhere close to fully notice it, but it is good to use something that has the potential to compete with APIs written with Go or nodeJS.        typing: FastAPI uses pydantic for data validation, so all I do is declare input and/or output models and FastAPI validates the data, converts the output to the type declaration of the models, adds a JSON schema in the OpenAPI path operation and uses the models as an input for the documentation. In the case of the output (response_model) FastAPI will also make sure to limit the output data to what is declared in the model.        automatic interactive documentation:Writing documentation for your API is a no-brainer. In Flask I would build my API and at the very least document, the endpoints my API provides, the example input parameters and some example output that is to be expected. Now, that’s two steps at least - build, document.With FastAPI, I build my API and the documentation is built automatically from the routes, python typing and the input/output models you define.        SwaggerUIAll of this comes with the interactive Swagger UI that you can only love. I don’t have to build a frontend to work with my API, I can just spin up a local uvicorn server with my FastAPI application and it’s automagically generated Swagger UI and I can interact with all endpoints of my application. Awesome, to say the least.  Now that’s my two cents on FastAPI, just give it a try in your next project and follow in the footsteps of other adopters.",
            "content_html": "<p>Whenever I wanted people to interface some module I wrote or let them do inference on a model I resorted to <a href=\"https://github.com/pallets/flask\">Flask</a>. After all it is lightweight, widely used, well documented and there are gazillion tutorials that help you out on getting started, e.g. the famous <a href=\"https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world\">Flask Mega Tutorial</a>.</p><h2 id=\"the-right-tool-for-the-job\">the right tool for the job</h2><p>I didn’t know that I needed something else until somebody told me about <a href=\"https://FastAPI.tiangolo.com/\">FastAPI</a>. See, you can build APIs with <a href=\"https://github.com/pallets/flask\">Flask</a>, but it was not specifically intended with that task in mind, but with the ability to construct complex web applications. While that is not specifically excluding, there’s still that “right tool for the job” mentality that I want to keep in mind when I build things.</p><h2 id=\"things-i-like-about-fastapi\">things I like about FastAPI</h2><p>I’m not one to tell you about the nitty-gritty details of both frameworks, and this is certainly not a Flask vs FastAPI discussion. I let other people talk about the differences between all of those available frameworks. One example would be a great <a href=\"https://www.youtube.com/watch?v=3DLwPcrE5mA\">talk by Chris Withers</a>. What I can do, however, is tell you what I like about FastAPI, you take a look at it and decide for yourself.</p><ul>  <li>    <p><strong>documentation</strong>: <br />I kind of judge documentation by the times I have to look up things outside of the actual documentation. In this case, it was (almost) none. <a href=\"https://FastAPI.tiangolo.com/\">FastAPI</a> has an outstanding documentation with a great <a href=\"https://FastAPI.tiangolo.com/tutorial/intro/\">tutorial section</a> that gets you started in no-time. Once you got started, it holds a section for everything you would love to do a deep-dive on.</p>  </li>  <li>    <p><strong>it’s fast</strong>: <br />It’s fast to code, and <a href=\"https://www.techempower.com/benchmarks/#section=test&amp;runid=7464e520-0dc2-473d-bd34-dbdfd7e85911&amp;hw=ph&amp;test=query&amp;l=zijzen-7\">fast</a>. Not that I will come anywhere close to fully notice it, but it is good to use something that has the potential to compete with APIs written with <strong>Go</strong> or <strong>nodeJS</strong>.</p>  </li>  <li>    <p><strong>typing</strong>: <br />FastAPI uses <a href=\"https://github.com/samuelcolvin/pydantic/\">pydantic</a> for data validation, so all I do is declare input and/or output models and FastAPI validates the data, converts the output to the type declaration of the models, adds a JSON schema in the OpenAPI path operation and uses the models as an input for the documentation. In the case of the output (<a href=\"https://FastAPI.tiangolo.com/tutorial/response-model/\">response_model</a>) FastAPI will also make sure to limit the output data to what is declared in the model.</p>  </li>  <li>    <p><strong>automatic interactive documentation</strong>:<br />Writing documentation for your API is a no-brainer. In Flask I would build my API and at the very least document, the endpoints my API provides, the example input parameters and some example output that is to be expected. Now, that’s two steps at least - build, document.With FastAPI, I build my API and the documentation is built automatically from the routes, python typing and the input/output models you define.</p>  </li>  <li>    <p><strong>SwaggerUI</strong>All of this comes with the interactive Swagger UI that you can only love. I don’t have to build a frontend to work with my API, I can just spin up a local uvicorn server with my FastAPI application and it’s automagically generated Swagger UI and I can interact with all endpoints of my application. Awesome, to say the least.</p>  </li></ul><p>Now that’s my two cents on FastAPI, just give it a try in your next project and follow in the footsteps of other <a href=\"https://FastAPI.tiangolo.com/#opinions\">adopters</a>.</p>",
            "url": "https://iwpnd.pw//articles/2020-01/opinion-on-fastapi",
            
            
            
            "tags": ["python","FastAPI"],
            
            "date_published": "2020-01-13T11:37:00+00:00",
            "date_modified": "2020-01-13T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-01/pre-commit-to-the-rescue",
            "title": "pre-commit to the rescue",
            "summary": null,
            "content_text": "During a keynote on last years PyCON Berlin somebody mentioned that he was watching that guy on twitch code for an audience. While hesitent at first, I actually followed @anthonywritescode / @asottile and checked him out the next weekend. While the amazement of watching other people, especially proficient people, code live and answer question in real time is a topic in itself,what I found especially interesting was what he was working on - something called pre-commit.Raise your hand if you have accidentally committed code that does not compile, or an invalid json without that nasty closing bracket.I’m sure you haven’t, but I most certainly did.git hooksThe whole concept of git hooks is new to me. What they do is to perform actions before or after git commit or before a git push. So if I commit, a hook will check my commit for valid json, yml or compilable code. Well, that’s brilliant. No, more unnecessary failing ci/cd pipelines. Hooray! The way I understand it tho, is that before pre-commit, git hooks were project specific and resided in your .git folder, or they’re global, tedious to setup and share among contributors of your projects.enter pre-commitPre-commit aims to solve this by managing the installation and execution of hooks. You can either use one of the out-of-the-box hooks or custom hooks ones, e.g. for black.installationThe installation is as easy aspip install pre-commitThen you create a .pre-commit-config.yaml. You can do this either on your own, or use the sample config that comes with it executingpre-commit sample-configWhat I happen to use is this:repos:    -   repo: https://github.com/pre-commit/pre-commit-hooks        rev: v2.3.0        hooks:        -   id: check-yaml            args: [--unsafe]        -   id: end-of-file-fixer        -   id: trailing-whitespace        -   id: check-json        -   id: check-added-large-files        -   id: check-ast    -   repo: https://github.com/psf/black        rev: 19.3b0        hooks:        -   id: blackAll the config does, is specifying the repositories that contain the hooks you want to use, and optional args you can add to specific hooks. After you setup your config, you attach the hooks to your repository using:pre-commit installFrom now, when you want to git add and git commit a diff, pre-commit will perform the checks you specified in your .pre-commit-config.yaml.If all checks pass, your commit goes through. If checks fail, or hooks modified your files during the checks, you either have to edit, add and commit them again, or automate that part. check-json for example has an --autofix argument, that automatically attempts to fix your brokes .json files before they’re committed.That’s it!",
            "content_html": "<p>During a keynote on last years <a href=\"https://pycon.de\">PyCON Berlin</a> somebody mentioned that he was watching that guy on <a href=\"https://www.twitch.tv/\">twitch</a> code for an audience. While hesitent at first, I actually followed <a href=\"https://twitch.tv/anthonywritescode\">@anthonywritescode</a> / <a href=\"https://github.com/asottile\">@asottile</a> and checked him out the next weekend. While the amazement of watching other people, especially proficient people, code live and answer question in real time is a topic in itself,what I found especially interesting was what he was working on - something called <a href=\"https://pre-commit.com/\">pre-commit</a>.</p><p>Raise your hand if you have accidentally committed code that does not compile, or an invalid json without that nasty closing bracket.</p><p>I’m sure you haven’t, but I most certainly did.</p><h3 id=\"git-hooks\">git hooks</h3><p>The whole concept of git hooks is new to me. What they do is to perform actions before or after <code class=\"language-plaintext highlighter-rouge\">git commit</code> or before a <code class=\"language-plaintext highlighter-rouge\">git push</code>. So if I commit, a hook will check my commit for valid json, yml or compilable code. Well, that’s brilliant. No, more unnecessary failing ci/cd pipelines. Hooray! The way I understand it tho, is that before pre-commit, git hooks were project specific and resided in your .git folder, or they’re global, tedious to setup and share among contributors of your projects.</p><h3 id=\"enter-pre-commit\">enter pre-commit</h3><p>Pre-commit aims to solve this by managing the installation and execution of hooks. You can either use one of the <a href=\"https://github.com/pre-commit/pre-commit-hooks\">out-of-the-box hooks</a> or custom hooks ones, e.g. for <a href=\"https://iwpnd.pw/articles/2020-01/black-python-code-formatter\">black</a>.</p><h3 id=\"installation\">installation</h3><p>The installation is as easy as</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>pip install pre-commit</code></pre></div></div><p>Then you create a <code class=\"language-plaintext highlighter-rouge\">.pre-commit-config.yaml</code>. You can do this either on your own, or use the sample config that comes with it executing</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>pre-commit sample-config</code></pre></div></div><p>What I happen to use is this:</p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">repos</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span>   <span class=\"na\">repo</span><span class=\"pi\">:</span> <span class=\"s\">https://github.com/pre-commit/pre-commit-hooks</span>        <span class=\"na\">rev</span><span class=\"pi\">:</span> <span class=\"s\">v2.3.0</span>        <span class=\"na\">hooks</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span>   <span class=\"na\">id</span><span class=\"pi\">:</span> <span class=\"s\">check-yaml</span>            <span class=\"na\">args</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"nv\">--unsafe</span><span class=\"pi\">]</span>        <span class=\"pi\">-</span>   <span class=\"na\">id</span><span class=\"pi\">:</span> <span class=\"s\">end-of-file-fixer</span>        <span class=\"pi\">-</span>   <span class=\"na\">id</span><span class=\"pi\">:</span> <span class=\"s\">trailing-whitespace</span>        <span class=\"pi\">-</span>   <span class=\"na\">id</span><span class=\"pi\">:</span> <span class=\"s\">check-json</span>        <span class=\"pi\">-</span>   <span class=\"na\">id</span><span class=\"pi\">:</span> <span class=\"s\">check-added-large-files</span>        <span class=\"pi\">-</span>   <span class=\"na\">id</span><span class=\"pi\">:</span> <span class=\"s\">check-ast</span>    <span class=\"pi\">-</span>   <span class=\"na\">repo</span><span class=\"pi\">:</span> <span class=\"s\">https://github.com/psf/black</span>        <span class=\"na\">rev</span><span class=\"pi\">:</span> <span class=\"s\">19.3b0</span>        <span class=\"na\">hooks</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span>   <span class=\"na\">id</span><span class=\"pi\">:</span> <span class=\"s\">black</span></code></pre></div></div><p>All the config does, is specifying the repositories that contain the hooks you want to use, and optional args you can add to specific hooks. After you setup your config, you attach the hooks to your repository using:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>pre-commit install</code></pre></div></div><p>From now, when you want to <code class=\"language-plaintext highlighter-rouge\">git add</code> and <code class=\"language-plaintext highlighter-rouge\">git commit</code> a diff, pre-commit will perform the checks you specified in your <code class=\"language-plaintext highlighter-rouge\">.pre-commit-config.yaml</code>.</p><p><img src=\"/img/2020-01-10-pre-commit.png\" alt=\"pre-commit workflow\" /></p><p>If all checks pass, your commit goes through. If checks fail, or hooks modified your files during the checks, you either have to edit, add and commit them again, or automate that part. <code class=\"language-plaintext highlighter-rouge\">check-json</code> for example has an <code class=\"language-plaintext highlighter-rouge\">--autofix</code> argument, that automatically attempts to fix your brokes .json files before they’re committed.</p><p>That’s it!</p>",
            "url": "https://iwpnd.pw//articles/2020-01/pre-commit-to-the-rescue",
            
            
            
            "tags": ["pre-commit","python","black","git hooks"],
            
            "date_published": "2020-01-10T11:37:00+00:00",
            "date_modified": "2020-01-10T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        },
    
        {
            "id": "https://iwpnd.pw//articles/2020-01/black-python-code-formatter",
            "title": "Black python code formatter",
            "summary": null,
            "content_text": "So, I was browsing the job offers of a company I’m particurly interested in working for and I noticed something odd. Never have a seen a job offer in which the company specifically talks about their code formatting, or while we’re at it, their code in general.  To ensure a standardized code style we use the formatter black.Okay. So black, hu?Can’t say I don’t like the colour, but I never heard of the code formatter, so I went ahead and watched Łukasz Langa’s talk Life Is Better Painted Black, or: How to Stop Worrying and Embrace Auto-Formatting. To say that it was a revelation is an understatement.When I was starting out with python I didn’t really know about PEP-8, and to be honest I didn’t really care. I was hyped that what I was writing code that worked and that’s it. Once exposed to other peoples code via github or the likes I noticed that they wrote code differently yet similar, and that I could quickly understand what their code was doing. So I read PEP-8 and tried to stick to it as much as I could without looking it up every so often.I eventually started working my first job where I was able to use Python professionally. We were a small team of five each having their own tasks and responsibilities, but working on the same product eventually. We had work agreements on a lot of stuff, from testing to code reviews, yet code formatting was that one agreement, where there was no agreement. While everybody claimed to know PEP-8 and claimed to stick to it, nobody actually did. When you point it out, people get defensive, there are arguments over line-length and line-breaks - all which is opinionated and finally, just wasted time.  Black is the uncompromising Python code formatter. By using it, you agree to cede control over minutiae of hand-formatting. In return, Black gives you speed, determinism, and freedom from pycodestyle nagging about formatting. You will save time and mental energy for more important matters.Black takes away the discussion. Does it stick to PEP8? Na, not completely. Is it opinionated? Hell yes! Does it matter that it is? No, not at all. As Dusty Phillips put it:  Black is opinionated so you don’t have to be.You can try it out yourself: black.now.sh.",
            "content_html": "<p>So, I was browsing the job offers of a company I’m particurly interested in working for and I noticed something odd. Never have a seen a job offer in which the company specifically talks about their code formatting, or while we’re at it, their code in general.</p><blockquote>  <p>To ensure a standardized code style we use the formatter black.</p></blockquote><p>Okay. So <a href=\"https://github.com/psf/black\">black</a>, hu?Can’t say I don’t like the colour, but I never heard of the code formatter, so I went ahead and watched Łukasz Langa’s talk <a href=\"https://www.youtube.com/watch?v=esZLCuWs_2Y\">Life Is Better Painted Black, or: How to Stop Worrying and Embrace Auto-Formatting</a>. To say that it was a revelation is an understatement.</p><p>When I was starting out with python I didn’t really know about <a href=\"https://www.python.org/dev/peps/pep-0008/\">PEP-8</a>, and to be honest I didn’t really care. I was hyped that what I was writing code that worked and that’s it. Once exposed to other peoples code via github or the likes I noticed that they wrote code differently yet similar, and that I could quickly understand what their code was doing. So I read <a href=\"https://www.python.org/dev/peps/pep-0008/\">PEP-8</a> and tried to stick to it as much as I could without looking it up every so often.</p><p>I eventually started working my first job where I was able to use Python professionally. We were a small team of five each having their own tasks and responsibilities, but working on the same product eventually. We had work agreements on a lot of stuff, from testing to code reviews, yet code formatting was that one agreement, where there was no agreement. While everybody claimed to know <a href=\"https://www.python.org/dev/peps/pep-0008/\">PEP-8</a> and claimed to stick to it, nobody actually did. When you point it out, people get defensive, there are arguments over line-length and line-breaks - all which is opinionated and finally, just wasted time.</p><blockquote>  <p>Black is the uncompromising Python code formatter. By using it, you agree to cede control over minutiae of hand-formatting. In return, Black gives you speed, determinism, and freedom from pycodestyle nagging about formatting. You will save time and mental energy for more important matters.</p></blockquote><p>Black takes away the discussion. Does it stick to PEP8? Na, not completely. Is it opinionated? Hell yes! Does it matter that it is? No, not at all. As <a href=\"https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=dusty+phillips\">Dusty Phillips</a> put it:</p><blockquote>  <p><em>Black</em> is opinionated so you don’t have to be.</p></blockquote><p>You can try it out yourself: <a href=\"https://black.now.sh/\">black.now.sh</a>.</p>",
            "url": "https://iwpnd.pw//articles/2020-01/black-python-code-formatter",
            
            
            
            
            
            "date_published": "2020-01-02T11:37:00+00:00",
            "date_modified": "2020-01-02T11:37:00+00:00",
            
                "author":  {
                "name": "iwpnd",
                "url": null,
                "avatar": "/img/iwpnd-logo.png"
                }
                
            
        }
    
    ]
}